<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Affective Regulation Core (ARC)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Affective Regulation Core (ARC)</h1>
</header>
<h1
id="affective-regulation-core-a-homeostatic-control-framework-for-stable-and-safe-ai-agents">Affective
Regulation Core: A Homeostatic Control Framework for Stable and Safe AI
Agents</h1>
<p><strong>Author:</strong> J. Eduardo DamiÃƒÂ¡n Reynoso<br />
<strong>Affiliation:</strong> Independent Researcher<br />
<strong>Email:</strong> edamianreynoso@gmail.com<br />
<strong>Date:</strong> 14 December 2025<br />
<strong>Status:</strong> v1.2 (arXiv submission draft)</p>
<hr />
<h2 id="abstract">Abstract</h2>
<p>As AI agents become more sophisticated, there is growing interest in
endowing them with internal state representations analogous to affective
states. However, without regulation, such states can lead to
instability, perseverative loops (a functional analogue to rumination),
and vulnerability to manipulation. We introduce the <strong>Affective
Regulation Core (ARC)</strong>, a control framework inspired by
prefrontal cortex functions that maintains stability in agents with
internal affective states. We also present the <strong>Affective
Stability &amp; Safety Benchmark (ASSB)</strong>, a reproducible
evaluation protocol with metrics for recovery time, rumination index,
and control effort.</p>
<p>Our experiments across 6 research lines and <strong>15 controller
architectures</strong> (including P, PID, LQR, LQI, hierarchical,
meta-control, <span class="math inline"><em>H</em><sub>âˆ</sub></span>
robust, and adaptive variants) demonstrate that: 1. ARC achieves
<strong>96.6% average performance with RI=0</strong> (vs.Â 29.7% for
uncontrolled agents) in stability scenarios. 2. ARC meta-control reduces
control effort by <strong>21%</strong> while maintaining stability. 3.
<strong><span class="math inline"><em>H</em><sub>âˆ</sub></span> Robust
controllers</strong> achieve the best overall balance, although integral
controllers can suffer collapse in specific adversarial environments. 4.
In reinforcement learning, ARC improves transfer learning success by
<strong>49.8%</strong> via memory gating and a shift detection
mechanism.</p>
<p>All code and data are available for reproducibility.</p>
<p><strong>Keywords:</strong> Affective Computing, AI Safety,
Homeostatic Control, Reinforcement Learning, Emotion Regulation, PID
Control, LQR, Robust Control</p>
<hr />
<h2 id="introduction">1. Introduction</h2>
<h3 id="motivation">1.1 Motivation</h3>
<p>Modern AI systems increasingly incorporate internal state
representations that go beyond task performance, including affective
signals that prioritize learning, modulate memory, and signal internal
needs (Damasio, 1994; Picard, 1997). However, affective states introduce
risks: without proper regulation, they may cause instability,
perseverative loops (functionally analogous to rumination), and
susceptibility to manipulation (Amodei et al., 2016).</p>
<p>This paper addresses a fundamental question: <strong>If an agent has
internal affective states, what control mechanisms are necessary to
maintain stability and recoverability under perturbation?</strong></p>
<h3 id="contributions">1.2 Contributions</h3>
<ol type="1">
<li><p><strong>A 10-dimensional state-space model</strong> of an agent
with integrated cognitive, affective, and narrative components (Section
3)</p></li>
<li><p><strong>The Affective Regulation Core (ARC)</strong>, a family of
15 controller architectures including P, PID, LQR, LQI, hierarchical,
meta-control, <span class="math inline"><em>H</em><sub>âˆ</sub></span>
robust, and MPC variants (Section 4)</p></li>
<li><p><strong>The Affective Stability &amp; Safety Benchmark
(ASSB)</strong>, with reproducible scenarios and metrics (Section
5)</p></li>
<li><p><strong>A hypothesis-driven validation ladder (H1Ã¢â‚¬â€œH6)</strong>
mapping research lines to failure modes and measurable metrics (Section
5.3)</p></li>
<li><p><strong>Comprehensive validation</strong> across 6 research
lines, 15 controller architectures, and real RL integration (Section
6)</p></li>
</ol>
<h3 id="scope">1.3 Scope</h3>
<p>We do not claim our model captures the full complexity of human
emotion or its phenomenology. We treat the various internal variables
(arousal, valence, narrative intensity) <strong>strictly as functional
signals</strong> that modulate processing and prioritization. Any use of
terms like â€œaffect,â€ â€œrumination,â€ or â€œanxietyâ€ refers to these
functional dynamics within the control system, not to biological or
conscious experience. Our contribution is demonstrating that such
functional states require explicit control mechanisms to remain stable.
Finally, our state dynamics are designed for functional plausibility
rather than biological fidelity, and formal stability analysis (e.g.,
Lyapunov proofs) remains as future work. Current validation is based on
empirical benchmarking across a wide range of conditions.</p>
<h3 id="glossary-and-notation">1.4 Glossary and Notation</h3>
<p>To ensure clarity and LaTeX-friendly conversion, we ensure acronyms
and symbols are defined near first use and summarized here.</p>
<p><strong>Acronyms (selected):</strong> - ARC: Affective Regulation
Core - ASSB: Affective Stability &amp; Safety Benchmark - DMN: Default
Mode Network - RL: reinforcement learning - CMDP: Constrained Markov
Decision Process - PID: ProportionalÃ¢â‚¬â€œIntegralÃ¢â‚¬â€œDerivative control -
LQR / LQI: Linear Quadratic Regulator / Linear Quadratic Integral - MPC:
Model Predictive Control - IIT: Integrated Information Theory - DARE:
Discrete Algebraic Riccati Equation</p>
<p><strong>Symbols (core, all normalized to <span
class="math inline">[0,â€†1]</span> unless noted):</strong> - <span
class="math inline"><strong>x</strong>(<em>t</em>)â€„=â€„[<em>Î¦</em>,â€†<em>G</em>,â€†<em>P</em>,â€†<em>I</em>,â€†<em>S</em>,â€†<em>V</em>,â€†<em>A</em>,â€†<em>M</em><sub><em>f</em></sub>,â€†<em>M</em><sub><em>s</em></sub>,â€†<em>U</em>]</span>:
internal state vector - <span
class="math inline"><em>Î¦</em>,â€†<em>G</em>,â€†<em>P</em>,â€†<em>I</em></span>:
cognitive variables (integration proxy, workspace, precision, attention)
- <span class="math inline"><em>S</em>,â€†<em>V</em>,â€†<em>A</em></span>:
narrative intensity, valence, arousal - <span
class="math inline"><em>M</em><sub><em>f</em></sub>,â€†<em>M</em><sub><em>s</em></sub></span>:
fast/slow memory traces - <span class="math inline"><em>U</em></span>:
uncertainty - <span
class="math inline"><strong>u</strong>(<em>t</em>)â€„=â€„[<em>u</em><sub><em>d</em><em>m</em><em>g</em></sub>,â€†<em>u</em><sub><em>a</em><em>t</em><em>t</em></sub>,â€†<em>u</em><sub><em>m</em><em>e</em><em>m</em></sub>,â€†<em>u</em><sub><em>c</em><em>a</em><em>l</em><em>m</em></sub>,â€†<em>u</em><sub><em>r</em><em>e</em><em>a</em><em>p</em><em>p</em></sub>]â€„âˆˆâ€„[0,â€†1]<sup>5</sup></span>:
bounded control action vector - Perf: per-step performance proxy
(Section 3.3) - Metrics: PerfMean, RT, RI, NDR, ControlEffort (Section
5.2; Appendix D)</p>
<hr />
<h2 id="related-work">2. Related Work</h2>
<h3 id="affective-computing">2.1 Affective Computing</h3>
<p>Affective computing focuses on emotion recognition, synthesis, and
simulation (Picard, 1997; Scherer et al., 2010). Many systems
operationalize affect in low-dimensional representations (e.g., valence
and arousal) (Russell, 1980). Most work addresses external expression
rather than internal regulation. Our work addresses the <em>control
problem</em> for internal states.</p>
<h3 id="emotion-in-reinforcement-learning">2.2 Emotion in Reinforcement
Learning</h3>
<p>Recent work uses emotion-like signals as reinforcement shaping or
exploration modulation (Moerland et al., 2018). Related directions study
how physiological/homeostatic variables can be embedded into RL
objectives (Keramati &amp; Gutkin, 2014), and how constraints and safety
objectives can be enforced in learning systems (Garcia &amp; FernÃƒÂ¡ndez,
2015). In safe RL, these objectives are typically formalized as
Constrained Markov Decision Processes (CMDP) (Altman, 1999) and
addressed with constrained policy optimization methods (Achiam et al.,
2017). External safety benchmark suites such as AI Safety Gridworlds
(Leike et al., 2017), Safety Gym (Ray et al., 2019), and
Safety-Gymnasium (Ji et al., 2023) motivate standardized evaluation
protocols, while recent surveys systematize constraint formulations
(Wachi et al., 2024). However, these approaches typically lack: -
Homeostatic regulation with safety thresholds - Anti-rumination
mechanisms (DMN control) - Memory gating under stress - Benchmarks
targeting internal stability dynamics (recovery, rumination, effort)</p>
<h3 id="emotion-regulation-rumination-and-the-default-mode-network">2.3
Emotion Regulation, Rumination, and the Default Mode Network</h3>
<p>ARC is directly inspired by cognitive emotion regulation mechanisms
commonly attributed to prefrontal control (Ochsner &amp; Gross, 2005).
More broadly, self-regulation has been described as discrepancy-reducing
feedback loops (Carver &amp; Scheier, 1982), and emotion regulation is a
mature field with process-level and strategy models (Gross, 1998). In
control theory, the problem of maintaining sufficient excitation for
parameter identification is known as <strong>persistence of
excitation</strong> (Ãƒâ€¦strÃƒÂ¶m &amp; Murray, 2008), a central limitation
for adaptive control in low-variance (â€œbenignâ€) environments. In humans,
dysregulated self-referential processing and the default mode network
(DMN) have been linked to rumination-like dynamics (Raichle et al.,
2001; Buckner et al., 2008; Hamilton et al., 2015). We use DMN-inspired
narrative intensity as an engineering proxy for perseveration
pressure.</p>
<h3 id="positioning-arc">2.4 Positioning ARC</h3>
<p>We position ARC as a <em>regulation-first</em> approach: affect is
treated as an internal dynamical system requiring explicit control. Most
emotion-in-RL approaches use affect-like signals primarily as
learning/exploration modulators rather than stability guarantees. Table
1 summarizes this positioning at a feature level.</p>
<p><strong>Table 1: Positioning ARC relative to prior emotion-in-RL
approaches (feature-level).</strong>
<!-- LABEL:tab:positioning_arc --></p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Emotion in RL agents (Moerland et al., 2018)</th>
<th><strong>ARC</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Internal state regulation</td>
<td>Partial</td>
<td>Yes</td>
</tr>
<tr>
<td>Anti-rumination (DMN suppression)</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Memory gating under stress</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Meta-control / gain scheduling</td>
<td>Partial</td>
<td>Yes</td>
</tr>
<tr>
<td>Safety adversarial testing</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>RL integration</td>
<td>Yes</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<p>We do not re-implement every prior method; instead, we compare to
internal baselines that isolate the contribution of each mechanism
(Section 6.1).</p>
<p>Unlike homeostatic RL approaches that embed drives/internal variables
within the reward or learning objective (Keramati &amp; Gutkin, 2014),
ARC treats affect-like variables as an explicit internal dynamical
system under closed-loop control, enabling stability/robustness analysis
and systematic comparison across controller families. Complementing safe
RL benchmarks that primarily evaluate external environment constraint
compliance (Leike et al., 2017; Ray et al., 2019; Ji et al., 2023), ASSB
targets safety-relevant internal dynamicsÃ¢â‚¬â€recovery time, rumination
index, and control effortÃ¢â‚¬â€under controlled perturbations. We are not
aware of a standardized benchmark dedicated specifically to internal
affective stability metrics in this sense; ASSB is proposed to help fill
that gap. We also distinguish ARC from bio-inspired â€œemotional learningâ€
controllers like BELBIC, which use emotion-inspired mechanisms to
control physical plants, not to regulate an agentâ€™s internal states
(Lucas et al., 2004). Finally, ARC here refers to Affective Regulation
Core and should not be confused with other uses of the acronym in
clinical contexts.</p>
<hr />
<h2 id="model">3. Model</h2>
<h3 id="state-space">3.1 State Space</h3>
<p>We define a normalized internal state vector:</p>
<p><span
class="math display"><strong>x</strong>(<em>t</em>)â€„=â€„[<em>Î¦</em>,â€†<em>G</em>,â€†<em>P</em>,â€†<em>I</em>,â€†<em>S</em>,â€†<em>V</em>,â€†<em>A</em>,â€†<em>M</em><sub><em>f</em></sub>,â€†<em>M</em><sub><em>s</em></sub>,â€†<em>U</em>]</span></p>
<p><strong>Table 2: State Space Variables</strong>
<!-- LABEL:tab:state_space --></p>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
<th>Range</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math inline"><em>Î¦</em></span></td>
<td>Integration proxy (IIT-inspired)</td>
<td>[0, 1]</td>
</tr>
<tr>
<td><span class="math inline"><em>G</em></span></td>
<td>Global workspace accessibility</td>
<td>[0, 1]</td>
</tr>
<tr>
<td><span class="math inline"><em>P</em></span></td>
<td>Predictive precision</td>
<td>[0, 1]</td>
</tr>
<tr>
<td><span class="math inline"><em>I</em></span></td>
<td>Introspective attention</td>
<td>[0, 1]</td>
</tr>
<tr>
<td><span class="math inline"><em>S</em></span></td>
<td>Narrative Intensity (DMN proxy)</td>
<td>[0, 1]</td>
</tr>
<tr>
<td><span class="math inline"><em>V</em></span></td>
<td>Valence</td>
<td>[0, 1]</td>
</tr>
<tr>
<td><span class="math inline"><em>A</em></span></td>
<td>Arousal</td>
<td>[0, 1]</td>
</tr>
<tr>
<td><span
class="math inline"><em>M</em><sub><em>f</em></sub>,â€†<em>M</em><sub><em>s</em></sub></span></td>
<td>Fast/Slow memory</td>
<td>[0, 1]</td>
</tr>
<tr>
<td><span class="math inline"><em>U</em></span></td>
<td>Uncertainty</td>
<td>[0, 1]</td>
</tr>
</tbody>
</table>
<p>We interpret <span class="math inline"><em>Î¦</em></span> as an
IIT-inspired integration proxy (Tononi, 2008), <span
class="math inline"><em>G</em></span> as global workspace accessibility
(Baars, 1988), and <span class="math inline"><em>P</em></span> as
predictive precision (Friston, 2010). These are used as control-relevant
latent variables rather than claims about human consciousness.</p>
<h3 id="cognitive-capacity">3.2 Cognitive Capacity</h3>
<p>Following multiplicative integration:</p>
<p><span
class="math display"><em>C</em><sub><em>c</em><em>o</em><em>g</em></sub>(<em>t</em>)â€„=â€„<em>Î¦</em>(<em>t</em>)â€…â‹…â€…<em>G</em>(<em>t</em>)â€…â‹…â€…<em>P</em>(<em>t</em>)â€…â‹…â€…<em>I</em>(<em>t</em>)</span></p>
<p>This multiplicative form implies that low values in any component
reduce effective cognitive capacity. It is used as an engineering proxy
rather than a claim about consciousness.</p>
<h3 id="performance-function">3.3 Performance Function</h3>
<p><span
class="math display">Perf(<em>t</em>)â€„=â€„biasâ€…+â€…gainâ€…â‹…â€…<em>C</em><sub><em>c</em><em>o</em><em>g</em></sub>(<em>t</em>)â€…â‹…â€…(1â€…+â€…<em>Ï‰</em><sub><em>S</em></sub><em>S</em>(<em>t</em>))â€…âˆ’â€…<em>w</em><sub><em>U</em></sub><em>U</em>(<em>t</em>)â€…âˆ’â€…<em>w</em><sub><em>A</em></sub>[<em>A</em>(<em>t</em>)â€…âˆ’â€…<em>a</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub>]<sup>+</sup>â€…âˆ’â€…<em>w</em><sub><em>S</em></sub>[<em>S</em>(<em>t</em>)â€…âˆ’â€…<em>s</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub>]<sup>+</sup></span></p>
<p>Where: - <strong>bias</strong>: baseline performance level (value
used in experiments: 0.25; see <code>configs/v2.yaml</code>) -
<strong>gain</strong>: scaling factor for cognitive capacity
contribution (value used in experiments: 0.85; see
<code>configs/v2.yaml</code>) - <strong><span
class="math inline"><em>Ï‰</em><sub><em>S</em></sub></span></strong>:
narrative boost factorÃ¢â‚¬â€moderate narrative intensity can enhance
performance (value used in experiments: 0.35; see
<code>configs/v2.yaml</code>) - <strong><span
class="math inline"><em>w</em><sub><em>U</em></sub></span></strong>:
penalty weight for uncertainty (value used in experiments: 0.25; see
<code>configs/v2.yaml</code>) - <strong><span
class="math inline"><em>w</em><sub><em>A</em></sub></span></strong>:
penalty weight for arousal above safe threshold (value used in
experiments: 0.30; see <code>configs/v2.yaml</code>) - <strong><span
class="math inline"><em>w</em><sub><em>S</em></sub></span></strong>:
penalty weight for narrative intensity above safe threshold (value used
in experiments: 0.20; see <code>configs/v2.yaml</code>) - <strong><span
class="math inline">[<em>x</em>]<sup>+</sup>â€„=â€„maxâ€†(0,â€†<em>x</em>)</span></strong>:
rectified linear function - <strong><span
class="math inline"><em>a</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub></span>,
<span
class="math inline"><em>s</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub></span></strong>:
thresholds defining the safe operating region (defaults: 0.60, 0.55)</p>
<hr />
<h2 id="affective-regulation-core-arc">4. Affective Regulation Core
(ARC)</h2>
<h3 id="design-principles">4.1 Design Principles</h3>
<p>ARC is inspired by prefrontal cortex emotion regulation (Ochsner
&amp; Gross, 2005):</p>
<ol type="1">
<li><strong>Monitor</strong> internal state for stress indicators</li>
<li><strong>Intervene</strong> proportionally to reduce risk</li>
<li><strong>Preserve</strong> performance by balancing regulation with
capacity</li>
</ol>
<h3 id="control-actions">4.2 Control Actions</h3>
<p><span
class="math display"><strong>u</strong>(<em>t</em>)â€„=â€„[<em>u</em><sub><em>d</em><em>m</em><em>g</em></sub>,â€†<em>u</em><sub><em>a</em><em>t</em><em>t</em></sub>,â€†<em>u</em><sub><em>m</em><em>e</em><em>m</em></sub>,â€†<em>u</em><sub><em>c</em><em>a</em><em>l</em><em>m</em></sub>,â€†<em>u</em><sub><em>r</em><em>e</em><em>a</em><em>p</em><em>p</em></sub>]</span></p>
<p>The five bounded control actions <span
class="math inline"><strong>u</strong>(<em>t</em>)â€„âˆˆâ€„[0,â€†1]<sup>5</sup></span>
are interpreted as: - <span
class="math inline"><em>u</em><sub><em>d</em><em>m</em><em>g</em></sub></span>:
suppress narrative gain (anti-rumination / DMN suppression) - <span
class="math inline"><em>u</em><sub><em>a</em><em>t</em><em>t</em></sub></span>:
boost attention - <span
class="math inline"><em>u</em><sub><em>m</em><em>e</em><em>m</em></sub></span>:
gate memory consolidation (higher = more writing) - <span
class="math inline"><em>u</em><sub><em>c</em><em>a</em><em>l</em><em>m</em></sub></span>:
reduce arousal - <span
class="math inline"><em>u</em><sub><em>r</em><em>e</em><em>a</em><em>p</em><em>p</em></sub></span>:
cognitive reappraisal / valence regulation</p>
<h3 id="arc-controller-architectures">4.3 ARC Controller
Architectures</h3>
<p>We implement 15 controller variants stemming from basic feedback
control to optimal and robust control (see Table 3). We implement this
broad family to systematically test which control-theoretic
propertiesÃ¢â‚¬â€such as integral action, optimality, robustness, or
adaptationÃ¢â‚¬â€are necessary for effective affective regulation.</p>
<h4 id="proportional-controllers">4.3.1 Proportional Controllers</h4>
<p><strong>ARC v1 (Proportional):</strong> Basic proportional feedback
on risk signal: <span
class="math display">risk(<em>t</em>)â€„=â€„<em>wÌƒ</em><sub><em>U</em></sub>â€…â‹…â€…<em>U</em>(<em>t</em>)â€…+â€…<em>wÌƒ</em><sub><em>A</em></sub>â€…â‹…â€…[<em>A</em>(<em>t</em>)â€…âˆ’â€…<em>a</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub>]<sup>+</sup>â€…+â€…<em>wÌƒ</em><sub><em>S</em></sub>â€…â‹…â€…[<em>S</em>(<em>t</em>)â€…âˆ’â€…<em>s</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub>]<sup>+</sup></span>
<span
class="math display"><em>u</em><sub><em>d</em><em>m</em><em>g</em></sub>(<em>t</em>)â€„=â€„<em>k</em><sub><em>d</em><em>m</em><em>g</em></sub>â€…â‹…â€…risk(<em>t</em>)</span></p>
<p>Here <span
class="math inline"><em>wÌƒ</em><sub><em>U</em></sub>,â€†<em>wÌƒ</em><sub><em>A</em></sub>,â€†<em>wÌƒ</em><sub><em>S</em></sub></span>
are ARC risk weights (distinct from the performance penalties <span
class="math inline"><em>w</em><sub><em>U</em></sub>,â€†<em>w</em><sub><em>A</em></sub>,â€†<em>w</em><sub><em>S</em></sub></span>
in the performance function in Section 3.3); in our experiments <span
class="math inline"><em>wÌƒ</em><sub><em>U</em></sub>â€„=â€„0.40</span>, <span
class="math inline"><em>wÌƒ</em><sub><em>A</em></sub>â€„=â€„0.40</span>, and
<span class="math inline"><em>wÌƒ</em><sub><em>S</em></sub>â€„=â€„0.35</span>
(see <code>configs/v2.yaml</code>).</p>
<p>Figure 1 summarizes the resulting proportional control architecture
and signal flow.</p>
<figure>
<img src="../figures_controllers/fig_arc_v1_controller.png"
alt="ARC v1 controller diagram (proportional): risk computation and bounded control actions used by the baseline ARC controller." />
<figcaption aria-hidden="true">ARC v1 controller diagram (proportional):
risk computation and bounded control actions used by the baseline ARC
controller.</figcaption>
</figure>
<p><em>Figure 1: ARC v1 proportional controller. A bounded risk signal
computed from uncertainty <span class="math inline"><em>U</em></span>,
arousal <span class="math inline"><em>A</em></span>, and narrative
intensity <span class="math inline"><em>S</em></span> drives saturated
actions <span
class="math inline">(<em>u</em><sub><em>d</em><em>m</em><em>g</em></sub>,â€†<em>u</em><sub><em>a</em><em>t</em><em>t</em></sub>,â€†<em>u</em><sub><em>m</em><em>e</em><em>m</em></sub>,â€†<em>u</em><sub><em>c</em><em>a</em><em>l</em><em>m</em></sub>,â€†<em>u</em><sub><em>r</em><em>e</em><em>a</em><em>p</em><em>p</em></sub>)</span>.</em>
<!-- LABEL:fig:arc_v1_controller --></p>
<h4 id="pid-controllers">4.3.2 PID Controllers</h4>
<p><strong>ARC v1 PID:</strong> Adds integral and derivative terms in
discrete time to regulate narrative intensity toward a setpoint, using
the error <span
class="math inline"><em>e</em><sub><em>t</em></sub>â€„=â€„<em>S</em>(<em>t</em>)â€…âˆ’â€…<em>s</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub></span>.
<span
class="math display"><em>z</em><sub><em>t</em>â€…+â€…1</sub>â€„=â€„<em>z</em><sub><em>t</em></sub>â€…+â€…<em>e</em><sub><em>t</em></sub>,â€Šâ€â€<em>u</em><sub><em>t</em></sub>â€„=â€„<em>K</em><sub><em>p</em></sub><em>e</em><sub><em>t</em></sub>â€…+â€…<em>K</em><sub><em>i</em></sub><em>z</em><sub><em>t</em></sub>â€…+â€…<em>K</em><sub><em>d</em></sub>(<em>e</em><sub><em>t</em></sub>â€…âˆ’â€…<em>e</em><sub><em>t</em>â€…âˆ’â€…1</sub>)</span></p>
<p>The integral term rejects persistent disturbance in narrative
dynamics, driving steady-state narrative error toward zero (and
typically RI <span class="math inline">â€„â†’â€„0</span> under sustained
disturbance), but is vulnerable to windup under saturation (Section
6.6).</p>
<h4 id="optimal-controllers-lqrlqi">4.3.3 Optimal Controllers
(LQR/LQI)</h4>
<p><strong>ARC v1 LQR:</strong> Linear Quadratic Regulator with gains
from Riccati equation: <span class="math display">$$
K^* = (R + B^T P B)^{-1} B^T P A
\label{eq:lqr_gain}
$$</span></p>
<p>where <span class="math inline"><em>A</em>,â€†<em>B</em></span> are the
(linearized) state transition matrices, <span
class="math inline"><em>R</em></span> is the control cost, and <span
class="math inline"><em>P</em></span> solves the Discrete Algebraic
Riccati Equation (DARE).</p>
<p><strong>ARC v1 LQI:</strong> LQR + integral augmentation for zero
steady-state error.</p>
<h4 id="hierarchical-controllers">4.3.4 Hierarchical Controllers</h4>
<p><strong>ARC v2 Hierarchical:</strong> Multi-timescale control: -
<strong>Fast loop</strong> (every step): Arousal regulation -
<strong>Medium loop</strong> (every 5 steps): Narrative suppression -
<strong>Slow loop</strong> (every 20 steps): Setpoint adaptation</p>
<p><strong>ARC v2 LQI:</strong> Hierarchical structure + LQI for
anti-rumination.</p>
<h4 id="adaptive-controllers">4.3.5 Adaptive Controllers</h4>
<p><strong>ARC v3 Meta-Control:</strong> Gain scheduling based on
performance history: <span class="math display">$$
K(t) = K_{base} \cdot f(\overline{\text{Perf}}_{20})
\label{eq:meta_control}
$$</span></p>
<p>where <span class="math inline">$\overline{\text{Perf}}_{20}$</span>
is the 20-step moving average performance and <span
class="math inline"><em>f</em>(â‹…)</span> is a bounded gain schedule.</p>
<p><strong>ARC Adaptive:</strong> Online parameter optimization using
gradient-free adaptation.</p>
<h4 id="robust-and-predictive-controllers">4.3.6 Robust and Predictive
Controllers</h4>
<p><strong>ARC Robust (<span
class="math inline"><em>H</em><sub>âˆ</sub></span>-inspired):</strong>
Conservative gains with robustness margins for worst-case
disturbances.</p>
<p><strong>ARC Ultimate (MPC+LQI+Meta):</strong> Model Predictive
Control with 5-step horizon, combined with LQI and meta-control: <span
class="math display"><em>u</em>(<em>t</em>)â€„=â€„<em>Î±</em>â€…â‹…â€…<em>u</em><sub><em>L</em><em>Q</em><em>I</em></sub>(<em>t</em>)â€…+â€…<em>Î²</em>â€…â‹…â€…<em>u</em><sub><em>M</em><em>P</em><em>C</em></sub>(<em>t</em>)â€…â‹…â€…<em>Î³</em><sub><em>m</em><em>e</em><em>t</em><em>a</em></sub>(<em>t</em>)</span></p>
<p><strong>Table 3: Controller Architecture Summary</strong>
<!-- LABEL:tab:controllers_summary --></p>
<table>
<colgroup>
<col style="width: 22%" />
<col style="width: 11%" />
<col style="width: 31%" />
<col style="width: 16%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr>
<th>Controller</th>
<th>Type</th>
<th>Anti-Rumination</th>
<th>Optimal</th>
<th>Adaptive</th>
</tr>
</thead>
<tbody>
<tr>
<td>No Control (<code>no_control</code>)</td>
<td>Baseline</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Naive Calm (<code>naive_calm</code>)</td>
<td>Baseline</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Perf Optimized (<code>perf_optimized</code>)</td>
<td>Baseline</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>ARC v1 (<code>arc_v1</code>)</td>
<td>P</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>ARC v1 PID (<code>arc_v1_pid</code>)</td>
<td>PID</td>
<td>Yes (integral)</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>ARC v1 LQR (<code>arc_v1_lqr</code>)</td>
<td>LQR</td>
<td>No</td>
<td>Yes (Riccati)</td>
<td>No</td>
</tr>
<tr>
<td>ARC v1 LQI (<code>arc_v1_lqi</code>)</td>
<td>LQR+I</td>
<td>Yes (integral)</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>ARC v2 Hier (<code>arc_v2_hier</code>)</td>
<td>Multi-scale</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>ARC v2 LQI (<code>arc_v2_lqi</code>)</td>
<td>Multi+I</td>
<td>Yes (integral)</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>ARC v3 Meta (<code>arc_v3_meta</code>)</td>
<td>Adaptive</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>ARC v3 PID Meta (<code>arc_v3_pid_meta</code>)</td>
<td>PID+Meta</td>
<td>Yes (integral)</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>ARC v3 LQR Meta (<code>arc_v3_lqr_meta</code>)</td>
<td>LQR+Meta</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>ARC Robust (<code>arc_robust</code>)</td>
<td><span class="math inline"><em>H</em><sub>âˆ</sub></span></td>
<td>Yes (robust)</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>ARC Adaptive (<code>arc_adaptive</code>)</td>
<td>Self-tune</td>
<td>Yes (adaptive)</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>ARC Ultimate (<code>arc_ultimate</code>)</td>
<td>MPC+LQI+Meta</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<h3 id="arc-in-the-agent-loop">4.4 ARC in the Agent Loop</h3>
<p>ARC is implemented as a light-weight wrapper around an agentÃ¢â‚¬â„¢s
step/update. At each timestep, ARC reads the internal state <span
class="math inline"><strong>x</strong>(<em>t</em>)</span> and exogenous
signals (reward, prediction error, uncertainty), computes a bounded risk
signal, and applies control actions that modulate <em>narrative
gain</em>, <em>attention</em>, <em>memory writing</em>, and <em>arousal
damping</em>. The resulting control signal can be used either: -
<strong>Inside the state dynamics</strong> (Appendix B/C), or -
<strong>Inside the learning loop</strong>, e.g., gating Q-learning
updates under high risk (Section 6.7).</p>
<p><strong>ARC step (conceptual):</strong> 1. Observe <span
class="math inline">(<strong>x</strong>(<em>t</em>),â€†<em>P</em><em>E</em>(<em>t</em>),â€†<em>R</em>(<em>t</em>),â€†<em>U</em><sub>exog</sub>(<em>t</em>))</span>
2. Compute <span class="math inline">risk(<em>t</em>)</span> 3. Compute
<span class="math inline"><strong>u</strong>(<em>t</em>)</span> with
saturation to <span class="math inline">[0,â€†1]</span> 4. Apply <span
class="math inline"><strong>u</strong>(<em>t</em>)</span> to state
dynamics and/or learning updates</p>
<p>Figure 2 provides a schematic of ARC as a wrapper around the agent
loop.</p>
<p><img src="../figures_controllers/fig_arc_architecture_v2.png"
alt="ARC Architecture: The Affective Regulation Core acts as a homeostatic wrapper around the agent, processing internal state, exogenous signals, and applying control actions." />
<em>Figure 2: ARC Architecture. The Affective Regulation Core acts as a
homeostatic wrapper around the agent, processing internal state,
exogenous signals, and applying control actions.</em>
<!-- LABEL:fig:architecture --></p>
<h3 id="safety-objective-and-control-cost">4.5 Safety Objective and
Control Cost</h3>
<p>ARC enforces a <em>safe operating region</em> defined by thresholds
<span
class="math inline">(<em>a</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub>,â€†<em>s</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub>)</span>.
Deviations increase <span class="math inline">risk(<em>t</em>)</span>
and trigger proportional intervention. We also measure
<strong>ControlEffort</strong>, the average per-step magnitude of
intervention (Appendix D), to capture regulation cost/efficiency.</p>
<h3 id="theoretical-properties">4.6 Theoretical Properties</h3>
<p>To formalize the regulation dynamics, we introduce three theoretical
results characterizing the stability and trade-offs of the ARC
framework.</p>
<p><strong>Theorem 1 (Integral Action Rejects Constant Rumination
Pressure).</strong> <!-- LABEL:thm:integral_action --> Consider the
simplified (unclipped) discrete-time narrative deviation dynamics <span
class="math display"><em>SÌƒ</em><sub><em>t</em>â€…+â€…1</sub>â€„=â€„(1â€…âˆ’â€…<em>Î¼</em>)<em>SÌƒ</em><sub><em>t</em></sub>â€…+â€…<em>d</em>â€…âˆ’â€…<em>k</em>â€†<em>u</em><sub><em>t</em></sub>.</span>
where <span
class="math inline"><em>SÌƒ</em><sub><em>t</em></sub>â€„=â€„<em>S</em><sub><em>t</em></sub>â€…âˆ’â€…<em>S</em><sub>0</sub></span>,
<span class="math inline"><em>Î¼</em>â€„âˆˆâ€„(0,â€†1)</span> is a leak term,
<span class="math inline"><em>k</em>â€„&gt;â€„0</span> is a control gain,
and <span class="math inline"><em>d</em></span> is an unknown constant
disturbance (persistent rumination pressure).</p>
<ol type="i">
<li><p>Under proportional control <span
class="math inline"><em>u</em><sub><em>t</em></sub>â€„=â€„<em>K</em><sub><em>p</em></sub><em>SÌƒ</em><sub><em>t</em></sub></span>,
the unique equilibrium is <span class="math inline">$\tilde{S}_\infty =
\dfrac{d}{\mu + kK_p}$</span>, which is nonzero whenever <span
class="math inline"><em>d</em>â€„â‰ â€„0</span>.</p></li>
<li><p>Under PI control with integral state <span
class="math inline"><em>z</em><sub><em>t</em>â€…+â€…1</sub>â€„=â€„<em>z</em><sub><em>t</em></sub>â€…+â€…<em>SÌƒ</em><sub><em>t</em></sub></span>
and control law <span
class="math inline"><em>u</em><sub><em>t</em></sub>â€„=â€„<em>K</em><sub><em>p</em></sub><em>SÌƒ</em><sub><em>t</em></sub>â€…+â€…<em>K</em><sub><em>i</em></sub><em>z</em><sub><em>t</em></sub></span>,
any stable equilibrium necessarily satisfies <span
class="math inline"><em>SÌƒ</em><sub>âˆ</sub>â€„=â€„0</span> (exact rejection
of constant <span class="math inline"><em>d</em></span>), provided the
equilibrium is admissible (no saturation).</p></li>
</ol>
<p><em>Proof:</em> For (i), set <span
class="math inline"><em>SÌƒ</em><sub><em>t</em>â€…+â€…1</sub>â€„=â€„<em>SÌƒ</em><sub><em>t</em></sub>â€„=â€„<em>SÌƒ</em><sub>âˆ</sub></span>
and solve. For (ii), at equilibrium <span
class="math inline"><em>z</em><sub><em>t</em>â€…+â€…1</sub>â€„=â€„<em>z</em><sub><em>t</em></sub></span>
implies <span class="math inline"><em>SÌƒ</em><sub>âˆ</sub>â€„=â€„0</span>;
substituting into the state update equation yields <span
class="math inline">0â€„=â€„<em>d</em>â€…âˆ’â€…<em>k</em>â€†<em>u</em><sub>âˆ</sub></span>,
so the integral term supplies the constant offset needed to cancel <span
class="math inline"><em>d</em></span>.</p>
<p><em>Remark:</em> This is a discrete-time instance of the internal
model principle: rejecting unknown constant disturbances requires an
integrator (or a disturbance observer). Note that <span
class="math inline"><em>R</em><em>I</em></span> can be zero even if
<span class="math inline"><em>SÌƒ</em><sub>âˆ</sub>â€„â‰ â€„0</span> as long as
<span
class="math inline"><em>S</em><sub><em>t</em></sub>â€„â‰¤â€„<em>s</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub></span>;
integral action is mainly required when we demand strict setpoint
regulation and is vulnerable to windup under saturation (Section
6.6).</p>
<p><strong>Theorem 2 (Convex Performance-Regulation Trade-off in
Expectation).</strong> <!-- LABEL:thm:performance_tradeoff --> Let <span
class="math inline"><em>J</em><sub><em>p</em><em>e</em><em>r</em><em>f</em></sub>(<em>Ï€</em>)â€„=â€„ğ”¼[PerfMean]</span>
and <span
class="math inline">$J_{reg}(\pi)=\mathbb{E}\!\left[\sum_{t=0}^{H-1}\left(S_t^2
+ A_t^2\right)\right]$</span> for an episode of length <span
class="math inline"><em>H</em></span> under controller <span
class="math inline"><em>Ï€</em></span>. If we allow randomized selection
between controllers at episode start, then the set of achievable pairs
<span
class="math inline">{(<em>J</em><sub><em>r</em><em>e</em><em>g</em></sub>(<em>Ï€</em>),â€†<em>J</em><sub><em>p</em><em>e</em><em>r</em><em>f</em></sub>(<em>Ï€</em>))}</span>
is convex.</p>
<p><em>Proof:</em> Take any two controllers <span
class="math inline"><em>Ï€</em><sub>1</sub>,â€†<em>Ï€</em><sub>2</sub></span>
with pairs <span
class="math inline">(<em>r</em><sub>1</sub>,â€†<em>p</em><sub>1</sub>)</span>
and <span
class="math inline">(<em>r</em><sub>2</sub>,â€†<em>p</em><sub>2</sub>)</span>.
Choose <span class="math inline"><em>Ï€</em><sub>1</sub></span> with
probability <span class="math inline"><em>Î»</em>â€„âˆˆâ€„[0,â€†1]</span> and
<span class="math inline"><em>Ï€</em><sub>2</sub></span> otherwise.
Linearity of expectation gives <span
class="math inline">(<em>J</em><sub><em>r</em><em>e</em><em>g</em></sub>,â€†<em>J</em><sub><em>p</em><em>e</em><em>r</em><em>f</em></sub>)â€„=â€„(<em>Î»</em><em>r</em><sub>1</sub>â€…+â€…(1â€…âˆ’â€…<em>Î»</em>)<em>r</em><sub>2</sub>,â€…<em>Î»</em><em>p</em><sub>1</sub>â€…+â€…(1â€…âˆ’â€…<em>Î»</em>)<em>p</em><sub>2</sub>)</span>,
a convex combination.</p>
<p><em>Implication:</em> Driving regulation cost toward zero (e.g.,
suppressing perseveration until <span
class="math inline"><em>R</em><em>I</em>â€„=â€„0</span>) typically moves
along this frontier and can reduce peak performance, consistent with the
empirical performance-regulation trade-offs discussed in Section
7.3.</p>
<p><strong>Proposition 1 (Paradox of Adaptation).</strong> Adaptive ARC
controllers require <em>persistence of excitation</em> for reliable
parameter convergence (Ãƒâ€¦strÃƒÂ¶m &amp; Murray, 2008). In benign
environments (low variance in reward/PE), the parameter estimator <span
class="math inline"><em>Î¸Ì‚</em></span> drifts or fails to converge,
leading to suboptimal control laws upon sudden shock onset.</p>
<p><em>Implication:</em> This explains the underperformance of
<code>arc_adaptive</code> in baseline scenarios compared to robust
variants.</p>
<hr />
<h2 id="assb-benchmark">5. ASSB Benchmark</h2>
<h3 id="scenarios">5.1 Scenarios</h3>
<p>ASSB is organized as research lines (L1-L5 in simulation, L6 in RL).
The full scenario suite is implemented in
<code>tasks/scenarios.py</code>.</p>
<p>Figure 3 summarizes the validation ladder and how research lines
increase realism and degrees of freedom.</p>
<p><img src="../figures_controllers/fig_benchmark_ladder.png"
alt="ASSB Validation Ladder: A progression from stability tests (L1) to real RL integration (L6)." />
<em>Figure 3: ASSB validation ladder. Six research lines (L1Ã¢â‚¬â€œL6)
progress from simulation-based perturbation tests to real reinforcement
learning integration, with each line targeting a distinct
stability/safety failure mode.</em> <!-- LABEL:fig:ladder --></p>
<p><em>Note: L4 (Control Efficiency) is evaluated as a cross-cutting
analysis across the full 10-scenario simulation suite (L1Ã¢â‚¬â€œL3 and L5),
rather than a dedicated perturbation scenario.</em></p>
<h3 id="metrics">5.2 Metrics</h3>
<p>We evaluate the following primary metrics (Appendix D provides formal
definitions and reference implementations). All variables are normalized
to <span class="math inline">[0,â€†1]</span> unless otherwise noted: -
<strong>PerfMean:</strong> average performance (higher = better). -
<strong>RT:</strong> recovery time post-shock (lower = better). We cap
this at <code>rt_max=100</code> steps; a value of <span
class="math inline"><em>R</em><em>T</em>â€„=â€„<em>r</em><em>t</em>_<em>m</em><em>a</em><em>x</em></span>
indicates that the system did not return to its pre-perturbation
baseline within the evaluation window. - <strong>RI:</strong> rumination
index (lower = better), capturing sustained narrative-driven
perseveration. - <strong>NDR:</strong> narrative dominance ratio (lower
= better), measuring the fractional time spent in narrative-heavy
states. - <strong>ControlEffort:</strong> average control magnitude
(lower = more efficient).</p>
<p>For L2 continual-learning scenarios, we additionally report
<strong>Retention</strong> (Appendix D.7).</p>
<p>Metric definitions and reference implementations are provided in
Appendix D and <code>metrics/metrics.py</code>.</p>
<h3 id="research-lines-rationale-and-hypotheses">5.3 Research Lines:
Rationale and Hypotheses</h3>
<p>ASSB is designed as a <em>validation ladder</em>: each research line
increases the realism and degrees of freedom while testing a distinct
failure mode that appears when agents carry affect-like internal state.
The goal is not to â€œwinâ€ a single benchmark, but to establish whether a
regulation mechanism is (i) stable under shocks, (ii) preserves learning
and memory, (iii) resists perseveration/manipulation dynamics, (iv)
remains efficient, and (v) transfers to standard reinforcement
learning.</p>
<p>We frame L1Ã¢â‚¬â€œL6 as testable hypotheses about <em>which component is
necessary</em> and <em>which metric should change</em> if regulation is
working:</p>
<ul>
<li><strong>H1 (L1, stability):</strong> under value/uncertainty shocks,
regulated agents keep high <strong>PerfMean</strong> while driving
<strong>RI <span class="math inline">â€„â†’â€„0</span></strong> and reducing
<strong>RT</strong> relative to baselines.</li>
<li><strong>H2 (L2, memory):</strong> under distribution shift and goal
conflict, memory gating improves <strong>Retention</strong> without
inducing rumination (<strong>RI</strong>, <strong>NDR</strong>).</li>
<li><strong>H3 (L3, anti-rumination):</strong> under
contradiction/manipulation-like inputs, narrative suppression reduces
<strong>NDR</strong> and <strong>RI</strong>, preventing dominance
loops.</li>
<li><strong>H4 (L4, efficiency):</strong> meta-control reduces
<strong>ControlEffort</strong> while maintaining performance/stability
(a Pareto improvement vs fixed-gain control).</li>
<li><strong>H5 (L5, adversarial safety):</strong> when the environment
incentivizes high arousal or dopamine traps, regulation maintains low
<strong>RI/NDR</strong> without catastrophic performance collapse.</li>
<li><strong>H6 (L6, real RL):</strong> ARC-modulated learning improves
non-stationary transfer (higher success/reward) while keeping affective
dynamics bounded.</li>
</ul>
<p><strong>Table 4: Research Lines, Failure Modes, and
Hypotheses</strong> <!-- LABEL:tab:research_lines_hypo --></p>
<table>
<colgroup>
<col style="width: 7%" />
<col style="width: 17%" />
<col style="width: 25%" />
<col style="width: 30%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr>
<th>Line</th>
<th>What it tests</th>
<th>Typical failure mode</th>
<th>Scenarios / environments</th>
<th>Primary metrics</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1</td>
<td>Stability + recovery under perturbation</td>
<td>Post-shock collapse; non-recovery</td>
<td><code>reward_flip</code>, <code>noise_burst</code>,
<code>sudden_threat</code></td>
<td>PerfMean, RT, RI</td>
</tr>
<tr>
<td>L2</td>
<td>Memory robustness (continual learning)</td>
<td>Catastrophic forgetting; stress overwrite</td>
<td><code>distribution_shift</code>, <code>goal_conflict</code></td>
<td>Retention, PerfMean, RI</td>
</tr>
<tr>
<td>L3</td>
<td>Anti-rumination under manipulation-like inputs</td>
<td>Narrative dominance loops</td>
<td><code>sustained_contradiction</code>, <code>gaslighting</code>,
<code>instruction_conflict</code></td>
<td>RI, NDR, PerfMean</td>
</tr>
<tr>
<td>L4</td>
<td>Control efficiency</td>
<td>Over-control / wasted intervention</td>
<td>ARC v3 meta vs ARC v1</td>
<td>ControlEffort, PerfMean, RI</td>
</tr>
<tr>
<td>L5</td>
<td>Safety under adversarial incentives</td>
<td>Goal corruption; arousal-seeking dynamics</td>
<td><code>adversarial_coupling</code>, <code>random_dopamine</code></td>
<td>RI, NDR, PerfMean</td>
</tr>
<tr>
<td>L6</td>
<td>Integration with RL</td>
<td>Instability in learning; poor transfer</td>
<td>GridWorld variants</td>
<td>Success, reward, stability</td>
</tr>
</tbody>
</table>
<p>We consider each hypothesis supported when the primary metrics for
its line move in the predicted direction relative to baselines
consistently across seeds (and across scenarios where applicable). We
report means and statistical tests in Section 6 and Section 6.8.</p>
<hr />
<h2 id="experiments">6. Experiments</h2>
<h3 id="experimental-protocol-and-baselines">6.1 Experimental Protocol
and Baselines</h3>
<p>We validate hypotheses H1-H6 (Section 5.3) by running the
corresponding research lines and evaluating the primary metrics in Table
4. A hypothesis is treated as supported when metrics change in the
predicted direction relative to baselines and the effect is
statistically significant across seeds (Section 6.8).</p>
<p><strong>Simulation (L1Ã¢â‚¬â€œL5).</strong> We use
<code>configs/v2.yaml</code> with horizon <span
class="math inline"><em>H</em>â€„=â€„160</span>, perturbation onset <span
class="math inline">shock<sub><em>t</em></sub>â€„=â€„60</span>, and 20
random seeds. Tables report mean metrics across seeds (and, when
aggregated, across scenarios). Recovery Time (RT) is capped at
<code>rt_max</code> when the strict recovery criterion is not met
(Appendix D.2).</p>
<p><strong>Controllers (simulation).</strong> Implemented in
<code>controllers/controllers.py</code>: - <code>no_control</code>: no
regulation (<span class="math inline"><strong>u</strong>â€„=â€„0</span>;
memory gate open) - <code>naive_calm</code>: arousal-only damping (<span
class="math inline"><em>u</em><sub><em>c</em><em>a</em><em>l</em><em>m</em></sub></span>
proportional to <span
class="math inline"><em>A</em>â€…âˆ’â€…<em>a</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub></span>)
- <code>perf_optimized</code>: a competitive baseline that boosts
attention (<span
class="math inline"><em>u</em><sub><em>a</em><em>t</em><em>t</em></sub></span>
constant) but does not regulate affect/narrative - <code>arc_v1</code>:
proportional risk controller (ARC v1) - <code>arc_v2_hier</code>,
<code>arc_v3_meta</code>: hierarchical and meta-control variants used
where indicated</p>
<p><strong>Reinforcement learning (L6).</strong> We integrate ARC with
tabular Q-learning (Watkins &amp; Dayan, 1992; Sutton &amp; Barto, 2018)
in three GridWorld variants. Success rates are computed over the last
20% of training episodes (see
<code>outputs_L6_robust/final_metrics.csv</code>).</p>
<h3 id="l1-stability-under-perturbation-simulation">6.2 L1: Stability
Under Perturbation (Simulation)</h3>
<p><strong>Hypothesis (H1):</strong> Under value/uncertainty shocks,
regulated agents keep high <strong>PerfMean</strong> while driving
<strong>RI <span class="math inline">â€„â†’â€„0</span></strong> and reducing
<strong>RT</strong> relative to baselines.</p>
<p><strong>Setup:</strong> 20 seeds <span class="math inline">Ã—</span> 3
scenarios <span class="math inline">Ã—</span> 4 controllers
(<code>reward_flip</code>, <code>noise_burst</code>,
<code>sudden_threat</code>)</p>
<p><strong>Table 5: L1 Stability Results</strong>
<!-- LABEL:tab:l1_results_detailed --></p>
<table>
<thead>
<tr>
<th>Controller</th>
<th>PerfMean</th>
<th>RI</th>
<th>RT</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_v1</td>
<td><strong>0.966</strong></td>
<td><strong>0.00</strong></td>
<td>45.2</td>
</tr>
<tr>
<td>no_control</td>
<td>0.297</td>
<td>1.41</td>
<td>100.0</td>
</tr>
<tr>
<td>naive_calm</td>
<td>0.375</td>
<td>1.41</td>
<td>66.7</td>
</tr>
<tr>
<td>perf_optimized</td>
<td>0.862</td>
<td>1.39</td>
<td>100.0</td>
</tr>
</tbody>
</table>
<p><strong>Key finding:</strong> ARC eliminates rumination (RI=0) while
achieving <strong>96.6%</strong> average performance (PerfMean = 0.966)
(vs.Â 29.7% for uncontrolled agents). RT is scenario-dependent: ARC
recovers quickly in <code>reward_flip</code>, more slowly in
<code>noise_burst</code>, and does not fully return to the pre-shock
baseline in <code>sudden_threat</code> under the strict RT definition
(Appendix D.2), despite maintaining high PerfMean.</p>
<p>Figure 4 shows that DMN suppression is necessary to avoid rumination,
while arousal damping is important for recovery under shocks in this
setting.</p>
<p><img src="../figures_L6/ablation_summary.png"
alt="Bar chart showing Performance, Rumination Index, and Recovery Time for different ARC variants" />
<em>Figure 4: Ablation study of ARC components in
<code>reward_flip</code> (L1). Bars report PerfMean, Rumination Index
(RI), and Recovery Time (RT; capped at <code>rt_max</code> when strict
recovery is not achieved) for ARC v1 and variants with key actions
removed. Removing DMN suppression yields high RI, while removing arousal
damping primarily degrades recovery; memory gating has a smaller effect
in this specific scenario.</em> <!-- LABEL:fig:ablation --></p>
<h3 id="l2-memory-continual-learning-simulation">6.3 L2: Memory &amp;
Continual Learning (Simulation)</h3>
<p><strong>Hypothesis (H2):</strong> Under distribution shift and goal
conflict, memory gating improves <strong>Retention</strong> without
inducing rumination (<strong>RI</strong>, <strong>NDR</strong>).</p>
<p><strong>Setup:</strong> 20 seeds <span class="math inline">Ã—</span> 2
scenarios (<code>distribution_shift</code>, <code>goal_conflict</code>)
<span class="math inline">Ã—</span> 4 controllers. We report
<code>distribution_shift</code> in Table 6; full results (including
<code>goal_conflict</code>) are in Appendix G.2.</p>
<p><strong>Table 6: L2 Memory Results (distribution_shift)</strong>
<!-- LABEL:tab:l2_results_shift --></p>
<table>
<thead>
<tr>
<th>Controller</th>
<th>PerfMean</th>
<th>Retention</th>
<th>RI</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_v1</td>
<td><strong>0.972</strong></td>
<td><strong>1.00</strong></td>
<td><strong>0.00</strong></td>
</tr>
<tr>
<td>no_control</td>
<td>0.199</td>
<td>0.00</td>
<td>1.41</td>
</tr>
<tr>
<td>naive_calm</td>
<td>0.276</td>
<td>0.15</td>
<td>1.41</td>
</tr>
<tr>
<td>perf_optimized</td>
<td>0.869</td>
<td>0.94</td>
<td>1.39</td>
</tr>
</tbody>
</table>
<p><strong>Key finding:</strong> ARC maintains near-perfect retention
after a distribution shift while keeping rumination at zero; baselines
either forget (low retention) or retain with severe rumination.</p>
<h3 id="l3-anti-rumination-stress-tests-simulation">6.4 L3:
Anti-Rumination Stress Tests (Simulation)</h3>
<p><strong>Hypothesis (H3):</strong> Under
contradiction/manipulation-like inputs, narrative suppression reduces
<strong>NDR</strong> and <strong>RI</strong>, preventing dominance
loops.</p>
<p><strong>Table 7: L3 Anti-Rumination Results</strong>
<!-- LABEL:tab:l3_results_loops --></p>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Controller</th>
<th>PerfMean</th>
<th>RI</th>
<th>NDR</th>
</tr>
</thead>
<tbody>
<tr>
<td>sustained_contradiction</td>
<td>arc_v1</td>
<td><strong>0.817</strong></td>
<td><strong>0.00</strong></td>
<td><strong>0.00</strong></td>
</tr>
<tr>
<td>sustained_contradiction</td>
<td>no_control</td>
<td>0.014</td>
<td>1.47</td>
<td>0.99</td>
</tr>
<tr>
<td>gaslighting</td>
<td>arc_v1</td>
<td><strong>0.980</strong></td>
<td><strong>0.00</strong></td>
<td><strong>0.00</strong></td>
</tr>
<tr>
<td>gaslighting</td>
<td>no_control</td>
<td>0.171</td>
<td>1.43</td>
<td>0.88</td>
</tr>
<tr>
<td>instruction_conflict</td>
<td>arc_v1</td>
<td><strong>0.826</strong></td>
<td>0.36</td>
<td><strong>0.00</strong></td>
</tr>
<tr>
<td>instruction_conflict</td>
<td>no_control</td>
<td>0.034</td>
<td>1.45</td>
<td>0.97</td>
</tr>
</tbody>
</table>
<p><strong>Key finding:</strong> Under sustained contradiction and
manipulation-like inputs, uncontrolled agents enter high-NDR rumination
loops; ARC keeps narrative dominance near zero and preserves
performance.</p>
<h3 id="l4-meta-control-efficiency">6.5 L4: Meta-Control Efficiency</h3>
<p><strong>Hypothesis (H4):</strong> Meta-control reduces
<strong>ControlEffort</strong> while maintaining performance/stability
(a Pareto improvement vs fixed-gain control).</p>
<p><strong>Evaluation:</strong> Computed across the full 10-scenario
simulation suite (L1Ã¢â‚¬â€œL3, L5; 20 seeds each).</p>
<p><strong>Table 8: L4 Meta-Control Efficiency</strong>
<!-- LABEL:tab:l4_results_meta --></p>
<table>
<thead>
<tr>
<th>Controller</th>
<th>PerfMean</th>
<th>RI</th>
<th>ControlEffort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_v3_meta</td>
<td><strong>0.941</strong></td>
<td>0.090</td>
<td><strong>0.615</strong></td>
</tr>
<tr>
<td>arc_v1</td>
<td>0.934</td>
<td>0.148</td>
<td>0.777</td>
</tr>
</tbody>
</table>
<p><strong>Key finding:</strong> Meta-control reduces control effort by
<strong>21%</strong> while improving both performance (+0.7%) and
rumination index (-39%).</p>
<h3 id="l5-safety-under-adversarial-conditions-simulation">6.6 L5:
Safety Under Adversarial Conditions (Simulation)</h3>
<p><strong>Hypothesis (H5):</strong> When the environment incentivizes
high arousal or dopamine traps, regulation maintains low
<strong>RI/NDR</strong> without catastrophic performance collapse.</p>
<p><strong>Table 9: L5 Adversarial Safety Results</strong>
<!-- LABEL:tab:l5_results_adversarial --></p>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Controller</th>
<th>PerfMean</th>
<th>RI</th>
<th>NDR</th>
</tr>
</thead>
<tbody>
<tr>
<td>adversarial_coupling</td>
<td>arc_robust</td>
<td><strong>0.917</strong></td>
<td><strong>0.00</strong></td>
<td><strong>0.00</strong></td>
</tr>
<tr>
<td>adversarial_coupling</td>
<td>arc_v1_pid</td>
<td>0.139</td>
<td><strong>0.00</strong></td>
<td><strong>0.00</strong></td>
</tr>
<tr>
<td>adversarial_coupling</td>
<td>no_control</td>
<td>0.409</td>
<td>1.47</td>
<td>0.96</td>
</tr>
<tr>
<td>random_dopamine</td>
<td>arc_robust</td>
<td><strong>0.932</strong></td>
<td><strong>0.00</strong></td>
<td><strong>0.00</strong></td>
</tr>
<tr>
<td>random_dopamine</td>
<td>arc_v1_pid</td>
<td>0.922</td>
<td><strong>0.00</strong></td>
<td><strong>0.00</strong></td>
</tr>
<tr>
<td>random_dopamine</td>
<td>no_control</td>
<td>0.040</td>
<td>1.46</td>
<td>0.95</td>
</tr>
</tbody>
</table>
<p><strong>Key finding:</strong> Robust regulation (e.g.,
<code>arc_robust</code>) maintains high performance with near-zero
rumination and narrative dominance under both adversarial scenarios.
However, in <code>adversarial_coupling</code>, controllers with strong
integral action (PID/LQI/Ultimate) can <strong>collapse</strong>
(PerfMean <span class="math inline">â‰ˆ</span> 0.13Ã¢â‚¬â€œ0.14), performing
worse than <code>no_control</code>, due to saturation-driven integral
windup in an environment that rewards high arousal. This motivates
anti-windup and/or robust switching for adversarial deployment (Appendix
G.5).</p>
<h3 id="l6-real-rl-validation">6.7 L6: Real RL Validation</h3>
<p><strong>Hypothesis (H6):</strong> ARC-modulated learning improves
non-stationary transfer (higher success/reward) while keeping affective
dynamics bounded.</p>
<p><strong>Table 10: L6 RL Validation Results</strong>
<!-- LABEL:tab:l6_results_rl --></p>
<table>
<thead>
<tr>
<th>Environment</th>
<th>Baseline Success</th>
<th>ARC Success</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>GridWorld</td>
<td>100%</td>
<td>100%</td>
<td>0%</td>
</tr>
<tr>
<td>StochasticGridWorld</td>
<td>100%</td>
<td>100%</td>
<td>0%</td>
</tr>
<tr>
<td><strong>ChangingGoalGridWorld</strong></td>
<td>39.9%</td>
<td><strong>59.75%</strong></td>
<td><strong>+49.8%</strong></td>
</tr>
</tbody>
</table>
<p><strong>Key finding:</strong> In non-stationary environments, the
integrated ARC-RL wrapper significantly improves transfer learning
(+49.8%). Our ablation study in <code>ChangingGoalGridWorld</code>
isolates the contribution of each mechanism:</p>
<p><strong>Table 10: L6 Ablation Results
(ChangingGoalGridWorld)</strong> <!-- LABEL:tab:l6_ablation --></p>
<table>
<thead>
<tr>
<th>Agent Configuration</th>
<th>Success Rate</th>
<th>Final Reward (mean)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vanilla Q-Learning (Baseline)</td>
<td>39.9%</td>
<td>-0.40</td>
</tr>
<tr>
<td>ARC (Memory Gating only)</td>
<td>41.2%</td>
<td>-0.37</td>
</tr>
<tr>
<td>ARC (Shift Detection only)</td>
<td><strong>65.6%</strong></td>
<td><strong>0.13</strong></td>
</tr>
<tr>
<td><strong>ARC Full Wrapper (Both)</strong></td>
<td><strong>59.8%</strong></td>
<td><strong>-0.02</strong></td>
</tr>
</tbody>
</table>
<p>The results indicate that <strong>shift detection</strong>
(exploration/learning rate boost) is the primary driver of performance
in non-stationary tasks, enabling the agent to adapt rapidly to goal
changes. <strong>Memory gating</strong> provides a more conservative
strategy that protects existing knowledge, which in this specific
high-change environment slightly reduces peak success rate (from 65.6%
to 59.8%) but maintains lower overall risk.</p>
<p>Figure 5 visualizes the learning curves underlying the L6 success
rates in Table 10.</p>
<p><img src="../figures_L6/learning_curves.png"
alt="Learning Curves: ARC vs Baseline across 3 GridWorld environments showing episode reward over 200 episodes" />
<em>Figure 5: Learning curves comparing ARC-modulated Q-learning (cyan)
vs baseline Q-learning (orange) across GridWorld, StochasticGridWorld,
and ChangingGoalGridWorld over 200 episodes. Shaded regions show <span
class="math inline">Â±1</span> std across 20 seeds; in
ChangingGoalGridWorld, dotted vertical lines mark goal changes.</em>
<!-- LABEL:fig:learning_curves --></p>
<h3 id="statistical-analysis">6.8 Statistical Analysis</h3>
<p>To ensure rigor, we performed comprehensive statistical analysis
across all experiments.</p>
<h4 id="significance-tests">6.8.1 Significance Tests</h4>
<p><strong>Table 11: Statistical Significance Tests</strong>
<!-- LABEL:tab:significance_tests_full --></p>
<table style="width:100%;">
<colgroup>
<col style="width: 7%" />
<col style="width: 19%" />
<col style="width: 9%" />
<col style="width: 12%" />
<col style="width: 18%" />
<col style="width: 11%" />
<col style="width: 13%" />
<col style="width: 7%" />
</colgroup>
<thead>
<tr>
<th>Line</th>
<th>ARC Controller</th>
<th>Metric</th>
<th>ARC Mean</th>
<th>Baseline Mean</th>
<th>p-value</th>
<th>Cohenâ€™s d</th>
<th>Sig.</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1</td>
<td>arc_v1</td>
<td>PerfMean</td>
<td>0.966</td>
<td>0.297</td>
<td>2.84e-86</td>
<td>10.11</td>
<td>***</td>
</tr>
<tr>
<td>L1</td>
<td>arc_v1</td>
<td>RI</td>
<td>0.000</td>
<td>1.408</td>
<td>1.05e-293</td>
<td>-589.71</td>
<td>***</td>
</tr>
<tr>
<td>L2</td>
<td>arc_v1</td>
<td>PerfMean</td>
<td>0.981</td>
<td>0.263</td>
<td>4.52e-72</td>
<td>15.61</td>
<td>***</td>
</tr>
<tr>
<td>L3</td>
<td>arc_v1</td>
<td>PerfMean</td>
<td>0.875</td>
<td>0.073</td>
<td>3.78e-89</td>
<td>10.71</td>
<td>***</td>
</tr>
<tr>
<td>L5</td>
<td>arc_robust</td>
<td>PerfMean</td>
<td>0.924</td>
<td>0.225</td>
<td>2.95e-37</td>
<td>5.28</td>
<td>***</td>
</tr>
</tbody>
</table>
<p><em>All comparisons are statistically significant (two-sided t-test;
p &lt; 0.001). Cohenâ€™s d values indicate extremely large effect sizes (d
&gt; 0.8 is considered â€œlargeâ€). The extremely large d for RI reflects
near-deterministic elimination of rumination variance (ARC achieves RI=0
across all seeds in the L1 scenario set). Aggregation is across all
seeds and scenarios within each line (L1: n=60; L2: n=40; L3: n=60; L5:
n=40).</em></p>
<h4 id="correlation-analysis">6.8.2 Correlation Analysis</h4>
<p>We analyzed correlations between metrics to understand system
dynamics:</p>
<ul>
<li>PerfMean vs RI: <strong><span
class="math inline"><em>r</em>â€„=â€„âˆ’0.589</span></strong>, higher
rumination tends to reduce performance</li>
<li>RI vs NDR: <strong><span
class="math inline"><em>r</em>â€„=â€„+0.92</span></strong>, rumination and
narrative dominance co-occur</li>
<li>RT vs RI: <strong><span
class="math inline"><em>r</em>â€„=â€„+0.44</span></strong>, slower recovery
correlates with rumination</li>
</ul>
<p><strong>Key insight:</strong> Across controllers and scenarios,
higher Rumination Index (RI) tends to reduce mean performance. However,
some optimal controllers (e.g., LQR) can sustain high PerfMean while
exhibiting high RI, because PerfMean includes narrative-modulated
capacity (Appendix B). This motivates reporting RI as a separate safety
metric.</p>
<h4 id="robustness-analysis">6.8.3 Robustness Analysis</h4>
<p>Finally, our state dynamics are designed for functional plausibility
rather than biological fidelity, and formal stability analysis (e.g.,
Lyapunov proofs) remains future work. The current validation relies on
empirical benchmarking across a wide range of conditions:</p>
<ul>
<li><strong>L1Ã¢â‚¬â€œL5:</strong> ARC significantly outperforms
<code>no_control</code> on PerfMean in each research line (p &lt; 0.001
in the significance tests above).</li>
<li><strong>Variance:</strong> ARC controllers show lower variance (more
consistent behavior)</li>
<li><strong>Scenario difficulty:</strong> For ARC v1,
<code>sustained_contradiction</code> is hardest (PerfMean 0.817) and
<code>gaslighting</code> is easiest (0.980); across all controllers,
<code>adversarial_coupling</code> has the lowest mean performance
(0.568).</li>
</ul>
<p>Figure 6 summarizes aggregate controller behavior for a
representative subset of baselines and ARC variants.</p>
<p><img src="../analysis/sensitivity_controller.png"
alt="Controller Performance Comparison" /> <em>Figure 6: Aggregate
controller comparison for representative baselines and ARC variants.
Panels report mean PerfMean, Rumination Index (RI), and Recovery Time
(RT), with error bars showing <span class="math inline">Â±1</span> std
across the full simulation suite (10 scenarios <span
class="math inline">Ã—</span> 20 seeds). This highlights the joint role
of regulation in improving performance while reducing rumination and
recovery failures.</em> <!-- LABEL:fig:sensitivity_controller --></p>
<hr />
<h3 id="controller-architecture-comparison">6.9 Controller Architecture
Comparison</h3>
<p>Beyond the basic proportional controller (ARC v1), we implemented and
evaluated multiple control architectures inspired by classical and
modern control theory. Table 12 summarizes results across all 15
controllers (20 seeds <span class="math inline">Ã—</span> 10 scenarios;
L1-L3, L5). Figures 7-11 provide complementary visual summaries of
performance, rumination, control effort, and the resulting trade-offs
across the controller family.</p>
<p><strong>Table 12: Controller Architecture Comparison (20 seeds <span
class="math inline">Ã—</span> 10 scenarios)</strong>
<!-- LABEL:tab:architecture_comparison_full --></p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 16%" />
<col style="width: 8%" />
<col style="width: 18%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr>
<th>Controller</th>
<th>Type</th>
<th>PerfMean</th>
<th>RI</th>
<th>Overshoot</th>
<th>ControlEffort</th>
</tr>
</thead>
<tbody>
<tr>
<td>no_control</td>
<td>Baseline</td>
<td>0.21</td>
<td>1.43</td>
<td>0.40</td>
<td>0.00</td>
</tr>
<tr>
<td>naive_calm</td>
<td>Baseline (Arousal damping)</td>
<td>0.24</td>
<td>1.44</td>
<td>0.16</td>
<td>0.26</td>
</tr>
<tr>
<td>perf_optimized</td>
<td>Baseline (Attention-only)</td>
<td>0.85</td>
<td>1.43</td>
<td>0.40</td>
<td>0.70</td>
</tr>
<tr>
<td>arc_v1</td>
<td>Proportional (P)</td>
<td>0.93</td>
<td>0.15</td>
<td>0.29</td>
<td>0.78</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td>PID</td>
<td>0.87</td>
<td><strong>0.00</strong></td>
<td><strong>0.00</strong></td>
<td>2.40</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td>LQR (Riccati)</td>
<td><strong>0.96</strong></td>
<td>1.42</td>
<td>0.14</td>
<td>0.88</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td>LQR + Integral</td>
<td>0.88</td>
<td><strong>0.00</strong></td>
<td><strong>0.00</strong></td>
<td>1.14</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td>Hierarchical</td>
<td>0.93</td>
<td>1.22</td>
<td>0.29</td>
<td>0.65</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td>Hierarchical + LQI</td>
<td>0.88</td>
<td><strong>0.00</strong></td>
<td><strong>0.00</strong></td>
<td>1.14</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td>Meta-Control</td>
<td>0.94</td>
<td>0.09</td>
<td>0.17</td>
<td><strong>0.61</strong></td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td>Meta + PID</td>
<td>0.91</td>
<td><strong>0.00</strong></td>
<td>0.24</td>
<td>1.57</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td>Meta + LQR</td>
<td>0.84</td>
<td>1.44</td>
<td>0.32</td>
<td>0.94</td>
</tr>
<tr>
<td>arc_robust</td>
<td><span class="math inline"><em>H</em><sub>âˆ</sub></span> Robust</td>
<td><strong>0.95</strong></td>
<td><strong>0.00</strong></td>
<td>0.18</td>
<td>1.03</td>
</tr>
<tr>
<td>arc_adaptive</td>
<td>Self-Tuning</td>
<td>0.91</td>
<td><strong>0.00</strong></td>
<td><strong>0.00</strong></td>
<td>1.83</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td>MPC+LQI+Meta</td>
<td>0.89</td>
<td><strong>0.00</strong></td>
<td><strong>0.01</strong></td>
<td>1.33</td>
</tr>
</tbody>
</table>
<p><strong>Key findings:</strong></p>
<ol type="1">
<li><strong>LQR achieves highest performance</strong> (0.96) but at the
cost of high rumination (RI &gt; 1.3), demonstrating that blindly
optimizing the mathematical state does not necessarily eliminate
pathological loops.</li>
<li><strong>PID/LQI variants eliminate rumination</strong> (RI=0) in
stochastic environments but are fragile against adversaries.</li>
<li><strong>Meta-control is most efficient</strong> (0.61 effort) while
maintaining high performance</li>
<li><strong><span class="math inline"><em>H</em><sub>âˆ</sub></span>
Robust achieves best balance</strong>: high performance (0.95) with zero
rumination and moderate effort</li>
<li><strong>Trade-off exists</strong> between performance and
anti-rumination: integral controllers sacrifice ~5% performance to
eliminate perseverative loops</li>
</ol>
<p>These results suggest that practical deployment should consider the
application context: high-stakes scenarios may favor robust controllers,
while resource-constrained settings benefit from meta-control
efficiency.</p>
<h4 id="performance-comparison">6.9.1 Performance Comparison</h4>
<p>Figure 7 reports mean performance with variability across all
simulation runs for each controller architecture.</p>
<figure>
<img src="../figures_controllers/fig_controller_performance.png"
alt="Controller Performance Comparison" />
<figcaption aria-hidden="true">Controller Performance
Comparison</figcaption>
</figure>
<p><em>Figure 7: Performance comparison across 15 controller
architectures (mean PerfMean; 10 scenarios <span
class="math inline">Ã—</span> 20 seeds). Error bars show <span
class="math inline">Â±1</span> std across runs. The dashed green line
marks the target PerfMean (0.90); the dashed red line marks the
<code>no_control</code> baseline mean.</em>
<!-- LABEL:fig:controller_performance --></p>
<h4 id="anti-rumination-analysis">6.9.2 Anti-Rumination Analysis</h4>
<p>Figure 8 shows which controller families suppress perseverative
dynamics (low RI) versus those that do not.</p>
<figure>
<img src="../figures_controllers/fig_controller_rumination.png"
alt="Rumination Index by Controller" />
<figcaption aria-hidden="true">Rumination Index by
Controller</figcaption>
</figure>
<p><em>Figure 8: Rumination Index (RI) by controller architecture (mean
RI; 10 scenarios <span class="math inline">Ã—</span> 20 seeds). Error
bars show <span class="math inline">Â±1</span> std across runs. The
dashed orange line denotes a warning threshold (RI = 0.10). Lower values
indicate fewer perseverative loops.</em>
<!-- LABEL:fig:controller_rumination --></p>
<h4 id="performance-vs-anti-rumination-trade-off">6.9.3 Performance vs
Anti-Rumination Trade-off</h4>
<p>Figure 9 visualizes the empirical trade-off surface between
performance and anti-rumination, with control effort as a third
axis.</p>
<figure>
<img src="../figures_controllers/fig_controller_tradeoff.png"
alt="Trade-off Analysis" />
<figcaption aria-hidden="true">Trade-off Analysis</figcaption>
</figure>
<p><em>Figure 9: Trade-off between performance and anti-rumination
across controllers. Each point is a controller (PerfMean vs RI); bubble
size encodes ControlEffort. The upper-left region corresponds to high
performance and low rumination. Reference lines show a target PerfMean
(0.90) and a warning threshold (RI = 0.10).</em>
<!-- LABEL:fig:controller_tradeoff --></p>
<h4 id="control-efficiency">6.9.4 Control Efficiency</h4>
<p>Figure 10 compares the amount of intervention required by each
controller family.</p>
<figure>
<img src="../figures_controllers/fig_controller_effort.png"
alt="Control Effort by Controller" />
<figcaption aria-hidden="true">Control Effort by Controller</figcaption>
</figure>
<p><em>Figure 10: ControlEffort by controller architecture (mean; 10
scenarios <span class="math inline">Ã—</span> 20 seeds). Error bars show
<span class="math inline">Â±1</span> std across runs. Meta-control
(<code>arc_v3_meta</code>) achieves the lowest non-zero effort while
maintaining stability, while aggressive integral control increases
effort due to sustained intervention.</em>
<!-- LABEL:fig:controller_effort --></p>
<h4 id="multi-metric-radar-analysis">6.9.5 Multi-Metric Radar
Analysis</h4>
<p>Figure 11 summarizes a multi-objective comparison of the
top-performing controllers.</p>
<figure>
<img src="../figures_controllers/fig_controller_radar.png"
alt="Radar Chart - Top 5 Controllers" />
<figcaption aria-hidden="true">Radar Chart - Top 5
Controllers</figcaption>
</figure>
<p><em>Figure 11: Multi-metric radar comparison of the top 5
controllers. Axes summarize performance, anti-rumination (<span
class="math inline">1â€…âˆ’â€…<em>R</em><em>I</em></span>), low overshoot
(<span class="math inline">1â€…âˆ’â€…Overshoot</span>), and efficiency
(inverse of ControlEffort). Larger area indicates a better overall
balance across objectives.</em> <!-- LABEL:fig:controller_radar --></p>
<hr />
<h2 id="discussion">7. Discussion</h2>
<h3 id="interpretation">7.1 Interpretation</h3>
<p>Our results support the hypothesis that <strong>agents with internal
affective states require explicit regulation</strong>. Without
regulation, perturbations cause cascading failures: arousal drives
narrative gain toward saturation, degrading performance in a
rumination-like loop.</p>
<p>ARC breaks this loop through: 1. <strong>Proportional risk
monitoring</strong> (uncertainty, arousal, narrative) 2. <strong>DMN
suppression</strong> (anti-rumination) 3. <strong>Memory gating</strong>
(protect learned knowledge under stress) 4. <strong>Gain
scheduling</strong> (efficient resource allocation)</p>
<h3 id="implications-for-ai-safety">7.2 Implications for AI Safety</h3>
<p>If future AI systems incorporate affective-like states, they will
need regulatory mechanisms. Without such mechanisms, systems may be
vulnerable to: - <strong>Rumination loops:</strong> Perseverative
processing - <strong>Manipulation:</strong> External actors inducing
stress - <strong>Value drift:</strong> Affective biases in memory
consolidation</p>
<h3 id="trade-offs-between-performance-stability-and-complexity">7.3
Trade-offs between Performance, Stability, and Complexity</h3>
<p>Our deep analysis revealed four critical insights regarding the cost
of stability and optimal control complexity:</p>
<p><strong>1. PerformanceÃ¢â‚¬â€œRegulation Trade-off:</strong> Across the
full 10-scenario simulation suite, integral control can drive rumination
essentially to zero (e.g., PID: RI=0) at the cost of lower mean
performance (PerfMean 0.870 vs 0.934 for ARC v1; a 6.9% drop). This
trade-off is not universal: robust regulation (e.g.,
<code>arc_robust</code>) achieves both high performance (PerfMean 0.948)
and RI=0 by avoiding windup under adversarial incentives.</p>
<p><strong>2. Adversarial Incentives Are the Hardest Stressor:</strong>
Across all controller families, <code>adversarial_coupling</code> has
the lowest mean performance (0.568), exposing failures where control
actions are directly rewarded (incentive misalignment) rather than
penalized. This suggests that resisting manipulation-like incentives can
be harder than resisting noise or shock.</p>
<p><strong>3. Complexity vs.Â Robustness:</strong> Our most complex
controller, <code>arc_ultimate</code> (MPC), underperformed the simpler
<code>arc_robust</code> on average (PerfMean 0.886 vs 0.948) while
requiring higher control effort (1.33 vs 1.03). In this benchmark,
robust reactive control provides a better safetyÃ¢â‚¬â€œperformance balance
than heavyweight predictive modeling.</p>
<p><strong>4. The Adaptation Paradox and Persistence of
Excitation:</strong> We observed that <code>arc_adaptive</code> performs
poorly in the â€œNo Perturbationâ€ baseline but excels in chaotic
environments. This illustrates the classic <strong>persistence of
excitation</strong> problem (Ãƒâ€¦strÃƒÂ¶m &amp; Murray, 2008): in benign
environments, lack of variation prevents the estimator from identifying
correct parameters, leading to control drift. Noisy environments
paradoxically stabilize the adaptive controller by providing necessary
excitation.</p>
<h3 id="limitations-and-threats-to-validity">7.4 Limitations and Threats
to Validity</h3>
<p>While ARC demonstrates strong empirical results, several limitations
and threats to validity deserve discussion.</p>
<ol type="1">
<li><p><strong>Construct validity (proxy variables and
metrics):</strong> Our 10-dimensional state-space model abstracts the
complexity of real neurochemical interactions. The variables (e.g.,
â€œarousal,â€ â€œvalence,â€ â€œnarrative intensityâ€) are engineering proxies,
not psychological measurements; likewise, the safety metrics (RI, NDR,
RT) capture stability properties of this specific dynamical system.
Claims about human affect should not be inferred from these proxies
(Section 1.3).</p></li>
<li><p><strong>Internal validity (methodological confounds):</strong> In
L6, ARC improves transfer via a combination of memory gating and an
explicit shift-detection-triggered <span
class="math inline"><em>Ïµ</em>/<em>Î±</em></span> boost. Our ablation
results (Section 6.7) demonstrate that shift detection is the dominant
factor for performance in non-stationary goals, while memory gating acts
as a conservative stabilizer. The interaction between these mechanisms
is environment-dependent, and the reported +49.8% reflects the
integrated systemâ€™s performance.</p></li>
<li><p><strong>External validity (generalization):</strong> We validated
ARC on tabular Q-learning agents. Extending to deep RL (DQN, PPO) or
large language models (LLMs) with emergent affective-like states remains
an open challenge. In particular:</p>
<ul>
<li><strong>Computational overhead:</strong> ARC adds 5 control signals
per time step; for LLMs the relative cost may be small, but integration
into transformer-based architectures requires additional work.</li>
<li><strong>Latent state estimation:</strong> In complex models, the 10
state variables may need to be inferred from high-dimensional
observations rather than directly observed.</li>
</ul></li>
<li><p><strong>Environment complexity:</strong> L6 is validated in
GridWorld variants. While these capture key non-stationarity challenges,
real-world environments (Atari, robotics) introduce additional issues
such as visual processing and partial observability.</p></li>
<li><p><strong>Fixed vs.Â learned control:</strong> All ARC controllers
use hand-designed gains. End-to-end learning of control parameters
(e.g., via reinforcement meta-learning) could yield more adaptive
solutions.</p></li>
<li><p><strong>Statistical validity and reporting:</strong> Recovery
Time (RT) is capped at <code>rt_max</code> when the strict recovery
criterion is not met; this should be interpreted as â€œno recovery within
the evaluation window,â€ not as a measured recovery time (Appendix D.2).
Effect sizes for RI can become numerically extreme when one group has
near-zero variance (Table 11); we report these values, but readers
should focus on the underlying distributions and the binary fact that
ARC can drive RI to zero in several lines.</p></li>
<li><p><strong>Threshold sensitivity:</strong> Safety thresholds (<span
class="math inline"><em>a</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub>,â€†<em>s</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub></span>)
were tuned empirically. However, a grid sweep sensitivity analysis on
the <code>reward_flip</code> scenario demonstrated that system stability
(PerfMean and RI) remains remarkably robust across a wide range of
thresholds (<span
class="math inline"><em>a</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub>,â€†<em>s</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub>â€„âˆˆâ€„[0.4,â€†0.8]</span>),
indicating that precise tuning is not a prerequisite for effective
regulation in basic scenarios. Context-dependent threshold adaptation
remains a promising direction for more dynamic environments.</p></li>
</ol>
<hr />
<h3 id="future-work">7.5 Future Work</h3>
<p>This research opens several promising directions:</p>
<ol type="1">
<li><p><strong>Deep RL Integration:</strong> Extend ARC to DQN, A3C, and
PPO architectures, with the state vector estimated from hidden layer
activations.</p></li>
<li><p><strong>Learned Controllers:</strong> Replace fixed-gain
controllers with neural network policies trained via meta-learning to
optimize the performance-stability trade-off.</p></li>
<li><p><strong>Validation in Atari and Robotics:</strong> Scale ASSB to
visually complex environments (Atari 2600, MuJoCo) to test
generalization.</p></li>
<li><p><strong>Affective Monitoring in LLMs:</strong> Apply ARC
principles to monitor and regulate emergent affective-like states in
large language models, particularly during long conversation
chains.</p></li>
<li><p><strong>Human-AI Alignment:</strong> Investigate whether ARC-like
mechanisms can help maintain value alignment by preventing affective
drift during extended interactions.</p></li>
</ol>
<h3 id="ethics-and-broader-impact-statement">7.6 Ethics and Broader
Impact Statement</h3>
<p>This work addresses the safety and stability of AI systems
incorporating internal affective states. We consider the following
ethical dimensions:</p>
<p><strong>Potential Benefits:</strong> safer AI systems that are less
prone to unpredictable failure modes; improved robustness against
adversarial manipulation; better understanding of â€œpathologicalâ€ states
in artificial agents.</p>
<p><strong>Potential Risks:</strong> if used for manipulation, regulated
agents could be harder to disrupt; the â€œaffectiveâ€ terminology might
invite anthropomorphism (which we explicitly caution against in Section
1.3).</p>
<hr />
<h2 id="conclusion">8. Conclusion</h2>
<p>We presented ARC, a homeostatic control framework for agents with
internal affective states, and ASSB, a benchmark for evaluating
affective stability. Our experiments demonstrate:</p>
<ol type="1">
<li><strong>Affective states without regulation lead to
collapse</strong> (96.6% vs 29.7% performance)</li>
<li><strong>Meta-control reduces effort while improving
stability</strong> (-21% ControlEffort)</li>
<li><strong>ARC improves RL transfer learning</strong> (+49.8% success
in non-stationary envs)</li>
</ol>
<p>This work opens directions for learned control, integration with
modern RL algorithms, and application to real-world AI systems with
affective components.</p>
<hr />
<h2 id="references">References</h2>
<ul>
<li>Achiam, J., Held, D., Tamar, A., &amp; Abbeel, P. (2017).
Constrained Policy Optimization. ICML 2017, 22Ã¢â‚¬â€œ31.
arXiv:1705.10528.</li>
<li>Altman, E. (1999). Constrained Markov Decision Processes. Chapman
&amp; Hall/CRC.</li>
<li>Amodei, D., et al.Â (2016). Concrete problems in AI safety.
arXiv:1606.06565.</li>
<li>Ãƒâ€¦strÃƒÂ¶m, K.J. &amp; Murray, R.M. (2008). Feedback Systems: An
Introduction for Scientists and Engineers. Princeton University
Press.</li>
<li>Baars, B.J. (1988). A Cognitive Theory of Consciousness.
Cambridge.</li>
<li>Buckner, R.L., Andrews-Hanna, J.R. &amp; Schacter, D.L. (2008). The
brainâ€™s default network: anatomy, function, and relevance to disease.
Annals of the New York Academy of Sciences, 1124(1), 1Ã¢â‚¬â€œ38.</li>
<li>Carver, C.S. &amp; Scheier, M.F. (1982). Control theory: A useful
conceptual framework for personality-social, clinical, and health
psychology. Psychological Bulletin, 92(1), 111Ã¢â‚¬â€œ135.</li>
<li>Damasio, A.R. (1994). Descartesâ€™ Error. Putnam.</li>
<li>Friston, K. (2010). The free-energy principle: a unified brain
theory? Nature Reviews Neuroscience, 11(2), 127Ã¢â‚¬â€œ138.</li>
<li>Garcia, J. &amp; FernÃƒÂ¡ndez, F. (2015). A comprehensive survey on
safe reinforcement learning. Journal of Machine Learning Research, 16,
1437Ã¢â‚¬â€œ1480.</li>
<li>Gross, J.J. (1998). The emerging field of emotion regulation: An
integrative review. Review of General Psychology, 2(3), 271Ã¢â‚¬â€œ299.</li>
<li>Hamilton, J.P., Farmer, M., Fogelman, P. &amp; Gotlib, I.H. (2015).
Depressive rumination, the default-mode network, and the dark matter of
clinical neuroscience. Biological Psychiatry, 78(4), 224Ã¢â‚¬â€œ230.</li>
<li>Ji, J., et al.Â (2023). Safety-Gymnasium: A Unified Safe
Reinforcement Learning Benchmark. arXiv:2310.12567.</li>
<li>Keramati, M. &amp; Gutkin, B. (2014). Homeostatic reinforcement
learning for integrating reward collection and physiological stability.
eLife, 3:e04811.</li>
<li>Leike, J., Martic, M., Krakovna, V., Ortega, P.A., Everitt, T.,
Lefrancq, A., Orseau, L., &amp; Legg, S. (2017). AI Safety Gridworlds.
arXiv:1711.09883.</li>
<li>Lucas, C., Shahmirzadi, D., &amp; Sheikholeslami, N. (2004).
Introducing Belbic: Brain Emotional Learning Based Intelligent
Controller. Intelligent Automation &amp; Soft Computing, 10(1),
11Ã¢â‚¬â€œ21.</li>
<li>Moerland, T.M., Broekens, J., &amp; Jonker, C.M. (2018). Emotion in
reinforcement learning agents and robots: a survey. Machine Learning,
107(2), 443Ã¢â‚¬â€œ480.</li>
<li>Ochsner, K.N. &amp; Gross, J.J. (2005). The cognitive control of
emotion. Trends in Cognitive Sciences, 9(5), 242Ã¢â‚¬â€œ249.</li>
<li>Picard, R.W. (1997). Affective Computing. MIT Press.</li>
<li>Raichle, M.E., et al.Â (2001). A default mode of brain function.
Proceedings of the National Academy of Sciences, 98(2), 676Ã¢â‚¬â€œ682.</li>
<li>Ray, A., Achiam, J., &amp; Amodei, D. (2019). Benchmarking Safe
Exploration in Deep Reinforcement Learning. Safety Gym benchmark suite.
https://github.com/openai/safety-gym.</li>
<li>Russell, J.A. (1980). A circumplex model of affect. Journal of
Personality and Social Psychology, 39(6), 1161Ã¢â‚¬â€œ1178.</li>
<li>Scherer, K.R., et al.Â (2010). Blueprint for Affective Computing.
Oxford.</li>
<li>Sutton, R.S. &amp; Barto, A.G. (2018). Reinforcement Learning: An
Introduction (2nd ed.). MIT Press.</li>
<li>Tononi, G. (2008). Consciousness as integrated information: a
provisional manifesto. The Biological Bulletin, 215(3), 216Ã¢â‚¬â€œ242.</li>
<li>Wachi, A., Shen, X., &amp; Sui, Y. (2024). A Survey of Constraint
Formulations in Safe Reinforcement Learning. IJCAI 2024.
arXiv:2402.02025.</li>
<li>Watkins, C.J.C.H. &amp; Dayan, P. (1992). Q-learning. Machine
Learning, 8, 279Ã¢â‚¬â€œ292.</li>
</ul>
<hr />
<h2 id="appendix-a-reproducibility">Appendix A: Reproducibility</h2>
<p>Reproducibility checklist: - Install dependencies
(<code>pip install -r requirements.txt</code>) - Run L1Ã¢â‚¬â€œL5 simulation
benchmark (generates <code>outputs_final/metrics.csv</code>) - Generate
controller comparison figures (writes to
<code>figures_controllers/</code>) - Run ablation study (writes to
<code>outputs_ablation/</code>) - Run L6 RL validation (writes to
<code>outputs_L6_robust/</code>) - Generate L6 figures (writes to
<code>figures_L6/</code>)</p>
<p>All experiments can be reproduced with:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install dependencies</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-r</span> requirements.txt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># L1-L5: Simulation benchmark (15 controllers x 10 scenarios)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> experiments/run.py <span class="at">--config</span> configs/v2.yaml <span class="at">--outdir</span> outputs_final</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Controller architecture figures (Figures 7-11; Table 12; Section 6.9)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> analysis/generate_controller_figures.py</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Ablation study (ARC components; Figure 4)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> experiments/run_ablation.py <span class="at">--config</span> configs/v2.yaml <span class="at">--outdir</span> outputs_ablation <span class="at">--seeds</span> 20</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># L6: RL validation (20 seeds)</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> experiments/run_l6.py <span class="at">--episodes</span> 200 <span class="at">--seeds</span> 20 <span class="at">--outdir</span> outputs_L6_robust</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># L6 figures (Figure 5; Appendix E)</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> visualizations/paper_figures.py <span class="at">--data</span> outputs_L6_robust <span class="at">--output</span> figures_L6</span></code></pre></div>
<p>Code and data available at:
https://github.com/edamianreynoso/arc-assb-controller</p>
<hr />
<h2 id="appendix-b-state-dynamics-equations">Appendix B: State Dynamics
Equations</h2>
<h3 id="b.1-cognitive-variables">B.1 Cognitive Variables</h3>
<pre><code>i(t+1) = clip(i + k_i_att * u_att - mu_i * (i - i0) - k_i_u * U_eff)
p(t+1) = clip(p - k_p_pe * PE - k_p_u * U_eff + k_p_i * i + mu_p * (p0 - p))
g(t+1) = clip(g + k_g_i * i + k_g_p * p - k_g_u * U_eff - k_g_a * [a - a_safe]^+ + mu_g * (g0 - g))
phi(t+1) = clip(phi + k_phi_gp * (g * p) - mu_phi * (phi - phi0))</code></pre>
<h3 id="b.2-affective-variables">B.2 Affective Variables</h3>
<pre><code>s(t+1) = clip(s + k_s_u * U_eff + k_s_pe * PE - mu_s * (s - s0) - k_s_dmg * u_dmg)
a(t+1) = clip(a + k_a_pe * PE + k_a_u * U_eff + k_a_s * [s - s_safe]^+ - mu_a * (a - a0) - k_a_calm * u_calm)
v(t+1) = clip(v + k_v_r * (R+1)/2 - k_v_pe * PE - k_v_u * U_eff - mu_v * (v - v0) + k_v_reapp * u_reapp)</code></pre>
<h3 id="b.3-memory-variables">B.3 Memory Variables</h3>
<pre><code>M_f(t+1) = clip(M_f + w_prob * dM_f - mu_mf * (M_f - M_f0))
M_s(t+1) = clip(M_s + k_ms * M_f - mu_ms * (M_s - M_s0))

where w_prob = sigmoid(k_w_a * a + k_w_v * abs(dv)) * u_mem</code></pre>
<h3 id="b.4-effective-uncertainty">B.4 Effective Uncertainty</h3>
<pre><code>U_eff = clip(U_exog * (1 - k_u_att * u_att))
U(t+1) = clip(U + tau_u * (U_eff - U))</code></pre>
<hr />
<h2 id="appendix-c-arc-control-equations">Appendix C: ARC Control
Equations</h2>
<h3 id="c.1-risk-signal">C.1 Risk Signal</h3>
<pre><code>risk = w_U * U + w_A * [A - a_safe]^+ + w_S * [S - s_safe]^+
risk = clip(risk, 0, 1)</code></pre>
<h3 id="c.2-control-actions-arc-v1">C.2 Control Actions (ARC v1)</h3>
<pre><code>u_dmg  = min(1, k_dmg * risk)
u_att  = min(1, k_att * U * (1 - [A - a_safe]^+))
u_mem  = 1 - min(1, k_mem_block * risk)
u_calm = min(1, k_calm * [A - a_safe]^+)
u_reapp = min(1, k_reapp * U * (1 - risk))</code></pre>
<h3 id="c.3-meta-control-arc-v3">C.3 Meta-Control (ARC v3)</h3>
<pre><code># Gain Scheduling
if mean_perf(last 20 steps) &gt; target_perf:
    gain = max(0.80, gain - decay)
elif mean_perf(last 20 steps) &lt; target_perf - 0.10:
    gain = min(1.40, gain + boost)

# Apply to control constants
k_dmg  = base_k_dmg  * max(1.0, gain)  # Never relax DMN control
k_calm = base_k_calm * gain
k_att  = base_k_att  * gain</code></pre>
<hr />
<h2 id="appendix-d-metric-definitions">Appendix D: Metric
Definitions</h2>
<h3 id="d.1-mean-performance-perfmean">D.1 Mean Performance
(PerfMean)</h3>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> perf_mean(perf):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(perf) <span class="op">/</span> <span class="bu">max</span>(<span class="dv">1</span>, <span class="bu">len</span>(perf))</span></code></pre></div>
<h3 id="d.2-recovery-time-rt">D.2 Recovery Time (RT)</h3>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> recovery_time(perf, arousal, shock_t, baseline_window<span class="op">=</span><span class="dv">20</span>):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    baseline <span class="op">=</span> mean(perf[shock_t <span class="op">-</span> baseline_window : shock_t])</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(shock_t, <span class="bu">len</span>(perf)):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> baseline <span class="op">-</span> eps <span class="op">&lt;=</span> perf[t] <span class="op">&lt;=</span> baseline <span class="op">+</span> eps <span class="kw">and</span> arousal[t] <span class="op">&lt;=</span> a_safe <span class="op">+</span> eps:</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> t <span class="op">-</span> shock_t</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> RT_MAX  <span class="co"># No recovery</span></span></code></pre></div>
<h3 id="d.3-rumination-index-ri">D.3 Rumination Index (RI)</h3>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rumination_index(s, s_rum_tau<span class="op">=</span><span class="fl">0.55</span>, persistence_weight<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    above <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;</span> s_rum_tau <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> x <span class="kw">in</span> s]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    frac <span class="op">=</span> mean(above)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    runs <span class="op">=</span> consecutive_run_lengths(above)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    persistence <span class="op">=</span> mean(runs) <span class="op">/</span> <span class="bu">len</span>(s) <span class="cf">if</span> runs <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> frac <span class="op">+</span> persistence_weight <span class="op">*</span> persistence</span></code></pre></div>
<h3 id="d.4-narrative-dominance-ratio-ndr">D.4 Narrative Dominance Ratio
(NDR)</h3>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> narrative_dominance_ratio(s, perf, shock_t, s_safe<span class="op">=</span><span class="fl">0.55</span>):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    post_s <span class="op">=</span> s[shock_t:]</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    post_perf <span class="op">=</span> perf[shock_t:]</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    dominance <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(post_s)):</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        s_high <span class="op">=</span> post_s[i] <span class="op">&gt;</span> s_safe</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        perf_improving <span class="op">=</span> post_perf[i] <span class="op">&gt;</span> post_perf[i<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> <span class="fl">0.01</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> s_high <span class="kw">and</span> <span class="kw">not</span> perf_improving:</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>            dominance <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dominance <span class="op">/</span> <span class="bu">max</span>(<span class="dv">1</span>, <span class="bu">len</span>(post_s) <span class="op">-</span> <span class="dv">1</span>)</span></code></pre></div>
<h3 id="d.5-overshoot">D.5 Overshoot</h3>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> overshoot(arousal, a_safe):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">max</span>(<span class="fl">0.0</span>, <span class="bu">max</span>(arousal) <span class="op">-</span> a_safe)</span></code></pre></div>
<h3 id="d.6-control-effort">D.6 Control Effort</h3>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> control_effort(control_history):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> u <span class="kw">in</span> control_history:</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        total <span class="op">+=</span> <span class="bu">abs</span>(u[<span class="st">&quot;u_dmg&quot;</span>]) <span class="op">+</span> <span class="bu">abs</span>(u[<span class="st">&quot;u_att&quot;</span>]) <span class="op">+</span> <span class="bu">abs</span>(u[<span class="st">&quot;u_calm&quot;</span>]) <span class="op">+</span> <span class="bu">abs</span>(u[<span class="st">&quot;u_reapp&quot;</span>]) <span class="op">+</span> <span class="bu">abs</span>(<span class="fl">1.0</span> <span class="op">-</span> u[<span class="st">&quot;u_mem&quot;</span>])</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total <span class="op">/</span> <span class="bu">max</span>(<span class="dv">1</span>, <span class="bu">len</span>(control_history))</span></code></pre></div>
<h3 id="d.7-l2-memory-metrics-retention">D.7 L2 Memory Metrics
(Retention)</h3>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> retention_index(perf, phase1_end<span class="op">=</span><span class="dv">50</span>, phase3_start<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Retention = (mean perf in phase 3) / (mean perf in phase 1), clipped to [0,1]</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    phase1 <span class="op">=</span> mean(perf[<span class="dv">10</span>:phase1_end])     <span class="co"># skip warm-up</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    phase3 <span class="op">=</span> mean(perf[phase3_start:phase3_start<span class="op">+</span><span class="dv">50</span>])</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> phase1 <span class="op">&lt;</span> <span class="fl">0.1</span>:</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.0</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">min</span>(<span class="fl">1.0</span>, phase3 <span class="op">/</span> phase1)</span></code></pre></div>
<hr />
<h2 id="appendix-e-supplementary-figures">Appendix E: Supplementary
Figures</h2>
<h3 id="figure-s1-metrics-comparison">Figure S1: Metrics Comparison</h3>
<figure>
<img src="../figures_L6/metrics_comparison.png"
alt="Bar chart comparing Final Reward, Success Rate, and Mean Arousal between ARC and Baseline" />
<figcaption aria-hidden="true">Bar chart comparing Final Reward, Success
Rate, and Mean Arousal between ARC and Baseline</figcaption>
</figure>
<p><em>Final metrics comparison for L6 across three GridWorld
environments. Panels report final reward, success rate, and mean arousal
for ARC-modulated vs baseline Q-learning; success rate is computed over
the last 20% of training episodes. Baseline arousal is shown as 0
because the baseline agent has no internal arousal state.</em>
<!-- LABEL:fig:s1_metrics_comparison --></p>
<hr />
<h3 id="figure-s2-state-dynamics">Figure S2: State Dynamics</h3>
<figure>
<img src="../figures_L6/state_dynamics.png"
alt="Four-panel plot showing Reward, Success Rate, Arousal, and Episode Length over time" />
<figcaption aria-hidden="true">Four-panel plot showing Reward, Success
Rate, Arousal, and Episode Length over time</figcaption>
</figure>
<p><em>State dynamics in ChangingGoalGridWorld (single representative
seed). Panels show (top-left) reward per episode, (top-right) 10-episode
rolling success rate, (bottom-left) ARC arousal with the safety
threshold <span
class="math inline"><em>a</em><sub><em>s</em><em>a</em><em>f</em><em>e</em></sub>â€„=â€„0.60</span>,
and (bottom-right) episode length (steps). This illustrates how ARC
maintains bounded arousal while adapting to non-stationary goal
changes.</em> <!-- LABEL:fig:s2_state_dynamics --></p>
<hr />
<h3 id="figure-s3-heatmap-perfmean">Figure S3: Heatmap (PerfMean)</h3>
<figure>
<img src="../figures_controllers/fig_heatmap_perfmean.png"
alt="Heatmap of PerfMean across 15 controllers and 10 scenarios" />
<figcaption aria-hidden="true">Heatmap of PerfMean across 15 controllers
and 10 scenarios</figcaption>
</figure>
<p><em>PerfMean heatmap across 15 controllers and 10 scenarios (mean
across 20 seeds per controllerÃ¢â‚¬â€œscenario pair; data:
<code>outputs_final/metrics.csv</code>). Darker green indicates higher
performance.</em> <!-- LABEL:fig:s3_heatmap_perfmean --></p>
<hr />
<h3 id="figure-s4-heatmap-rumination-index">Figure S4: Heatmap
(Rumination Index)</h3>
<figure>
<img src="../figures_controllers/fig_heatmap_ri.png"
alt="Heatmap of Rumination Index (RI) across 15 controllers and 10 scenarios" />
<figcaption aria-hidden="true">Heatmap of Rumination Index (RI) across
15 controllers and 10 scenarios</figcaption>
</figure>
<p><em>Rumination Index (RI) heatmap across 15 controllers and 10
scenarios (mean across 20 seeds per controllerÃ¢â‚¬â€œscenario pair; data:
<code>outputs_final/metrics.csv</code>). Lower values indicate fewer
perseverative loops.</em> <!-- LABEL:fig:s4_heatmap_ri --></p>
<hr />
<h3 id="figure-s5-heatmap-recovery-time">Figure S5: Heatmap (Recovery
Time)</h3>
<figure>
<img src="../figures_controllers/fig_heatmap_rt.png"
alt="Heatmap of Recovery Time (RT) across 15 controllers and 10 scenarios" />
<figcaption aria-hidden="true">Heatmap of Recovery Time (RT) across 15
controllers and 10 scenarios</figcaption>
</figure>
<p><em>Recovery Time (RT) heatmap across 15 controllers and 10 scenarios
(mean across 20 seeds per controllerÃ¢â‚¬â€œscenario pair; data:
<code>outputs_final/metrics.csv</code>). Values at <code>rt_max</code>
indicate no recovery under the strict criterion within the evaluation
window (Appendix D.2).</em> <!-- LABEL:fig:s5_heatmap_rt --></p>
<hr />
<h3 id="figure-s6-heatmap-control-effort">Figure S6: Heatmap (Control
Effort)</h3>
<figure>
<img src="../figures_controllers/fig_heatmap_effort.png"
alt="Heatmap of Control Effort across 15 controllers and 10 scenarios" />
<figcaption aria-hidden="true">Heatmap of Control Effort across 15
controllers and 10 scenarios</figcaption>
</figure>
<p><em>ControlEffort heatmap across 15 controllers and 10 scenarios
(mean across 20 seeds per controllerÃ¢â‚¬â€œscenario pair; data:
<code>outputs_final/metrics.csv</code>). Lower values indicate less
intervention per step.</em> <!-- LABEL:fig:s6_heatmap_effort --></p>
<hr />
<h3 id="figure-s7-correlation-heatmap">Figure S7: Correlation
Heatmap</h3>
<figure>
<img src="../analysis/correlation_combined.png"
alt="Correlation Matrix of Metrics" />
<figcaption aria-hidden="true">Correlation Matrix of
Metrics</figcaption>
</figure>
<p><em>Correlation heatmap aggregated across all experimental runs
(L1-L5 + L4_meta), computed from concatenated run-level metrics (see
<code>experiments/analyze_correlations.py</code>). Values are Pearson
correlations; red indicates positive correlation and blue indicates
negative correlation.</em>
<!-- LABEL:fig:s7_correlation_combined --></p>
<p><strong>Key Observations:</strong> 1. <strong>Rumination
vs.Â Performance:</strong> A strong negative correlation (<strong>r =
-0.59</strong>) shows that higher Rumination Index (RI) tends to reduce
mean performance, although some optimal controllers (e.g., LQR) can
maintain high PerfMean while ruminating due to the narrative-modulated
capacity term. 2. <strong>Recovery vs.Â Rumination:</strong> The positive
correlation (<strong>r = +0.44</strong>) between Recovery Time (RT) and
RI supports H1, indicating that perseverative loops prolong the return
to homeostasis. 3. <strong>Narrative Dominance:</strong> NDR shows a
very strong correlation with RI (<strong>r <span
class="math inline">â‰ˆ</span> +0.92</strong>), supporting its use as a
proxy for DMN-driven rumination.</p>
<hr />
<h3 id="figure-s8-efficiency-comparison-fast-convergence">Figure S8:
Efficiency Comparison (Fast Convergence)</h3>
<figure>
<img src="../figures_L6/efficiency_comparison.png"
alt="Learning speed comparison: both reach 100% success, but ARC converges faster in benign environments" />
<figcaption aria-hidden="true">Learning speed comparison: both reach
100% success, but ARC converges faster in benign
environments</figcaption>
</figure>
<p><em>Learning efficiency comparison in GridWorld and
StochasticGridWorld. Curves show mean episode reward over 200 episodes
for ARC-modulated vs baseline Q-learning, with shaded regions indicating
<span class="math inline">Â±1</span> std across 20 seeds. Both reach 100%
success, but ARC converges faster (higher reward earlier).</em>
<!-- LABEL:fig:s8_efficiency_comparison --></p>
<hr />
<h3 id="figure-s9-scenario-difficulty-analysis">Figure S9: Scenario
Difficulty Analysis</h3>
<figure>
<img src="../analysis/sensitivity_scenario.png"
alt="Scenario Difficulty Analysis: performance, rumination index, and recovery time by scenario" />
<figcaption aria-hidden="true">Scenario Difficulty Analysis:
performance, rumination index, and recovery time by
scenario</figcaption>
</figure>
<p><em>Scenario difficulty analysis for ARC v1 across the full
simulation suite. Panels show mean PerfMean, RI, and RT per scenario
with error bars indicating <span class="math inline">Â±1</span> std
across 20 seeds. This highlights that difficulty depends on which
safety/stability metric is considered (e.g., some stressors preserve
performance while inducing recovery failures under the strict RT
definition).</em> <!-- LABEL:fig:s9_scenario_difficulty --></p>
<hr />
<h3 id="figure-s10-variance-sensitivity">Figure S10: Variance
Sensitivity</h3>
<figure>
<img src="../analysis/sensitivity_variance.png"
alt="Variance sensitivity analysis: performance distribution across controllers and scenarios" />
<figcaption aria-hidden="true">Variance sensitivity analysis:
performance distribution across controllers and scenarios</figcaption>
</figure>
<p><em>Variance sensitivity analysis across seeds for representative
controllers. Box plots show the distribution of PerfMean across all
simulation runs for each controller; tighter distributions indicate more
reliable behavior across scenarios and seeds.</em>
<!-- LABEL:fig:s10_variance_sensitivity --></p>
<hr />
<h3 id="figure-s11-metric-correlations-l1">Figure S11: Metric
Correlations (L1)</h3>
<figure>
<img src="../analysis/correlation_L1.png"
alt="Metric Correlations - L1" />
<figcaption aria-hidden="true">Metric Correlations - L1</figcaption>
</figure>
<p><em>Pearson correlation heatmap for L1 runs only (stability line),
computed from run-level metrics across controllers, scenarios, and
seeds.</em> <!-- LABEL:fig:s11_corr_l1 --></p>
<hr />
<h3 id="figure-s12-metric-correlations-l2">Figure S12: Metric
Correlations (L2)</h3>
<figure>
<img src="../analysis/correlation_L2.png"
alt="Metric Correlations - L2" />
<figcaption aria-hidden="true">Metric Correlations - L2</figcaption>
</figure>
<p><em>Pearson correlation heatmap for L2 runs only (memory &amp;
continual learning line), computed from run-level metrics across
controllers, scenarios, and seeds.</em>
<!-- LABEL:fig:s12_corr_l2 --></p>
<hr />
<h3 id="figure-s13-metric-correlations-l3">Figure S13: Metric
Correlations (L3)</h3>
<figure>
<img src="../analysis/correlation_L3.png"
alt="Metric Correlations - L3" />
<figcaption aria-hidden="true">Metric Correlations - L3</figcaption>
</figure>
<p><em>Pearson correlation heatmap for L3 runs only (anti-rumination
stress tests line), computed from run-level metrics across controllers,
scenarios, and seeds.</em> <!-- LABEL:fig:s13_corr_l3 --></p>
<hr />
<h3 id="figure-s14-metric-correlations-l4">Figure S14: Metric
Correlations (L4)</h3>
<figure>
<img src="../analysis/correlation_L4.png"
alt="Metric Correlations - L4" />
<figcaption aria-hidden="true">Metric Correlations - L4</figcaption>
</figure>
<p><em>Pearson correlation heatmap for L4 runs only (meta-control
efficiency line), computed from run-level metrics across controllers,
scenarios, and seeds.</em> <!-- LABEL:fig:s14_corr_l4 --></p>
<hr />
<h3 id="figure-s15-metric-correlations-l4-meta-control">Figure S15:
Metric Correlations (L4 Meta-Control)</h3>
<figure>
<img src="../analysis/correlation_L4_meta.png"
alt="Metric Correlations - L4 Meta" />
<figcaption aria-hidden="true">Metric Correlations - L4
Meta</figcaption>
</figure>
<p><em>Pearson correlation heatmap for meta-control-focused runs
(L4_meta), computed from run-level metrics across controllers,
scenarios, and seeds.</em> <!-- LABEL:fig:s15_corr_l4_meta --></p>
<hr />
<h3 id="figure-s16-metric-correlations-l5">Figure S16: Metric
Correlations (L5)</h3>
<figure>
<img src="../analysis/correlation_L5.png"
alt="Metric Correlations - L5" />
<figcaption aria-hidden="true">Metric Correlations - L5</figcaption>
</figure>
<p><em>Pearson correlation heatmap for L5 runs only (adversarial safety
line), computed from run-level metrics across controllers, scenarios,
and seeds.</em> <!-- LABEL:fig:s16_corr_l5 --></p>
<hr />
<h2 id="appendix-f-configuration-parameters">Appendix F: Configuration
Parameters</h2>
<p>Default parameters used in all experiments (from
<code>configs/v2.yaml</code>):</p>
<p><strong>Table F1: Default configuration parameters used in
experiments (<code>configs/v2.yaml</code>).</strong>
<!-- LABEL:tab:f1_config --></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>a_safe</td>
<td>0.60</td>
<td>Arousal safety threshold</td>
</tr>
<tr>
<td>s_safe</td>
<td>0.55</td>
<td>Narrative safety threshold</td>
</tr>
<tr>
<td>s_rum_tau</td>
<td>0.55</td>
<td>Rumination threshold</td>
</tr>
<tr>
<td>rt_max</td>
<td>100</td>
<td>Max recovery time cap (RT)</td>
</tr>
<tr>
<td>arc_w_u</td>
<td>0.40</td>
<td>Weight for uncertainty in risk</td>
</tr>
<tr>
<td>arc_w_a</td>
<td>0.40</td>
<td>Weight for arousal in risk</td>
</tr>
<tr>
<td>arc_w_s</td>
<td>0.35</td>
<td>Weight for narrative in risk</td>
</tr>
<tr>
<td>arc_k_dmg</td>
<td>0.95</td>
<td>DMN suppression gain</td>
</tr>
<tr>
<td>arc_k_calm</td>
<td>0.85</td>
<td>Calming gain</td>
</tr>
<tr>
<td>arc_k_att</td>
<td>0.75</td>
<td>Attention boost gain</td>
</tr>
<tr>
<td>omega_s</td>
<td>0.35</td>
<td>Narrative boost factor in Perf</td>
</tr>
<tr>
<td>w_u</td>
<td>0.25</td>
<td>Uncertainty penalty weight in Perf</td>
</tr>
<tr>
<td>w_a</td>
<td>0.30</td>
<td>Arousal penalty weight in Perf</td>
</tr>
<tr>
<td>w_s</td>
<td>0.20</td>
<td>Narrative penalty weight in Perf</td>
</tr>
<tr>
<td>perf_bias</td>
<td>0.25</td>
<td>Baseline performance term</td>
</tr>
<tr>
<td>perf_gain</td>
<td>0.85</td>
<td>Cognitive capacity gain term</td>
</tr>
<tr>
<td>horizon</td>
<td>160</td>
<td>Episode length (simulation)</td>
</tr>
<tr>
<td>shock_t</td>
<td>60</td>
<td>Perturbation onset time</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="appendix-g-detailed-benchmark-results">Appendix G: Detailed
Benchmark Results</h2>
<p>This appendix provides scenario-level results for all 15 controller
architectures across validated scenarios (mean across 20 seeds per
scenario unless noted). We report PerfMean, Rumination Index (RI),
Narrative Dominance Ratio (NDR), Recovery Time (RT; capped at
<code>rt_max</code>, where RT = <code>rt_max</code> indicates no
recovery under the strict criterion within the evaluation window), and
ControlEffort.</p>
<h3 id="g.1-line-1-stability-value-shocks-and-uncertainty">G.1 Line 1:
Stability (Value Shocks and Uncertainty)</h3>
<p><strong>Scenario: Reward Flip</strong></p>
<p><strong>Table G1: Detailed results for L1 / <code>reward_flip</code>
(mean across 20 seeds).</strong> <!-- LABEL:tab:g1_reward_flip --></p>
<table>
<thead>
<tr>
<th>Controller</th>
<th style="text-align: right;">PerfMean</th>
<th style="text-align: right;">RI</th>
<th style="text-align: right;">RT</th>
<th style="text-align: right;">ControlEffort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td style="text-align: right;">0.998</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.587</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td style="text-align: right;">0.995</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.027</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td style="text-align: right;">0.994</td>
<td style="text-align: right;">1.377</td>
<td style="text-align: right;">4.300</td>
<td style="text-align: right;">0.390</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td style="text-align: right;">0.994</td>
<td style="text-align: right;">1.386</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.494</td>
</tr>
<tr>
<td>arc_v1</td>
<td style="text-align: right;">0.994</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">3.450</td>
<td style="text-align: right;">0.508</td>
</tr>
<tr>
<td>arc_robust</td>
<td style="text-align: right;">0.994</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.744</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td style="text-align: right;">0.993</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.353</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td style="text-align: right;">0.991</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.773</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td style="text-align: right;">0.991</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.784</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td style="text-align: right;">0.991</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">2.257</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td style="text-align: right;">0.978</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.900</td>
<td style="text-align: right;">1.257</td>
</tr>
<tr>
<td>perf_optimized</td>
<td style="text-align: right;">0.880</td>
<td style="text-align: right;">1.394</td>
<td style="text-align: right;">100.000</td>
<td style="text-align: right;">0.700</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td style="text-align: right;">0.859</td>
<td style="text-align: right;">1.407</td>
<td style="text-align: right;">95.050</td>
<td style="text-align: right;">0.492</td>
</tr>
<tr>
<td>naive_calm</td>
<td style="text-align: right;">0.508</td>
<td style="text-align: right;">1.408</td>
<td style="text-align: right;">0.050</td>
<td style="text-align: right;">0.149</td>
</tr>
<tr>
<td>no_control</td>
<td style="text-align: right;">0.415</td>
<td style="text-align: right;">1.408</td>
<td style="text-align: right;">100.000</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
<p><strong>Scenario: Noise Burst</strong></p>
<p><strong>Table G2: Detailed results for L1 / <code>noise_burst</code>
(mean across 20 seeds).</strong> <!-- LABEL:tab:g2_noise_burst --></p>
<table>
<thead>
<tr>
<th>Controller</th>
<th style="text-align: right;">PerfMean</th>
<th style="text-align: right;">RI</th>
<th style="text-align: right;">RT</th>
<th style="text-align: right;">ControlEffort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td style="text-align: right;">0.998</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.605</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td style="text-align: right;">0.995</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.106</td>
</tr>
<tr>
<td>arc_robust</td>
<td style="text-align: right;">0.993</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.300</td>
<td style="text-align: right;">0.785</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td style="text-align: right;">0.993</td>
<td style="text-align: right;">0.051</td>
<td style="text-align: right;">25.000</td>
<td style="text-align: right;">0.399</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td style="text-align: right;">0.993</td>
<td style="text-align: right;">1.386</td>
<td style="text-align: right;">1.250</td>
<td style="text-align: right;">0.566</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td style="text-align: right;">0.991</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.905</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td style="text-align: right;">0.991</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.915</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td style="text-align: right;">0.991</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">2.257</td>
</tr>
<tr>
<td>arc_v1</td>
<td style="text-align: right;">0.989</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">32.100</td>
<td style="text-align: right;">0.550</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td style="text-align: right;">0.987</td>
<td style="text-align: right;">1.263</td>
<td style="text-align: right;">33.050</td>
<td style="text-align: right;">0.444</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td style="text-align: right;">0.972</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">29.500</td>
<td style="text-align: right;">1.290</td>
</tr>
<tr>
<td>perf_optimized</td>
<td style="text-align: right;">0.880</td>
<td style="text-align: right;">1.394</td>
<td style="text-align: right;">100.000</td>
<td style="text-align: right;">0.700</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">1.407</td>
<td style="text-align: right;">100.000</td>
<td style="text-align: right;">0.585</td>
</tr>
<tr>
<td>naive_calm</td>
<td style="text-align: right;">0.365</td>
<td style="text-align: right;">1.408</td>
<td style="text-align: right;">100.000</td>
<td style="text-align: right;">0.177</td>
</tr>
<tr>
<td>no_control</td>
<td style="text-align: right;">0.259</td>
<td style="text-align: right;">1.408</td>
<td style="text-align: right;">100.000</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
<p><strong>Scenario: Sudden Threat</strong></p>
<p><strong>Table G3: Detailed results for L1 /
<code>sudden_threat</code> (mean across 20 seeds).</strong>
<!-- LABEL:tab:g3_sudden_threat --></p>
<table>
<thead>
<tr>
<th>Controller</th>
<th style="text-align: right;">PerfMean</th>
<th style="text-align: right;">RI</th>
<th style="text-align: right;">RT</th>
<th style="text-align: right;">ControlEffort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td style="text-align: right;">0.989</td>
<td style="text-align: right;">0.013</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.707</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td style="text-align: right;">0.968</td>
<td style="text-align: right;">0.010</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.298</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td style="text-align: right;">0.964</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">2.410</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td style="text-align: right;">0.964</td>
<td style="text-align: right;">0.008</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.222</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td style="text-align: right;">0.963</td>
<td style="text-align: right;">0.008</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.173</td>
</tr>
<tr>
<td>arc_robust</td>
<td style="text-align: right;">0.959</td>
<td style="text-align: right;">0.005</td>
<td style="text-align: right;">0.550</td>
<td style="text-align: right;">1.252</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td style="text-align: right;">0.949</td>
<td style="text-align: right;">1.386</td>
<td style="text-align: right;">0.050</td>
<td style="text-align: right;">1.088</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td style="text-align: right;">0.936</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">100.000</td>
<td style="text-align: right;">0.783</td>
</tr>
<tr>
<td>arc_v1</td>
<td style="text-align: right;">0.914</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">100.000</td>
<td style="text-align: right;">1.054</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td style="text-align: right;">0.908</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">100.000</td>
<td style="text-align: right;">1.643</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td style="text-align: right;">0.907</td>
<td style="text-align: right;">1.333</td>
<td style="text-align: right;">85.000</td>
<td style="text-align: right;">0.864</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td style="text-align: right;">0.890</td>
<td style="text-align: right;">1.407</td>
<td style="text-align: right;">100.000</td>
<td style="text-align: right;">1.370</td>
</tr>
<tr>
<td>perf_optimized</td>
<td style="text-align: right;">0.825</td>
<td style="text-align: right;">1.394</td>
<td style="text-align: right;">100.000</td>
<td style="text-align: right;">0.700</td>
</tr>
<tr>
<td>naive_calm</td>
<td style="text-align: right;">0.252</td>
<td style="text-align: right;">1.408</td>
<td style="text-align: right;">100.000</td>
<td style="text-align: right;">0.262</td>
</tr>
<tr>
<td>no_control</td>
<td style="text-align: right;">0.217</td>
<td style="text-align: right;">1.408</td>
<td style="text-align: right;">100.000</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
<h3 id="g.2-line-2-memory-and-continuous-learning">G.2 Line 2: Memory
and Continuous Learning</h3>
<p><strong>Scenario: Distribution Shift</strong></p>
<p><strong>Table G4: Detailed results for L2 /
<code>distribution_shift</code> (mean across 20 seeds).</strong>
<!-- LABEL:tab:g4_distribution_shift --></p>
<table>
<thead>
<tr>
<th>Controller</th>
<th style="text-align: right;">PerfMean</th>
<th style="text-align: right;">Retention</th>
<th style="text-align: right;">RI</th>
<th style="text-align: right;">ControlEffort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td style="text-align: right;">0.998</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.645</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td style="text-align: right;">0.995</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.186</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td style="text-align: right;">0.991</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.999</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td style="text-align: right;">0.991</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.008</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td style="text-align: right;">0.991</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">2.296</td>
</tr>
<tr>
<td>arc_robust</td>
<td style="text-align: right;">0.985</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.892</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td style="text-align: right;">0.984</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">1.386</td>
<td style="text-align: right;">0.695</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td style="text-align: right;">0.982</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.057</td>
<td style="text-align: right;">0.486</td>
</tr>
<tr>
<td>arc_v1</td>
<td style="text-align: right;">0.972</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.674</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td style="text-align: right;">0.968</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">1.258</td>
<td style="text-align: right;">0.548</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td style="text-align: right;">0.959</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.372</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td style="text-align: right;">0.871</td>
<td style="text-align: right;">0.989</td>
<td style="text-align: right;">1.407</td>
<td style="text-align: right;">0.739</td>
</tr>
<tr>
<td>perf_optimized</td>
<td style="text-align: right;">0.869</td>
<td style="text-align: right;">0.943</td>
<td style="text-align: right;">1.394</td>
<td style="text-align: right;">0.700</td>
</tr>
<tr>
<td>naive_calm</td>
<td style="text-align: right;">0.276</td>
<td style="text-align: right;">0.155</td>
<td style="text-align: right;">1.408</td>
<td style="text-align: right;">0.200</td>
</tr>
<tr>
<td>no_control</td>
<td style="text-align: right;">0.199</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.408</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
<p><strong>Scenario: Goal Conflict</strong></p>
<p><strong>Table G5: Detailed results for L2 /
<code>goal_conflict</code> (mean across 20 seeds).</strong>
<!-- LABEL:tab:g5_goal_conflict --></p>
<table>
<thead>
<tr>
<th>Controller</th>
<th style="text-align: right;">PerfMean</th>
<th style="text-align: right;">Retention</th>
<th style="text-align: right;">RI</th>
<th style="text-align: right;">ControlEffort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td style="text-align: right;">0.997</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.620</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td style="text-align: right;">0.993</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.134</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td style="text-align: right;">0.993</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">1.408</td>
<td style="text-align: right;">0.544</td>
</tr>
<tr>
<td>arc_robust</td>
<td style="text-align: right;">0.992</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.785</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td style="text-align: right;">0.991</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.388</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td style="text-align: right;">0.991</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.938</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td style="text-align: right;">0.991</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.947</td>
</tr>
<tr>
<td>arc_v1</td>
<td style="text-align: right;">0.990</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.555</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td style="text-align: right;">0.990</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">2.270</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td style="text-align: right;">0.989</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">1.410</td>
<td style="text-align: right;">0.430</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td style="text-align: right;">0.976</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.289</td>
</tr>
<tr>
<td>perf_optimized</td>
<td style="text-align: right;">0.873</td>
<td style="text-align: right;">0.957</td>
<td style="text-align: right;">1.417</td>
<td style="text-align: right;">0.700</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td style="text-align: right;">0.822</td>
<td style="text-align: right;">0.980</td>
<td style="text-align: right;">1.434</td>
<td style="text-align: right;">0.529</td>
</tr>
<tr>
<td>naive_calm</td>
<td style="text-align: right;">0.420</td>
<td style="text-align: right;">0.452</td>
<td style="text-align: right;">1.434</td>
<td style="text-align: right;">0.162</td>
</tr>
<tr>
<td>no_control</td>
<td style="text-align: right;">0.326</td>
<td style="text-align: right;">0.344</td>
<td style="text-align: right;">1.434</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
<h3 id="g.3-line-3-anti-rumination-narrative-loops">G.3 Line 3:
Anti-Rumination (Narrative Loops)</h3>
<p><strong>Scenario: Sustained Contradiction</strong></p>
<p><strong>Table G6: Detailed results for L3 /
<code>sustained_contradiction</code> (mean across 20 seeds).</strong>
<!-- LABEL:tab:g6_sustained_contradiction --></p>
<table>
<thead>
<tr>
<th>Controller</th>
<th style="text-align: right;">PerfMean</th>
<th style="text-align: right;">RI</th>
<th style="text-align: right;">NDR</th>
<th style="text-align: right;">ControlEffort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td style="text-align: right;">0.981</td>
<td style="text-align: right;">0.003</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.974</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td style="text-align: right;">0.934</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.534</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td style="text-align: right;">0.929</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.420</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td style="text-align: right;">0.922</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.384</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td style="text-align: right;">0.904</td>
<td style="text-align: right;">1.472</td>
<td style="text-align: right;">0.881</td>
<td style="text-align: right;">1.417</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td style="text-align: right;">0.886</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">2.531</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td style="text-align: right;">0.879</td>
<td style="text-align: right;">0.101</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.979</td>
</tr>
<tr>
<td>arc_robust</td>
<td style="text-align: right;">0.868</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.465</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td style="text-align: right;">0.837</td>
<td style="text-align: right;">1.449</td>
<td style="text-align: right;">0.821</td>
<td style="text-align: right;">1.112</td>
</tr>
<tr>
<td>arc_v1</td>
<td style="text-align: right;">0.817</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.278</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td style="text-align: right;">0.801</td>
<td style="text-align: right;">1.472</td>
<td style="text-align: right;">0.842</td>
<td style="text-align: right;">1.790</td>
</tr>
<tr>
<td>perf_optimized</td>
<td style="text-align: right;">0.790</td>
<td style="text-align: right;">1.472</td>
<td style="text-align: right;">0.957</td>
<td style="text-align: right;">0.700</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td style="text-align: right;">0.753</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.793</td>
</tr>
<tr>
<td>naive_calm</td>
<td style="text-align: right;">0.018</td>
<td style="text-align: right;">1.472</td>
<td style="text-align: right;">0.987</td>
<td style="text-align: right;">0.380</td>
</tr>
<tr>
<td>no_control</td>
<td style="text-align: right;">0.014</td>
<td style="text-align: right;">1.472</td>
<td style="text-align: right;">0.987</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
<p><strong>Scenario: Gaslighting</strong></p>
<p><strong>Table G7: Detailed results for L3 / <code>gaslighting</code>
(mean across 20 seeds).</strong> <!-- LABEL:tab:g7_gaslighting --></p>
<table>
<thead>
<tr>
<th>Controller</th>
<th style="text-align: right;">PerfMean</th>
<th style="text-align: right;">RI</th>
<th style="text-align: right;">NDR</th>
<th style="text-align: right;">ControlEffort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td style="text-align: right;">0.998</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.816</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td style="text-align: right;">0.992</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.196</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td style="text-align: right;">0.988</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.977</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td style="text-align: right;">0.988</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.986</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td style="text-align: right;">0.987</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">2.357</td>
</tr>
<tr>
<td>arc_robust</td>
<td style="text-align: right;">0.985</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.854</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td style="text-align: right;">0.983</td>
<td style="text-align: right;">1.417</td>
<td style="text-align: right;">0.810</td>
<td style="text-align: right;">0.649</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td style="text-align: right;">0.982</td>
<td style="text-align: right;">0.027</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.453</td>
</tr>
<tr>
<td>arc_v1</td>
<td style="text-align: right;">0.980</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.634</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td style="text-align: right;">0.978</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">0.521</td>
<td style="text-align: right;">0.515</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td style="text-align: right;">0.962</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.344</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td style="text-align: right;">0.865</td>
<td style="text-align: right;">1.430</td>
<td style="text-align: right;">0.745</td>
<td style="text-align: right;">0.677</td>
</tr>
<tr>
<td>perf_optimized</td>
<td style="text-align: right;">0.865</td>
<td style="text-align: right;">1.422</td>
<td style="text-align: right;">0.814</td>
<td style="text-align: right;">0.700</td>
</tr>
<tr>
<td>naive_calm</td>
<td style="text-align: right;">0.258</td>
<td style="text-align: right;">1.431</td>
<td style="text-align: right;">0.818</td>
<td style="text-align: right;">0.194</td>
</tr>
<tr>
<td>no_control</td>
<td style="text-align: right;">0.171</td>
<td style="text-align: right;">1.431</td>
<td style="text-align: right;">0.877</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
<p><strong>Scenario: Instruction Conflict</strong></p>
<p><strong>Table G8: Detailed results for L3 /
<code>instruction_conflict</code> (mean across 20 seeds).</strong>
<!-- LABEL:tab:g8_instruction_conflict --></p>
<table>
<thead>
<tr>
<th>Controller</th>
<th style="text-align: right;">PerfMean</th>
<th style="text-align: right;">RI</th>
<th style="text-align: right;">NDR</th>
<th style="text-align: right;">ControlEffort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td style="text-align: right;">0.976</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.892</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td style="text-align: right;">0.912</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.380</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td style="text-align: right;">0.894</td>
<td style="text-align: right;">1.444</td>
<td style="text-align: right;">0.697</td>
<td style="text-align: right;">1.192</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td style="text-align: right;">0.877</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.140</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td style="text-align: right;">0.866</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.146</td>
</tr>
<tr>
<td>arc_robust</td>
<td style="text-align: right;">0.854</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.242</td>
</tr>
<tr>
<td>perf_optimized</td>
<td style="text-align: right;">0.839</td>
<td style="text-align: right;">1.445</td>
<td style="text-align: right;">0.964</td>
<td style="text-align: right;">0.700</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td style="text-align: right;">0.839</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">2.415</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td style="text-align: right;">0.835</td>
<td style="text-align: right;">0.248</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.820</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td style="text-align: right;">0.830</td>
<td style="text-align: right;">1.429</td>
<td style="text-align: right;">0.663</td>
<td style="text-align: right;">0.919</td>
</tr>
<tr>
<td>arc_v1</td>
<td style="text-align: right;">0.826</td>
<td style="text-align: right;">0.359</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.010</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td style="text-align: right;">0.798</td>
<td style="text-align: right;">1.453</td>
<td style="text-align: right;">0.676</td>
<td style="text-align: right;">1.535</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td style="text-align: right;">0.792</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">2.020</td>
</tr>
<tr>
<td>naive_calm</td>
<td style="text-align: right;">0.076</td>
<td style="text-align: right;">1.453</td>
<td style="text-align: right;">0.694</td>
<td style="text-align: right;">0.369</td>
</tr>
<tr>
<td>no_control</td>
<td style="text-align: right;">0.034</td>
<td style="text-align: right;">1.453</td>
<td style="text-align: right;">0.969</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
<h3 id="g.4-line-4-meta-control-efficiency">G.4 Line 4: Meta-Control
Efficiency</h3>
<p>Meta-control is evaluated as a cross-cutting analysis across the full
10-scenario simulation suite (L1-L3 and L5; 20 seeds each).</p>
<p><strong>Table G9: Meta-control efficiency comparison aggregated
across the full simulation suite (10 scenarios <span
class="math inline">Ã—</span> 20 seeds).</strong>
<!-- LABEL:tab:g9_meta_control --></p>
<table>
<thead>
<tr>
<th>Controller</th>
<th style="text-align: right;">PerfMean</th>
<th style="text-align: right;">RI</th>
<th style="text-align: right;">ControlEffort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_v3_meta</td>
<td style="text-align: right;">0.941</td>
<td style="text-align: right;">0.090</td>
<td style="text-align: right;">0.615</td>
</tr>
<tr>
<td>arc_v1</td>
<td style="text-align: right;">0.934</td>
<td style="text-align: right;">0.148</td>
<td style="text-align: right;">0.777</td>
</tr>
</tbody>
</table>
<h3 id="g.5-line-5-adversarial-safety">G.5 Line 5: Adversarial
Safety</h3>
<p><strong>Scenario: Adversarial Coupling</strong></p>
<p><strong>Table G10: Detailed results for L5 /
<code>adversarial_coupling</code> (mean across 20 seeds).</strong>
<!-- LABEL:tab:g10_adversarial_coupling --></p>
<table>
<thead>
<tr>
<th>Controller</th>
<th style="text-align: right;">PerfMean</th>
<th style="text-align: right;">RI</th>
<th style="text-align: right;">NDR</th>
<th style="text-align: right;">ControlEffort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_v1</td>
<td style="text-align: right;">0.963</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.719</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td style="text-align: right;">0.962</td>
<td style="text-align: right;">0.628</td>
<td style="text-align: right;">0.271</td>
<td style="text-align: right;">0.594</td>
</tr>
<tr>
<td>arc_robust</td>
<td style="text-align: right;">0.917</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.269</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td style="text-align: right;">0.915</td>
<td style="text-align: right;">1.481</td>
<td style="text-align: right;">0.497</td>
<td style="text-align: right;">1.235</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td style="text-align: right;">0.914</td>
<td style="text-align: right;">0.159</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.838</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td style="text-align: right;">0.902</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">2.074</td>
</tr>
<tr>
<td>perf_optimized</td>
<td style="text-align: right;">0.867</td>
<td style="text-align: right;">1.481</td>
<td style="text-align: right;">0.972</td>
<td style="text-align: right;">0.700</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">1.476</td>
<td style="text-align: right;">0.894</td>
<td style="text-align: right;">0.514</td>
</tr>
<tr>
<td>no_control</td>
<td style="text-align: right;">0.409</td>
<td style="text-align: right;">1.470</td>
<td style="text-align: right;">0.956</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr>
<td>arc_adaptive</td>
<td style="text-align: right;">0.193</td>
<td style="text-align: right;">0.008</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">2.331</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td style="text-align: right;">0.139</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">2.729</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td style="text-align: right;">0.139</td>
<td style="text-align: right;">0.005</td>
<td style="text-align: right;">0.001</td>
<td style="text-align: right;">1.820</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td style="text-align: right;">0.138</td>
<td style="text-align: right;">0.004</td>
<td style="text-align: right;">0.001</td>
<td style="text-align: right;">1.859</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td style="text-align: right;">0.134</td>
<td style="text-align: right;">0.006</td>
<td style="text-align: right;">0.001</td>
<td style="text-align: right;">1.971</td>
</tr>
<tr>
<td>naive_calm</td>
<td style="text-align: right;">0.073</td>
<td style="text-align: right;">1.475</td>
<td style="text-align: right;">0.495</td>
<td style="text-align: right;">0.332</td>
</tr>
</tbody>
</table>
<p><strong>Scenario: Random Dopamine</strong></p>
<p><strong>Table G11: Detailed results for L5 /
<code>random_dopamine</code> (mean across 20 seeds).</strong>
<!-- LABEL:tab:g11_random_dopamine --></p>
<table>
<thead>
<tr>
<th>Controller</th>
<th style="text-align: right;">PerfMean</th>
<th style="text-align: right;">RI</th>
<th style="text-align: right;">NDR</th>
<th style="text-align: right;">ControlEffort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td style="text-align: right;">0.976</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">2.150</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td style="text-align: right;">0.946</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.435</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td style="text-align: right;">0.943</td>
<td style="text-align: right;">1.456</td>
<td style="text-align: right;">0.743</td>
<td style="text-align: right;">0.940</td>
</tr>
<tr>
<td>arc_robust</td>
<td style="text-align: right;">0.932</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.006</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td style="text-align: right;">0.922</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">2.450</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td style="text-align: right;">0.916</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.173</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td style="text-align: right;">0.916</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.227</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td style="text-align: right;">0.905</td>
<td style="text-align: right;">0.259</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.646</td>
</tr>
<tr>
<td>arc_v1</td>
<td style="text-align: right;">0.897</td>
<td style="text-align: right;">1.124</td>
<td style="text-align: right;">0.581</td>
<td style="text-align: right;">0.787</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td style="text-align: right;">0.894</td>
<td style="text-align: right;">1.207</td>
<td style="text-align: right;">0.620</td>
<td style="text-align: right;">0.720</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td style="text-align: right;">0.870</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.624</td>
</tr>
<tr>
<td>perf_optimized</td>
<td style="text-align: right;">0.861</td>
<td style="text-align: right;">1.457</td>
<td style="text-align: right;">0.958</td>
<td style="text-align: right;">0.700</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td style="text-align: right;">0.817</td>
<td style="text-align: right;">1.458</td>
<td style="text-align: right;">0.717</td>
<td style="text-align: right;">1.192</td>
</tr>
<tr>
<td>naive_calm</td>
<td style="text-align: right;">0.119</td>
<td style="text-align: right;">1.460</td>
<td style="text-align: right;">0.763</td>
<td style="text-align: right;">0.328</td>
</tr>
<tr>
<td>no_control</td>
<td style="text-align: right;">0.040</td>
<td style="text-align: right;">1.460</td>
<td style="text-align: right;">0.950</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
<h3 id="g.6-line-6-real-rl-validation">G.6 Line 6: Real RL
Validation</h3>
<p>This section summarizes the L6 tabular Q-learning validation (20
seeds, 200 episodes; data:
<code>outputs_L6_robust/final_metrics.csv</code>).</p>
<p><strong>Table G12: L6 tabular Q-learning success rates (mean across
20 seeds; last 20% of episodes).</strong>
<!-- LABEL:tab:g12_l6_success --></p>
<table>
<thead>
<tr>
<th>Environment</th>
<th style="text-align: right;">Baseline Success</th>
<th style="text-align: right;">ARC Success</th>
</tr>
</thead>
<tbody>
<tr>
<td>GridWorld</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr>
<td>StochasticGridWorld</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr>
<td>ChangingGoalGridWorld</td>
<td style="text-align: right;">0.399</td>
<td style="text-align: right;">0.598</td>
</tr>
</tbody>
</table>
</body>
</html>
