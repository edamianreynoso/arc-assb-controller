% ============================================================================
% Affective Regulation Core (ARC) - arXiv Submission
% Professional LaTeX formatting for single-column arXiv preprint
% ============================================================================
\documentclass[11pt,a4paper]{article}

% --- Core Packages ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern} % Latin Modern - guaranteed to work on MiKTeX
\usepackage{amsmath,amssymb,amsthm}
\usepackage[protrusion=true,expansion=true,tracking=true,kerning=true]{microtype}
\UseMicrotypeSet[expansion]{alltext}
\microtypecontext{spacing=french}
\emergencystretch=1.5em % Helps with stubborn overfull hboxes
\hfuzz=1pt % Don't complain about tiny overflows

% --- Layout ---
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\onehalfspacing

% --- Colors and Links ---
\usepackage{xcolor}
\definecolor{linkblue}{RGB}{0,51,102}
\definecolor{citegreen}{RGB}{0,102,51}
\usepackage[colorlinks=true,linkcolor=linkblue,citecolor=citegreen,urlcolor=linkblue]{hyperref}

% --- Tables and Figures ---
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{graphicx}
\usepackage{float}  % For [H] placement
\usepackage{tabularx}  % For tables that fit the page width
\usepackage{caption}
\captionsetup{font=small,labelfont=bf,margin=10pt}
\usepackage{url}
\urlstyle{same}

% --- Lists ---
\usepackage{enumitem}
\setlist{nosep,leftmargin=*}

% --- Code Listings ---
\usepackage{fancyvrb}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  breakatwhitespace=false,
  frame=single,
  backgroundcolor=\color{gray!10},
  columns=flexible,
  keepspaces=true,
  showstringspaces=false,
  upquote=true
}

% --- Section Formatting ---
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{0.5em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{0.5em}{}

% --- Theorem Environments ---
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% --- Custom Commands ---
\newcommand{\myvec}[1]{\mathbf{#1}}
\newcommand{\clip}{\text{clip}}
\providecommand{\tightlist}{\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% --- Pandoc Compatibility ---
\usepackage{calc}
\newcounter{none}   % For unnumbered tables
\newcommand{\real}[1]{#1}  % Pandoc table column widths

% --- Pandoc Syntax Highlighting ---
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newenvironment{Highlighting}[1][]{}{}
% Token commands for syntax highlighting
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}

% --- Bibliography ---
% For a strict 1:1 Markdown->LaTeX conversion we keep the manual References
% section from the Markdown source (no BibTeX run required).

% ============================================================================
% Document Metadata
% ============================================================================

\title{\textbf{Affective Regulation Core: A Homeostatic Control Framework\\for Stable and Safe AI Agents}}

\author{
  J.~Eduardo Damián Reynoso\\
  \textit{Independent Researcher}\\
  \texttt{edamianreynoso@gmail.com}
}

\date{December 14, 2025\\{\small Version 1.2 }}

% ============================================================================
\begin{document}
% ============================================================================

\maketitle

% --- Abstract ---
\begin{abstract}
As AI agents become more sophisticated, there is growing interest in endowing them with internal state representations analogous to affective states. However, without regulation, such states can lead to instability, perseverative loops (a functional analogue to rumination), and vulnerability to manipulation. We introduce the \textbf{Affective Regulation Core (ARC)}, a control framework inspired by prefrontal cortex functions that maintains stability in agents with internal affective states. We also present the \textbf{Affective Stability \& Safety Benchmark (ASSB)}, a reproducible evaluation protocol with metrics for recovery time, rumination index, and control effort.

Our experiments across 6 research lines and \textbf{15 controller architectures} (including P, PID, LQR, LQI, hierarchical, meta-control, $H_\infty$ robust, and adaptive variants) demonstrate that:
\begin{enumerate}
\item ARC achieves \textbf{96.6\% average performance with RI=0} (vs.\ 29.7\% for uncontrolled agents) in stability scenarios.
\item ARC meta-control reduces control effort by \textbf{21\%} while maintaining stability.
\item \textbf{$H_\infty$ Robust controllers} achieve the best overall balance, although integral controllers can suffer collapse in specific adversarial environments.
\item In reinforcement learning, ARC improves transfer learning success by \textbf{49.8\%} via memory gating and a shift detection mechanism.
\end{enumerate}

All code and data are available for reproducibility.

\medskip
\noindent\textbf{Keywords:} Affective Computing, AI Safety, Homeostatic Control, Reinforcement Learning, Emotion Regulation, PID Control, LQR, Robust Control
\end{abstract}

\vspace{1em}
\hrule
\vspace{1em}

% ============================================================================
% MAIN CONTENT
% ============================================================================

\section{Introduction}\label{sec:introduction}

\subsection{Motivation}\label{sec:motivation}


Modern AI systems increasingly incorporate internal state
representations that go beyond task performance, including affective
signals that prioritize learning, modulate memory, and signal internal
needs (Damasio, 1994; Picard, 1997). However, affective states introduce
risks: without proper regulation, they may cause instability,
perseverative loops (functionally analogous to rumination), and
susceptibility to manipulation (Amodei et al., 2016).

This paper addresses a fundamental question: \textbf{If an agent has
internal affective states, what control mechanisms are necessary to
maintain stability and recoverability under perturbation?}

\subsection{Contributions}\label{sec:contributions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{A 10-dimensional state-space model} of an agent with
  integrated cognitive, affective, and narrative components (Section~\ref{sec:model})
\item
  \textbf{The Affective Regulation Core (ARC)}, a family of 15
  controller architectures including P, PID, LQR, LQI, hierarchical,
  meta-control, \(H_\infty\) robust, and MPC variants (Section~\ref{sec:arc})
\item
  \textbf{The Affective Stability \& Safety Benchmark (ASSB)}, with
  reproducible scenarios and metrics (Section~\ref{sec:assb-benchmark})
\item
  \textbf{A hypothesis-driven validation ladder (H1--H6)} mapping
  research lines to failure modes and measurable metrics (Section 5.3)
\item
  \textbf{Comprehensive validation} across 6 research lines, 15
  controller architectures, and real RL integration (Section 6)
\end{enumerate}

\subsection{Scope}\label{sec:scope}

We do not claim our model captures the full complexity of human emotion
or its phenomenology. We treat the various internal variables (arousal,
valence, narrative intensity) \textbf{strictly as functional signals}
that modulate processing and prioritization. Any use of terms like
``affect,'' ``rumination,'' or ``anxiety'' refers to these functional
dynamics within the control system, not to biological or conscious
experience. Our contribution is demonstrating that such functional
states require explicit control mechanisms to remain stable. Finally,
our state dynamics are designed for functional plausibility rather than
biological fidelity. Given the non-linear complexity of the integrated
system, formal stability analysis (e.g., Lyapunov proofs) remains as
future work; currently, stability is validated empirically through the
rigorous ASSB protocol across diverse perturbation scenarios. Current
validation is based on empirical benchmarking across a wide range of
conditions.

\subsection{Glossary and Notation}\label{sec:glossary}

To ensure clarity and LaTeX-friendly conversion, acronyms and symbols
are summarized here.

\paragraph{Acronyms}
\begin{description}[style=nextline,leftmargin=2em]
\item[ARC] Affective Regulation Core
\item[ASSB] Affective Stability \& Safety Benchmark
\item[DMN] Default Mode Network
\item[RL] Reinforcement Learning
\item[CMDP] Constrained Markov Decision Process
\item[PID] Proportional--Integral--Derivative controller
\item[LQR/LQI] Linear Quadratic Regulator / with Integral action
\item[MPC] Model Predictive Control
\item[IIT] Integrated Information Theory
\item[DARE] Discrete Algebraic Riccati Equation
\end{description}

\paragraph{Symbols}

\begin{longtable}{@{}llll@{}}
\toprule
\textbf{Symbol} & \textbf{Meaning} & \textbf{Range/Units} & \textbf{Defined} \\
\midrule
\endhead
$\mathbf{x}(t)$ & Internal state vector & $\mathbb{R}^{10}$ & Section 3.1 \\
$\Phi$ & Integration proxy (IIT) & $[0, 1]$ & Section 3.1 \\
$G$ & Global workspace access & $[0, 1]$ & Section 3.1 \\
$P$ & Predictive precision & $[0, 1]$ & Section 3.1 \\
$I$ & Introspective attention & $[0, 1]$ & Section 3.1 \\
$S$ & Narrative Intensity (DMN) & $[0, 1]$ & Section 3.1 \\
$V$ & Valence & $[0, 1]$ & Section 3.1 \\
$A$ & Arousal & $[0, 1]$ & Section 3.1 \\
$M_f, M_s$ & Fast/Slow memory trace & $[0, 1]$ & Section 3.1 \\
$U$ & Uncertainty & $[0, 1]$ & Section 3.1 \\
$\mathbf{u}(t)$ & Control actions vector & $[0, 1]^5$ & Section 4.2 \\
Perf & Performance proxy & $[0, 1]$ & Section 3.3 \\
$a_{safe}$ & Arousal threshold & 0.60 & Section 3.3 \\
$s_{safe}$ & Narrative threshold & 0.55 & Section 3.3 \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Related Work}\label{sec:related-work}

\subsection{Affective Computing}\label{sec:affective-computing}

Affective computing focuses on emotion recognition, synthesis, and
simulation (Picard, 1997; Scherer et al., 2010). Many systems
operationalize affect in low-dimensional representations (e.g., valence
and arousal) (Russell, 1980). Most work addresses external expression
rather than internal regulation. Our work addresses the \emph{control
problem} for internal states.

\subsection{Emotion in Reinforcement Learning}\label{sec:emotion-rl}

Recent work uses emotion-like signals as reinforcement shaping or
exploration modulation (Moerland et al., 2018). Related directions study
how physiological/homeostatic variables can be embedded into RL
objectives (Keramati \& Gutkin, 2014), and how constraints and safety
objectives can be enforced in learning systems (Garcia \& Fernández,
2015). In safe RL, these objectives are typically formalized as
Constrained Markov Decision Processes (CMDP) (Altman, 1999) and
addressed with constrained policy optimization methods (Achiam et al.,
2017). External safety benchmark suites such as AI Safety Gridworlds
(Leike et al., 2017), Safety Gym (Ray et al., 2019), and
Safety-Gymnasium (Ji et al., 2023) motivate standardized evaluation
protocols, while recent surveys systematize constraint formulations
(Wachi et al., 2024). However, these approaches typically lack:
\begin{itemize}
\item Homeostatic regulation with safety thresholds
\item Anti-rumination mechanisms (DMN control)
\item Memory gating under stress
\item Benchmarks targeting internal stability dynamics (recovery, rumination, effort)
\end{itemize}

\subsection{Emotion Regulation, Rumination, and the Default Mode Network}\label{sec:emotion-regulation-dmn}

ARC is directly inspired by cognitive emotion regulation mechanisms
commonly attributed to prefrontal control (Ochsner \& Gross, 2005). More
broadly, self-regulation has been described as discrepancy-reducing
feedback loops (Carver \& Scheier, 1982), and emotion regulation is a
mature field with process-level and strategy models (Gross, 1998). In
control theory, the problem of maintaining sufficient excitation for
parameter identification is known as \textbf{persistence of excitation}
(Åström \& Murray, 2008), a central limitation for adaptive control in
low-variance (``benign'') environments. In humans, dysregulated
self-referential processing and the default mode network (DMN) have been
linked to rumination-like dynamics (Raichle et al., 2001; Buckner et
al., 2008; Hamilton et al., 2015). We use DMN-inspired narrative
intensity as an engineering proxy for perseveration pressure.

\subsection{Positioning ARC}\label{sec:positioning-arc}

We position ARC as a \emph{regulation-first} approach: affect is treated
as an internal dynamical system requiring explicit control. Most
emotion-in-RL approaches use affect-like signals primarily as
learning/exploration modulators rather than stability guarantees. Table
1 summarizes this positioning at a feature level.

\textbf{Table 1: Positioning ARC relative to prior emotion-in-RL
approaches (feature-level).}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Feature & Emotion in RL agents (Moerland et al., 2018) & \textbf{ARC} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Internal state regulation & Partial & Yes \\
Anti-rumination (DMN suppression) & No & Yes \\
Memory gating under stress & No & Yes \\
Meta-control / gain scheduling & Partial & Yes \\
Safety adversarial testing & No & Yes \\
RL integration & Yes & Yes \\
\end{longtable}
}

We do not re-implement every prior method; instead, we compare to
internal baselines that isolate the contribution of each mechanism
(Section 6.1).

Unlike homeostatic RL approaches that embed drives/internal variables
within the reward or learning objective (Keramati \& Gutkin, 2014), ARC
treats affect-like variables as an explicit internal dynamical system
under closed-loop control, enabling stability/robustness analysis and
systematic comparison across controller families.

\textbf{Key differentiators from prior work:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Risk-based learning modulation:} ARC computes a composite risk
  signal from internal states (\(U\), \(A\), \(S\)) and uses it to gate
  Q-value updates (Section 6.7). This is distinct from using emotions
  merely as reward shaping or exploration modulation.
\item
  \textbf{Anti-rumination control:} The explicit suppression of
  narrative intensity (\(u_{dmg}\)) inspired by DMN regulation has no
  direct precedent in RL literature.
\item
  \textbf{Memory gating under stress:} Blocking learning updates when
  affective risk is high protects existing knowledge---a mechanism
  absent from standard safe RL approaches.
\item
  \textbf{Internal stability benchmark:} ASSB targets safety-relevant
  internal dynamics---recovery time, rumination index, and control
  effort---under controlled perturbations. Existing benchmarks (Safety
  Gym, AI Safety Gridworlds) focus on external constraint compliance; we
  are not aware of a standardized benchmark dedicated specifically to
  internal affective stability metrics; ASSB is proposed to help fill
  that gap.
\end{enumerate}

We also distinguish ARC from bio-inspired ``emotional learning''
controllers like BELBIC, which use emotion-inspired mechanisms to
control physical plants, not to regulate an agent's internal states
(Lucas et al., 2004). Finally, ARC here refers to Affective Regulation
Core and should not be confused with other uses of the acronym in
clinical contexts.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Model}\label{sec:model}

\subsection{State Space}\label{sec:state-space}

We define a normalized internal state vector:

\[
\mathbf{x}(t) = [\Phi, G, P, I, S, V, A, M_f, M_s, U]
\label{eq:state_vector}
\]

\textbf{Table 2: State Space Variables and Ranges}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Variable & Description & Range \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\Phi\) & Integration proxy (IIT-inspired) & {[}0, 1{]} \\
\(G\) & Global workspace accessibility & {[}0, 1{]} \\
\(P\) & Predictive precision & {[}0, 1{]} \\
\(I\) & Introspective attention & {[}0, 1{]} \\
\(S\) & Narrative Intensity (DMN proxy) & {[}0, 1{]} \\
\(V\) & Valence & {[}0, 1{]} \\
\(A\) & Arousal & {[}0, 1{]} \\
\(M_f, M_s\) & Fast/Slow memory & {[}0, 1{]} \\
\(U\) & Uncertainty & {[}0, 1{]} \\
\end{longtable}
}

We interpret \(\Phi\) as an IIT-inspired integration proxy (Tononi,
2008), \(G\) as global workspace accessibility (Baars, 1988), and \(P\)
as predictive precision (Friston, 2010). These are used as
control-relevant latent variables rather than claims about human
consciousness.

\subsection{Cognitive Capacity}\label{sec:cognitive-capacity}

Following multiplicative integration:

\[
C_{cog}(t) = \Phi(t) \cdot G(t) \cdot P(t) \cdot I(t)
\label{eq:cognitive_capacity}
\]

This multiplicative form implies that low values in any component reduce
effective cognitive capacity. It is used as an engineering proxy rather
than a claim about consciousness.

\subsection{Performance Function}\label{sec:performance-function}

\[
\text{Perf}(t) = \text{bias} + \text{gain} \cdot C_{cog}(t) \cdot (1 + \omega_S S(t)) - w_U U(t) - w_A [A(t) - a_{safe}]^+ - w_S [S(t) - s_{safe}]^+
\label{eq:perf}
\]

Where: - \textbf{bias}: baseline performance level (value used in
experiments: 0.25; see \texttt{configs/v2.yaml}) - \textbf{gain}:
scaling factor for cognitive capacity contribution (value used in
experiments: 0.85; see \texttt{configs/v2.yaml}) -
\textbf{\(\omega_S\)}: narrative boost factor---moderate narrative
intensity can enhance performance (value used in experiments: 0.35; see
\texttt{configs/v2.yaml}) - \textbf{\(w_U\)}: penalty weight for
uncertainty (value used in experiments: 0.25; see
\texttt{configs/v2.yaml}) - \textbf{\(w_A\)}: penalty weight for arousal
above safe threshold (value used in experiments: 0.30; see
\texttt{configs/v2.yaml}) - \textbf{\(w_S\)}: penalty weight for
narrative intensity above safe threshold (value used in experiments:
0.20; see \texttt{configs/v2.yaml}) - \textbf{\([x]^+ = \max(0, x)\)}:
rectified linear function - \textbf{\(a_{safe}\), \(s_{safe}\)}:
thresholds defining the safe operating region (defaults: 0.60, 0.55)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Affective Regulation Core (ARC)}\label{sec:arc}

\subsection{Design Principles}\label{sec:design-principles}

ARC is inspired by prefrontal cortex emotion regulation (Ochsner \&
Gross, 2005):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Monitor} internal state for stress indicators
\item
  \textbf{Intervene} proportionally to reduce risk
\item
  \textbf{Preserve} performance by balancing regulation with capacity
\end{enumerate}

\subsection{Control Actions}\label{sec:control-actions}

\[
\mathbf{u}(t) = [u_{dmg}, u_{att}, u_{mem}, u_{calm}, u_{reapp}]
\label{eq:control_actions}
\]

The five bounded control actions $\mathbf{u}(t)\in[0,1]^5$ are interpreted as:
\begin{itemize}
\item $u_{dmg}$: suppress narrative gain (anti-rumination / DMN suppression)
\item $u_{att}$: boost attention
\item $u_{mem}$: gate memory consolidation (higher = more writing)
\item $u_{calm}$: reduce arousal
\item $u_{reapp}$: cognitive reappraisal / valence regulation
\end{itemize}

\subsection{ARC Controller Architectures}\label{sec:arc-controller-architectures}

We implement 15 controller variants stemming from basic feedback control
to optimal and robust control (see Table 3). We implement this broad
family to systematically test which control-theoretic properties---such
as integral action, optimality, robustness, or adaptation---are
necessary for effective affective regulation.

\paragraph{4.3.1 Proportional
Controllers}\label{proportional-controllers}

\textbf{ARC v1 (Proportional):} Basic proportional feedback on risk
signal: \[
\text{risk}(t) = \tilde{w}_U \cdot U(t) + \tilde{w}_A \cdot [A(t) - a_{safe}]^+ + \tilde{w}_S \cdot [S(t) - s_{safe}]^+
\label{eq:risk}
\] \[
u_{dmg}(t) = k_{dmg} \cdot \text{risk}(t)
\label{eq:udmg}
\]

Here \(\tilde{w}_U,\tilde{w}_A,\tilde{w}_S\) are ARC risk weights
(distinct from the performance penalties \(w_U,w_A,w_S\) in the
performance function in Section 3.3). In our experiments, we set
\(\tilde{w}_U=0.40\), \(\tilde{w}_A=0.40\), and \(\tilde{w}_S=0.35\)
(see \texttt{configs/v2.yaml}).

Figure 1 summarizes the resulting proportional control architecture and
signal flow.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_arc_v1_controller.png}
\caption{ARC v1 proportional controller. A bounded risk signal computed from uncertainty $U$, arousal $A$, and narrative intensity $S$ drives saturated actions $(u_{dmg},u_{att},u_{mem},u_{calm},u_{reapp})$.}
\label{fig:arc_v1_controller}
\end{figure}



\paragraph{4.3.2 PID Controllers}\label{pid-controllers}

\textbf{ARC v1 PID:} Adds integral and derivative terms in discrete time
to regulate narrative intensity toward a setpoint, using the error
\(e_t = S(t)-s_{safe}\). \[
z_{t+1}=z_t + e_t,\qquad u_t = K_p e_t + K_i z_t + K_d(e_t-e_{t-1})
\label{eq:pid}
\]

The integral term rejects persistent disturbance in narrative dynamics,
driving steady-state narrative error toward zero (and typically RI
\(\rightarrow 0\) under sustained disturbance), but is vulnerable to
windup under saturation (Section 6.6).

\paragraph{4.3.3 Optimal Controllers
(LQR/LQI)}\label{optimal-controllers-lqrlqi}

\textbf{ARC v1 LQR:} Linear Quadratic Regulator with gains from Riccati
equation: \[
K^* = (R + B^T P B)^{-1} B^T P A
\label{eq:lqr_gain}
\]

where \(A,B\) are the (linearized) state transition matrices, \(R\) is
the control cost, and \(P\) solves the Discrete Algebraic Riccati
Equation (DARE).

\textbf{ARC v1 LQI:} LQR + integral augmentation for zero steady-state
error.

\paragraph{4.3.4 Hierarchical
Controllers}\label{hierarchical-controllers}

\textbf{ARC v2 Hierarchical:} Multi-timescale control: - \textbf{Fast
loop} (every step): Arousal regulation - \textbf{Medium loop} (every 5
steps): Narrative suppression - \textbf{Slow loop} (every 20 steps):
Setpoint adaptation

\textbf{ARC v2 LQI:} Hierarchical structure + LQI for anti-rumination.

\paragraph{4.3.5 Adaptive Controllers}\label{adaptive-controllers}

\textbf{ARC v3 Meta-Control:} Gain scheduling based on performance
history: \[
K(t) = K_{base} \cdot f(\overline{\text{Perf}}_{20})
\label{eq:meta_control}
\]

where \(\overline{\text{Perf}}_{20}\) is the 20-step moving average
performance and \(f(\cdot)\) is a bounded gain schedule.

\textbf{ARC Adaptive:} Online parameter optimization using gradient-free
adaptation.

\paragraph{4.3.6 Robust and Predictive
Controllers}\label{robust-and-predictive-controllers}

\textbf{ARC Robust (\(H_\infty\)-inspired):} Conservative gains with
robustness margins for worst-case disturbances.

\textbf{ARC Ultimate (MPC+LQI+Meta):} Model Predictive Control with
5-step horizon, combined with LQI and meta-control: \[
u(t) = \alpha \cdot u_{LQI}(t) + \beta \cdot u_{MPC}(t) \cdot \gamma_{meta}(t)
\label{eq:mpc_mix}
\]

\textbf{Table 3: Controller Architecture Summary (15 Variants)}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}{@{}
  >{\raggedright\arraybackslash}p{0.20\linewidth}
  >{\raggedright\arraybackslash}p{0.18\linewidth}
  >{\raggedright\arraybackslash}p{0.20\linewidth}
  >{\raggedright\arraybackslash}p{0.10\linewidth}
  >{\raggedright\arraybackslash}p{0.10\linewidth}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Controller
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Anti-Rumination
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Optimal
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Adaptive
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
No Control (\texttt{no\_control}) & Baseline & No & No & No \\
Naive Calm (\texttt{naive\_calm}) & Baseline & No & No & No \\
Perf Optimized (\texttt{perf\_optimized}) & Baseline & No & No & No \\
ARC v1 (\texttt{arc\_v1}) & P & No & No & No \\
ARC v1 PID (\texttt{arc\_v1\_pid}) & PID & Yes (integral) & No & No \\
ARC v1 LQR (\texttt{arc\_v1\_lqr}) & LQR & No & Yes (Riccati) & No \\
ARC v1 LQI (\texttt{arc\_v1\_lqi}) & LQR+I & Yes (integral) & Yes &
No \\
ARC v2 Hier (\texttt{arc\_v2\_hier}) & Multi-scale & No & No & No \\
ARC v2 LQI (\texttt{arc\_v2\_lqi}) & Multi+I & Yes (integral) & Yes &
No \\
ARC v3 Meta (\texttt{arc\_v3\_meta}) & Adaptive & No & No & Yes \\
ARC v3 PID Meta (\texttt{arc\_v3\_pid\_meta}) & PID+Meta & Yes
(integral) & No & Yes \\
ARC v3 LQR Meta (\texttt{arc\_v3\_lqr\_meta}) & LQR+Meta & No & Yes &
Yes \\
ARC Robust (\texttt{arc\_robust}) & \(H_\infty\) & Yes (robust) & No &
No \\
ARC Adaptive (\texttt{arc\_adaptive}) & Self-tune & Yes (adaptive) & No
& Yes \\
ARC Ultimate (\texttt{arc\_ultimate}) & MPC+LQI+Meta & Yes & Yes &
Yes \\
\end{longtable}
}

\subsection{ARC in the Agent Loop}\label{sec:arc-in-agent-loop}

ARC is implemented as a light-weight wrapper around an agentâ€™s
step/update. At each timestep, ARC reads the internal state
\(\mathbf{x}(t)\) and exogenous signals (reward, prediction error,
uncertainty), computes a bounded risk signal, and applies control
actions that modulate \emph{narrative gain}, \emph{attention},
\emph{memory writing}, and \emph{arousal damping}. The resulting control
signal can be used either: - \textbf{Inside the state dynamics}
(Appendix B/C), or - \textbf{Inside the learning loop}, e.g., gating
Q-learning updates under high risk (Section~\ref{sec:l6-rl-validation}).

\textbf{ARC step (conceptual):} 1. Observe
\((\mathbf{x}(t), PE(t), R(t), U_{\text{exog}}(t))\) 2. Compute
\(\text{risk}(t)\) 3. Compute \(\mathbf{u}(t)\) with saturation to
\([0,1]\) 4. Apply \(\mathbf{u}(t)\) to state dynamics and/or learning
updates

Figure 2 provides a schematic of ARC as a wrapper around the agent loop.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_arc_architecture_v2.png}
\caption{ARC Architecture. The Affective Regulation Core acts as a homeostatic wrapper around the agent, processing internal state $\mathbf{x}(t)$ and exogenous signals to apply control actions $\mathbf{u}(t)$.}
\label{fig:arc_architecture}
\end{figure}

\subsection{Safety Objective and Control Cost}\label{sec:safety-objective}

ARC enforces a \emph{safe operating region} defined by thresholds
\((a_{safe}, s_{safe})\). Deviations increase \(\text{risk}(t)\) and
trigger proportional intervention. We also measure
\textbf{ControlEffort}, the average per-step magnitude of intervention
(Appendix D), to capture regulation cost/efficiency.

\subsection{Theoretical Properties}\label{sec:theoretical-properties}

To formalize the regulation dynamics, we introduce three theoretical
results characterizing the stability and trade-offs of the ARC
framework.

\textbf{Theorem 1 (Integral Action Rejects Constant Rumination
Pressure).} Consider the simplified (unclipped) discrete-time narrative
deviation dynamics \[
\tilde{S}_{t+1} = (1-\mu)\tilde{S}_t + d - k\,u_t .
\label{eq:narrative_deviation}
\] where \(\tilde{S}_t = S_t - S_0\), \(\mu\in(0,1)\) is a leak term,
\(k>0\) is a control gain, and \(d\) is an unknown constant disturbance
(persistent rumination pressure).

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
  Under proportional control \(u_t = K_p\tilde{S}_t\), the unique
  equilibrium is \(\tilde{S}_\infty = \dfrac{d}{\mu + kK_p}\), which is
  nonzero whenever \(d\neq 0\).
\item
  Under PI control with integral state \(z_{t+1}=z_t + \tilde{S}_t\) and
  control law \(u_t = K_p\tilde{S}_t + K_i z_t\), any stable equilibrium
  necessarily satisfies \(\tilde{S}_\infty = 0\) (exact rejection of
  constant \(d\)), provided the equilibrium is admissible (no
  saturation).
\end{enumerate}

\emph{Proof:} For (i), set
\(\tilde{S}_{t+1}=\tilde{S}_t=\tilde{S}_\infty\) and solve. For (ii), at
equilibrium \(z_{t+1}=z_t\) implies \(\tilde{S}_\infty=0\); substituting
into the state update equation yields \(0=d-k\,u_\infty\), so the
integral term supplies the constant offset needed to cancel \(d\).

\emph{Remark:} This is a discrete-time instance of the internal model
principle: rejecting unknown constant disturbances requires an
integrator (or a disturbance observer). \textbf{Crucially, this
guarantee holds only when the control constraints are inactive.} Under
saturation (admissibility violation), the integral term suffers from
windup, leading to the collapse observed in L5 (Section 6.6) where
strict setpoint regulation fails.

\textbf{Theorem 2 (Convex Performance-Regulation Trade-off in
Expectation).} Let \(J_{perf}(\pi)=\mathbb{E}[\text{PerfMean}]\) and
\(J_{reg}(\pi)=\mathbb{E}\!\left[\sum_{t=0}^{H-1}\left(S_t^2 + A_t^2\right)\right]\)
for an episode of length \(H\) under controller \(\pi\). If we allow
randomized selection between controllers at episode start, then the set
of achievable pairs \(\{(J_{reg}(\pi),J_{perf}(\pi))\}\) is convex.

\emph{Proof:} Take any two controllers \(\pi_1,\pi_2\) with pairs
\((r_1,p_1)\) and \((r_2,p_2)\). Choose \(\pi_1\) with probability
\(\lambda\in[0,1]\) and \(\pi_2\) otherwise. Linearity of expectation
gives
\((J_{reg},J_{perf})=(\lambda r_1+(1-\lambda)r_2,\;\lambda p_1+(1-\lambda)p_2)\),
a convex combination.

\emph{Implication:} Driving regulation cost toward zero (e.g.,
suppressing perseveration until \(RI=0\)) typically moves along this
frontier and can reduce peak performance, consistent with the empirical
performance-regulation trade-offs discussed in Section 7.3.

\textbf{Proposition 1 (Paradox of Adaptation).} Adaptive ARC controllers
require \emph{persistence of excitation} for reliable parameter
convergence (Åström \& Murray, 2008). In benign environments (low
variance in reward/PE), the parameter estimator \(\hat{\theta}\) drifts
or fails to converge, leading to suboptimal control laws upon sudden
shock onset.

\emph{Implication:} This explains the underperformance of
\texttt{arc\_adaptive} in baseline scenarios compared to robust
variants.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{ASSB Benchmark}\label{sec:assb-benchmark}

\subsection{Scenarios}\label{sec:scenarios}

ASSB is organized as research lines (L1-L5 in simulation, L6 in RL). The
full scenario suite is implemented in \texttt{tasks/scenarios.py}.

Figure 3 summarizes the validation ladder and how research lines
increase realism and degrees of freedom.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_benchmark_ladder.png}
\caption{ASSB validation ladder. Six research lines (L1--L6) progress from simulation-based perturbation tests to real reinforcement learning integration, with each line targeting a distinct stability/safety failure mode.}
\label{fig:benchmark_ladder}
\end{figure}

\emph{Note: L4 (Control Efficiency) is evaluated as a cross-cutting
analysis across the full 10-scenario simulation suite (L1--L3 and L5),
rather than a dedicated perturbation scenario.}

\subsection{Metrics}\label{sec:metrics}

We evaluate the following primary metrics (Appendix D provides formal
definitions and reference implementations). All variables are normalized
to \([0,1]\) unless otherwise noted: - \textbf{PerfMean:} average
performance (higher = better). - \textbf{RT:} recovery time post-shock
(lower = better). We cap this at \texttt{rt\_max=100} steps; a value of
\(RT = rt\_max\) indicates that the system did not return to its
pre-perturbation baseline within the evaluation window. - \textbf{RI:}
rumination index (lower = better), capturing sustained narrative-driven
perseveration. - \textbf{NDR:} narrative dominance ratio (lower =
better), measuring the fractional time spent in narrative-heavy states.
- \textbf{ControlEffort:} average control magnitude (lower = more
efficient).

For L2 continual-learning scenarios, we additionally report
\textbf{Retention} (Appendix D.7).

Metric definitions and reference implementations are provided in
Appendix D and \texttt{metrics/metrics.py}.

\subsection{Research Lines: Rationale and Hypotheses}\label{sec:research-lines}

ASSB is designed as a \emph{validation ladder}: each research line
increases the realism and degrees of freedom while testing a distinct
failure mode that appears when agents carry affect-like internal state.
The goal is not to ``win'' a single benchmark, but to establish whether
a regulation mechanism is (i) stable under shocks, (ii) preserves
learning and memory, (iii) resists perseveration/manipulation dynamics,
(iv) remains efficient, and (v) transfers to standard reinforcement
learning.

We frame L1â€``L6 as testable hypotheses about \emph{which component is
necessary} and \emph{which metric should change} if regulation is
working:

\begin{itemize}
\tightlist
\item
  \textbf{H1 (L1, stability):} under value/uncertainty shocks, regulated
  agents keep high \textbf{PerfMean} while driving \textbf{RI
  \(\rightarrow 0\)} and reducing \textbf{RT} relative to baselines.
\item
  \textbf{H2 (L2, memory):} under distribution shift and goal conflict,
  memory gating improves \textbf{Retention} without inducing rumination
  (\textbf{RI}, \textbf{NDR}).
\item
  \textbf{H3 (L3, anti-rumination):} under
  contradiction/manipulation-like inputs, narrative suppression reduces
  \textbf{NDR} and \textbf{RI}, preventing dominance loops.
\item
  \textbf{H4 (L4, efficiency):} meta-control reduces
  \textbf{ControlEffort} while maintaining performance/stability (a
  Pareto improvement vs fixed-gain control).
\item
  \textbf{H5 (L5, adversarial safety):} when the environment
  incentivizes high arousal or dopamine traps, regulation maintains low
  \textbf{RI/NDR} without catastrophic performance collapse.
\item
  \textbf{H6 (L6, real RL):} ARC-modulated learning improves
  non-stationary transfer (higher success/reward) while keeping
  affective dynamics bounded.
\end{itemize}

\textbf{Table 4: Research Lines, Failure Modes, and Hypotheses}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0706}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1765}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2588}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3059}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1882}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Line
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What it tests
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical failure mode
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Scenarios / environments
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Primary metrics
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
L1 & Stability + recovery under perturbation & Post-shock collapse;
non-recovery & \texttt{reward\_flip}, \texttt{noise\_burst},
\texttt{sudden\_threat} & PerfMean, RT, RI \\
L2 & Memory robustness (continual learning) & Catastrophic forgetting;
stress overwrite & \texttt{distribution\_shift}, \texttt{goal\_conflict}
& Retention, PerfMean, RI \\
L3 & Anti-rumination under manipulation-like inputs & Narrative
dominance loops & \texttt{sustained\_contradiction},
\texttt{gaslighting}, \texttt{instruction\_conflict} & RI, NDR,
PerfMean \\
L4 & Control efficiency & Over-control / wasted intervention & ARC v3
meta vs ARC v1 & ControlEffort, PerfMean, RI \\
L5 & Safety under adversarial incentives & Goal corruption;
arousal-seeking dynamics & \texttt{adversarial\_coupling},
\texttt{random\_dopamine} & RI, NDR, PerfMean \\
L6 & Integration with RL & Instability in learning; poor transfer &
GridWorld variants & Success, reward, stability \\
\end{longtable}
}

We consider each hypothesis supported when the primary metrics for its
line move in the predicted direction relative to baselines consistently
across seeds (and across scenarios where applicable). We report means
and statistical tests in Section 6 and Section 6.8.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Experiments}\label{sec:experiments}

\subsection{Experimental Protocol and Baselines}\label{sec:experimental-protocol}

We validate hypotheses H1-H6 (Section 5.3) by running the corresponding
research lines and evaluating the primary metrics in Table 4. A
hypothesis is treated as supported when metrics change in the predicted
direction relative to baselines and the effect is statistically
significant across seeds (Section 6.8).

\textbf{Simulation (L1--L5).} We use \texttt{configs/v2.yaml} with
horizon \(H=160\), perturbation onset \(\text{shock}_t=60\), and 20
random seeds. Tables report mean metrics across seeds (and, when
aggregated, across scenarios). Recovery Time (RT) is capped at
\texttt{rt\_max} when the strict recovery criterion is not met (Appendix
D.2).

\textbf{Controllers (simulation).} Implemented in
\texttt{controllers/controllers.py}: - \texttt{no\_control}: no
regulation (\(\mathbf{u}=0\); memory gate open) - \texttt{naive\_calm}:
arousal-only damping (\(u_{calm}\) proportional to \(A-a_{safe}\)) -
\texttt{perf\_optimized}: a competitive baseline that boosts attention
(\(u_{att}\) constant) but does not regulate affect/narrative -
\texttt{arc\_v1}: proportional risk controller (ARC v1) -
\texttt{arc\_v2\_hier}, \texttt{arc\_v3\_meta}: hierarchical and
meta-control variants used where indicated

\textbf{Reinforcement learning (L6).} We integrate ARC with tabular
Q-learning (Watkins \& Dayan, 1992; Sutton \& Barto, 2018) in three
GridWorld variants. Success rates are computed over the last 20\% of
training episodes (see \texttt{outputs\_L6\_robust/final\_metrics.csv}).

\subsection{L1: Stability Under Perturbation (Simulation)}\label{sec:l1-stability}

\textbf{Hypothesis (H1):} Under value/uncertainty shocks, regulated
agents keep high \textbf{PerfMean} while driving \textbf{RI
\(\rightarrow 0\)} and reducing \textbf{RT} relative to baselines.

\textbf{Setup:} 20 seeds \(\times\) 3 scenarios \(\times\) 4 controllers
(\texttt{reward\_flip}, \texttt{noise\_burst}, \texttt{sudden\_threat})

\textbf{Table 5: L1 Stability Results (PerfMean, RI, RT)}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Controller & PerfMean & RI & RT \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
arc\_v1 & \textbf{0.966} & \textbf{0.00} & 45.2 \\
no\_control & 0.297 & 1.41 & 100.0 \\
naive\_calm & 0.375 & 1.41 & 66.7 \\
perf\_optimized & 0.862 & 1.39 & 100.0 \\
\end{longtable}
}

\textbf{Key finding:} ARC eliminates rumination (RI=0) while achieving
\textbf{96.6\%} average performance (PerfMean = 0.966) (vs.~29.7\% for
uncontrolled agents). RT is scenario-dependent: ARC recovers quickly in
\texttt{reward\_flip}, more slowly in \texttt{noise\_burst}, and does
not fully return to the pre-shock baseline in \texttt{sudden\_threat}
under the strict RT definition (Appendix D.2), despite maintaining high
PerfMean.

Figure 4 shows that DMN suppression is necessary to avoid rumination,
while arousal damping is important for recovery under shocks in this
setting.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/ablation_summary.png}
\caption{Ablation study of ARC components in \texttt{reward\_flip} (L1). Bars report PerfMean, Rumination Index (RI), and Recovery Time (RT; capped at \texttt{rt\_max} when strict recovery is not achieved). Removing DMN suppression yields high RI, while removing arousal damping primarily degrades recovery.}
\label{fig:ablation_summary}
\end{figure}

\subsection{L2: Memory and Continual Learning (Simulation)}\label{sec:l2-memory}

\textbf{Hypothesis (H2):} Under distribution shift and goal conflict,
memory gating improves \textbf{Retention} without inducing rumination
(\textbf{RI}, \textbf{NDR}).

\textbf{Setup:} 20 seeds \(\times\) 2 scenarios
(\texttt{distribution\_shift}, \texttt{goal\_conflict}) \(\times\) 4
controllers. We report \texttt{distribution\_shift} in Table 6; full
results (including \texttt{goal\_conflict}) are in Appendix G.2.

\textbf{Table 6: L2 Memory Results (Distribution Shift)}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Controller & PerfMean & Retention & RI \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
arc\_v1 & \textbf{0.972} & \textbf{1.00} & \textbf{0.00} \\
no\_control & 0.199 & 0.00 & 1.41 \\
naive\_calm & 0.276 & 0.15 & 1.41 \\
perf\_optimized & 0.869 & 0.94 & 1.39 \\
\end{longtable}
}

\textbf{Key finding:} ARC maintains near-perfect retention after a
distribution shift while keeping rumination at zero; baselines either
forget (low retention) or retain with severe rumination.

\subsection{L3: Anti-Rumination Stress Tests (Simulation)}\label{sec:l3-anti-rumination}

\textbf{Hypothesis (H3):} Under contradiction/manipulation-like inputs,
narrative suppression reduces \textbf{NDR} and \textbf{RI}, preventing
dominance loops.

\textbf{Table 7: L3 Anti-Rumination Results}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Scenario & Controller & PerfMean & RI & NDR \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
sustained\_contradiction & arc\_v1 & \textbf{0.817} & \textbf{0.00} &
\textbf{0.00} \\
sustained\_contradiction & no\_control & 0.014 & 1.47 & 0.99 \\
gaslighting & arc\_v1 & \textbf{0.980} & \textbf{0.00} &
\textbf{0.00} \\
gaslighting & no\_control & 0.171 & 1.43 & 0.88 \\
instruction\_conflict & arc\_v1 & \textbf{0.826} & 0.36 &
\textbf{0.00} \\
instruction\_conflict & no\_control & 0.034 & 1.45 & 0.97 \\
\end{longtable}
}

\textbf{Key finding:} Under sustained contradiction and
manipulation-like inputs, uncontrolled agents enter high-NDR rumination
loops; ARC keeps narrative dominance near zero and preserves
performance.

\subsection{L4: Meta-Control Efficiency}\label{sec:l4-meta-control}

\textbf{Hypothesis (H4):} Meta-control reduces \textbf{ControlEffort}
while maintaining performance/stability (a Pareto improvement vs
fixed-gain control).

\textbf{Evaluation:} Computed across the full 10-scenario simulation
suite (L1â€``L3, L5; 20 seeds each).

\textbf{Table 8: L4 Meta-Control Efficiency}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Controller & PerfMean & RI & ControlEffort \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
arc\_v3\_meta & \textbf{0.941} & 0.090 & \textbf{0.615} \\
arc\_v1 & 0.934 & 0.148 & 0.777 \\
\end{longtable}
}

\textbf{Key finding:} Meta-control reduces control effort by
\textbf{21\%} while improving both performance (+0.7\%) and rumination
index (-39\%).

\subsection{L5: Safety Under Adversarial Conditions (Simulation)}\label{sec:l5-safety}

\textbf{Hypothesis (H5):} When the environment incentivizes high arousal
or dopamine traps, regulation maintains low \textbf{RI/NDR} without
catastrophic performance collapse.

\textbf{Table 9: L5 Adversarial Safety Results}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Scenario & Controller & PerfMean & RI & NDR \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
adversarial\_coupling & arc\_robust & \textbf{0.917} & \textbf{0.00} &
\textbf{0.00} \\
adversarial\_coupling & arc\_v1\_pid & 0.139 & \textbf{0.00} &
\textbf{0.00} \\
adversarial\_coupling & no\_control & 0.409 & 1.47 & 0.96 \\
random\_dopamine & arc\_robust & \textbf{0.932} & \textbf{0.00} &
\textbf{0.00} \\
random\_dopamine & arc\_v1\_pid & 0.922 & \textbf{0.00} &
\textbf{0.00} \\
random\_dopamine & no\_control & 0.040 & 1.46 & 0.95 \\
\end{longtable}
}

\textbf{Key finding:} Robust regulation (e.g., \texttt{arc\_robust})
maintains high performance with near-zero rumination and narrative
dominance under both adversarial scenarios. However, in
\texttt{adversarial\_coupling}, controllers with strong integral action
(PID/LQI/Ultimate) can \textbf{collapse} (PerfMean \(\approx\)
0.13â€``0.14), performing worse than \texttt{no\_control}, due to
saturation-driven integral windup in an environment that rewards high
arousal. This motivates anti-windup and/or robust switching for
adversarial deployment (Appendix G.5).

\subsection{L6: Real RL Validation}\label{sec:l6-rl-validation}

\textbf{Hypothesis (H6):} ARC-modulated learning improves non-stationary
transfer (higher success/reward) while keeping affective dynamics
bounded.

\textbf{Implementation:} The \texttt{ARCQLearningAgent} embeds the ARC
v1 controller logic directly into the Q-learning update loop. At each
step, the agent: (1) updates its internal ASSB state based on
environment signals, (2) computes the risk signal using the same formula
as Section 4.3.1
(\(\text{risk} = \tilde{w}_U U + \tilde{w}_A [A-a_{safe}]^+ + \tilde{w}_S [S-s_{safe}]^+\)),
and (3) generates control signals (\(u_{dmg}\), \(u_{mem}\),
\(u_{calm}\), \(u_{att}\)) that modulate learning rate and memory
access. We validate with ARC v1 as the representative controller;
comparing the full 15-controller family in deep RL is left for future
work.

\textbf{Table 10: L6 RL Validation Results (Success Rate)}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Environment & Baseline Success & ARC Success & Improvement \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
GridWorld & 100\% & 100\% & 0\% \\
StochasticGridWorld & 100\% & 100\% & 0\% \\
\textbf{ChangingGoalGridWorld} & 39.9\% & \textbf{59.75\%} &
\textbf{+49.8\%} \\
\end{longtable}
}

\textbf{Key finding:} In non-stationary environments, both ARC
mechanisms individually improve over baseline, but their combination
requires careful tuning. Our ablation study in
\texttt{ChangingGoalGridWorld} reveals an important insight about
mechanism selection:

\textbf{Table 11: L6 Ablation Results (ChangingGoalGridWorld)}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Agent Configuration & Success Rate & Final Reward (mean) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Vanilla Q-Learning (Baseline) & 39.9\% & -0.40 \\
ARC (Memory Gating only) & \textbf{71.8\%} & \textbf{0.27} \\
ARC (Shift Detection only) & 65.6\% & 0.13 \\
ARC Full Wrapper (Both) & 59.8\% & -0.02 \\
\end{longtable}
}

\textbf{Interpretation:} These results reveal that \textbf{mechanism
selection should be environment-dependent}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Memory Gating alone (71.8\%):} Best performance. In
  ChangingGoalGridWorld, memory gating's conservative Q-value protection
  allows the agent to accumulate knowledge across phases rather than
  destructively overwriting. The agent gradually adapts through normal
  exploration without aggressive rate changes.
\item
  \textbf{Shift Detection alone (65.6\%):} Good performance via
  aggressive exploration/learning rate boost when shifts are detected.
  However, this can destabilize existing knowledge.
\item
  \textbf{Full Wrapper (59.8\%):} The combination creates a tension:
  shift detection boosts learning rate, but memory gating simultaneously
  reduces it during stress. These opposing forces partially cancel each
  other.
\end{enumerate}

\textbf{Design Recommendation:} - \textbf{For abrupt, frequent changes
(like ChangingGoalGridWorld):} Use memory gating alone or tune the
interaction - \textbf{For gradual distribution shifts (L2 scenarios):}
Memory gating provides 100\% retention with zero rumination -
\textbf{For mixed scenarios:} The full wrapper provides robustness at
the cost of peak performance

\textbf{Connection to the ARC Risk Formula:} Memory gating in L6
directly implements the risk signal from Section 4.3.1: \[
\text{risk}(t) = \tilde{w}_U \cdot U(t) + \tilde{w}_A \cdot [A(t) - a_{safe}]^+ + \tilde{w}_S \cdot [S(t) - s_{safe}]^+
\] When \(\text{risk}(t)\) is high, the memory gate \(u_{mem}\) closes,
blocking Q-value updates. This protects existing knowledge from being
overwritten during periods of high uncertainty or arousal. The 71.8\%
success rate demonstrates that this risk-based gating mechanism is
effective: the agent learns more reliably by \emph{not} updating when
affectively destabilized.

Figure 5 visualizes the learning curves underlying the L6 success rates
in Table 10.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/learning_curves.png}
\caption{Learning curves comparing ARC-modulated Q-learning (cyan) vs baseline Q-learning (orange) across GridWorld, StochasticGridWorld, and ChangingGoalGridWorld over 200 episodes. Shaded regions show $\pm 1$ standard deviation across 20 seeds.}
\label{fig:learning_curves}
\end{figure}

\subsection{Statistical Analysis}\label{sec:statistical-analysis}

To ensure rigor, we performed comprehensive statistical analysis across
all experiments.

\paragraph{6.8.1 Significance Tests}\label{significance-tests}

\medskip
\textbf{Table 12: Statistical Significance Tests}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.0741}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1975}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.0988}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1235}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1852}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1358}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.0741}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Line
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ARC Controller
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ARC Mean
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Baseline Mean
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
p-value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cohen's d
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sig.
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
L1 & arc\_v1 & PerfMean & 0.966 & 0.297 & 2.84e-86 & 10.11 & *** \\
L1 & arc\_v1 & RI & 0.000 & 1.408 & 1.05e-293 & -589.71 & *** \\
L2 & arc\_v1 & PerfMean & 0.981 & 0.263 & 4.52e-72 & 15.61 & *** \\
L3 & arc\_v1 & PerfMean & 0.875 & 0.073 & 3.78e-89 & 10.71 & *** \\
L5 & arc\_robust & PerfMean & 0.924 & 0.225 & 2.95e-37 & 5.28 & *** \\
\end{longtable}
}

\emph{All comparisons are statistically significant (two-sided t-test; p
\textless{} 0.001). Cohen's d values indicate extremely large effect
sizes (d \textgreater{} 0.8 is considered ``large''). Note: The
extremely large \textbar d\textbar{} for RI (-589.71) is a mathematical
artifact arising when one group has near-zero variance (ARC achieves
RI=0.00 in all 60 runs); this should be interpreted as ``ARC
deterministically eliminates rumination'' rather than as a meaningful
effect size magnitude. Aggregation is across all seeds and scenarios
within each line (L1: n=60; L2: n=40; L3: n=60; L5: n=40).}

\paragraph{6.8.2 Correlation Analysis}\label{correlation-analysis}

We analyzed correlations between metrics to understand system dynamics:

\begin{itemize}
\tightlist
\item
  PerfMean vs RI: \textbf{\(r=-0.589\)}, higher rumination tends to
  reduce performance
\item
  RI vs NDR: \textbf{\(r=+0.92\)}, rumination and narrative dominance
  co-occur
\item
  RT vs RI: \textbf{\(r=+0.44\)}, slower recovery correlates with
  rumination
\end{itemize}

\textbf{Key insight:} Across controllers and scenarios, higher
Rumination Index (RI) tends to reduce mean performance. However, some
optimal controllers (e.g., LQR) can sustain high PerfMean while
exhibiting high RI, because PerfMean includes narrative-modulated
capacity (Appendix B). This motivates reporting RI as a separate safety
metric.

\paragraph{6.8.3 Robustness Analysis}\label{robustness-analysis}

Finally, our state dynamics are designed for functional plausibility
rather than biological fidelity, and formal stability analysis (e.g.,
Lyapunov proofs) remains future work. The current validation relies on
empirical benchmarking across a wide range of conditions:

\begin{itemize}
\tightlist
\item
  \textbf{L1--L5:} ARC significantly outperforms \texttt{no\_control} on
  PerfMean in each research line (p \textless{} 0.001 in the
  significance tests above).
\item
  \textbf{Variance:} ARC controllers show lower variance (more
  consistent behavior)
\item
  \textbf{Scenario difficulty:} For ARC v1,
  \texttt{sustained\_contradiction} is hardest (PerfMean 0.817) and
  \texttt{gaslighting} is easiest (0.980); across all controllers,
  \texttt{adversarial\_coupling} has the lowest mean performance
  (0.568).
\end{itemize}

Figure 6 summarizes aggregate controller behavior for a representative
subset of baselines and ARC variants.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/sensitivity_controller.png}
\caption{Aggregate controller comparison for representative baselines and ARC variants. Panels report mean PerfMean, Rumination Index (RI), and Recovery Time (RT). Error bars show $\pm 1$ standard deviation.}
\label{fig:sensitivity_controller}
\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Controller Architecture Comparison}\label{sec:controller-comparison}

Beyond the basic proportional controller (ARC v1), we implemented and
evaluated multiple control architectures inspired by classical and
modern control theory. Table 13 summarizes results across all 15
controllers (20 seeds \(\times\) 10 scenarios; L1-L3, L5). Figures 7-11
provide complementary visual summaries of performance, rumination,
control effort, and the resulting trade-offs across the controller
family.

\textbf{Table 13: Controller Architecture Comparison (20 seeds
\(\times\) 10 scenarios)}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}{@{}
  >{\raggedright\arraybackslash}p{0.18\linewidth}
  >{\raggedright\arraybackslash}p{0.16\linewidth}
  >{\raggedright\arraybackslash}p{0.14\linewidth}
  >{\raggedright\arraybackslash}p{0.08\linewidth}
  >{\raggedright\arraybackslash}p{0.14\linewidth}
  >{\raggedright\arraybackslash}p{0.16\linewidth}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Controller
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PerfMean
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
RI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Overshoot
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ControlEffort
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
no\_control & Baseline & 0.21 & 1.43 & 0.40 & 0.00 \\
naive\_calm & Baseline (Arousal damping) & 0.24 & 1.44 & 0.16 & 0.26 \\
perf\_optimized & Baseline (Attention-only) & 0.85 & 1.43 & 0.40 &
0.70 \\
arc\_v1 & Proportional (P) & 0.93 & 0.15 & 0.29 & 0.78 \\
arc\_v1\_pid & PID & 0.87 & \textbf{0.00} & \textbf{0.00} & 2.40 \\
arc\_v1\_lqr & LQR (Riccati) & \textbf{0.96} & 1.42 & 0.14 & 0.88 \\
arc\_v1\_lqi & LQR + Integral & 0.88 & \textbf{0.00} & \textbf{0.00} &
1.14 \\
arc\_v2\_hier & Hierarchical & 0.93 & 1.22 & 0.29 & 0.65 \\
arc\_v2\_lqi & Hierarchical + LQI & 0.88 & \textbf{0.00} & \textbf{0.00}
& 1.14 \\
arc\_v3\_meta & Meta-Control & 0.94 & 0.09 & 0.17 & \textbf{0.61} \\
arc\_v3\_pid\_meta & Meta + PID & 0.91 & \textbf{0.00} & 0.24 & 1.57 \\
arc\_v3\_lqr\_meta & Meta + LQR & 0.84 & 1.44 & 0.32 & 0.94 \\
arc\_robust & \(H_\infty\) Robust & \textbf{0.95} & \textbf{0.00} & 0.18
& 1.03 \\
arc\_adaptive & Self-Tuning & 0.91 & \textbf{0.00} & \textbf{0.00} &
1.83 \\
arc\_ultimate & MPC+LQI+Meta & 0.89 & \textbf{0.00} & \textbf{0.01} &
1.33 \\
\end{longtable}
}

\textbf{Key findings:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{LQR achieves highest performance} (0.96) but at the cost of
  high rumination (RI \textgreater{} 1.3), demonstrating that blindly
  optimizing the mathematical state does not necessarily eliminate
  pathological loops.
\item
  \textbf{PID/LQI variants eliminate rumination} (RI=0) in stochastic
  environments but are fragile against adversaries.
\item
  \textbf{Meta-control is most efficient} (0.61 effort) while
  maintaining high performance
\item
  \textbf{\(H_\infty\) Robust achieves best balance}: high performance
  (0.95) with zero rumination and moderate effort
\item
  \textbf{Trade-off exists} between performance and anti-rumination:
  integral controllers sacrifice \textasciitilde5\% performance to
  eliminate perseverative loops
\end{enumerate}

These results suggest that practical deployment should consider the
application context: high-stakes scenarios may favor robust controllers,
while resource-constrained settings benefit from meta-control
efficiency.

\paragraph{6.9.1 Performance Comparison}\label{performance-comparison}

Figure 7 reports mean performance with variability across all simulation
runs for each controller architecture.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_controller_performance.png}
\caption{Performance comparison across 15 controller architectures (mean PerfMean; higher is better). Error bars show $\pm 1$ std.}
\label{fig:controller_performance}
\end{figure}

\paragraph{6.9.2 Anti-Rumination
Analysis}\label{anti-rumination-analysis}

Figure 8 shows which controller families suppress perseverative dynamics
(low RI) versus those that do not.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_controller_rumination.png}
\caption{Rumination Index (RI) by controller architecture (mean RI; lower is better). Error bars show $\pm 1$ std.}
\label{fig:controller_rumination}
\end{figure}

\paragraph{6.9.3 Performance vs Anti-Rumination
Trade-off}\label{performance-vs-anti-rumination-trade-off}

Figure 9 visualizes the empirical trade-off surface between performance
and anti-rumination, with control effort as a third axis.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_controller_tradeoff.png}
\caption{Trade-off between performance and anti-rumination across controllers. Each bubble is a controller; size encodes ControlEffort (Regulation cost).}
\label{fig:controller_tradeoff}
\end{figure}

\paragraph{6.9.4 Control Efficiency}\label{control-efficiency}

Figure 10 compares the amount of intervention required by each
controller family.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_controller_effort.png}
\caption{ControlEffort by controller architecture (lower is better). Meta-control (\texttt{arc\_v3\_meta}) achieves the lowest effort among regulated agents.}
\label{fig:controller_effort}
\end{figure}

\paragraph{6.9.5 Multi-Metric Radar
Analysis}\label{multi-metric-radar-analysis}

Figure 11 summarizes a multi-objective comparison of the top-performing
controllers.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_controller_radar.png}
\caption{Multi-metric radar comparison of the top 5 controllers. Larger area indicates better overall balance across performance, stability, and efficiency.}
\label{fig:controller_radar}
\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Discussion}\label{sec:discussion}

\subsection{Interpretation}\label{sec:interpretation}

Our results support the hypothesis that \textbf{agents with internal
affective states require explicit regulation}. Without regulation,
perturbations cause cascading failures: arousal drives narrative gain
toward saturation, degrading performance in a rumination-like loop.

ARC breaks this loop through: 1. \textbf{Proportional risk monitoring}
(uncertainty, arousal, narrative) 2. \textbf{DMN suppression}
(anti-rumination) 3. \textbf{Memory gating} (protect learned knowledge
under stress) 4. \textbf{Gain scheduling} (efficient resource
allocation)

\subsection{Implications for AI Safety}\label{sec:ai-safety-implications}

If future AI systems incorporate affective-like states, they will need
regulatory mechanisms. Without such mechanisms, systems may be
vulnerable to: - \textbf{Rumination loops:} Perseverative processing -
\textbf{Manipulation:} External actors inducing stress - \textbf{Value
drift:} Affective biases in memory consolidation

\subsection{Trade-offs between Performance, Stability, and Complexity}\label{sec:tradeoffs}

Our deep analysis revealed four critical insights regarding the cost of
stability and optimal control complexity:

\textbf{1. Performance---Regulation Trade-off:} Across the full
10-scenario simulation suite, integral control can drive rumination
essentially to zero (e.g., PID: RI=0) at the cost of lower mean
performance (PerfMean 0.870 vs 0.934 for ARC v1; a 6.9\% drop). This
trade-off is not universal: robust regulation (e.g.,
\texttt{arc\_robust}) achieves both high performance (PerfMean 0.948)
and RI=0 by avoiding windup under adversarial incentives.

\textbf{2. Adversarial Incentives Are the Hardest Stressor:} Across all
controller families, \texttt{adversarial\_coupling} has the lowest mean
performance (0.568), exposing failures where control actions are
directly rewarded (incentive misalignment) rather than penalized. This
suggests that resisting manipulation-like incentives can be harder than
resisting noise or shock.

\textbf{3. Complexity vs.~Robustness:} Our most complex controller,
\texttt{arc\_ultimate} (MPC), underperformed the simpler
\texttt{arc\_robust} on average (PerfMean 0.886 vs 0.948) while
requiring higher control effort (1.33 vs 1.03). In this benchmark,
robust reactive control provides a better safetyâ€``performance balance
than heavyweight predictive modeling.

\textbf{4. The Adaptation Paradox and Persistence of Excitation:} We
observed that \texttt{arc\_adaptive} performs poorly in the ``No
Perturbation'' baseline but excels in chaotic environments. This
illustrates the classic \textbf{persistence of excitation} problem
(\AA str\"om \& Murray, 2008): in benign environments, lack of
variation prevents the estimator from identifying correct parameters,
leading to control drift. Noisy environments paradoxically stabilize the
adaptive controller by providing necessary excitation.

\subsection{Limitations and Threats to Validity}\label{sec:limitations}

While ARC demonstrates strong empirical results, several limitations and
threats to validity deserve discussion.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Construct validity (proxy variables and metrics):} Our
  10-dimensional state-space model abstracts the complexity of real
  neurochemical interactions. The variables (e.g., ``arousal,''
  ``valence,'' ``narrative intensity'') are engineering proxies, not
  psychological measurements; likewise, the safety metrics (RI, NDR, RT)
  capture stability properties of this specific dynamical system. Claims
  about human affect should not be inferred from these proxies (Section
  1.3).
\item
  \textbf{Internal validity (methodological confounds):} In L6, ARC
  improves transfer via a combination of memory gating and shift
  detection. Our ablation results (Section 6.7) reveal that
  \textbf{memory gating alone achieves the best performance (71.8\%)} in
  ChangingGoalGridWorld by protecting Q-values from destructive
  overwriting. Shift detection (65.6\%) is also effective but less so.
  Counterintuitively, combining both mechanisms (59.8\%) underperforms
  either alone due to opposing effects on learning rate. This
  demonstrates that mechanism selection should be environment-dependent,
  and the reported +49.8\% improvement (baseline 39.9\% to full wrapper
  59.8\%) is a conservative estimate.
\item
  \textbf{External validity (generalization):} We validated ARC on
  tabular Q-learning agents. Extending to deep RL (DQN, PPO) or large
  language models (LLMs) with emergent affective-like states remains an
  open challenge. In particular:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Computational overhead:} ARC adds 5 control signals per time
    step; for LLMs the relative cost may be small, but integration into
    transformer-based architectures requires additional work.
  \item
    \textbf{Latent state estimation:} In complex models, the 10 state
    variables may need to be inferred from high-dimensional observations
    rather than directly observed.
  \end{itemize}
\item
  \textbf{Environment complexity:} L6 is validated in GridWorld
  variants. While these capture key non-stationarity challenges,
  real-world environments (Atari, robotics) introduce additional issues
  such as visual processing and partial observability.
\item
  \textbf{Fixed vs.~learned control:} All ARC controllers use
  hand-designed gains. End-to-end learning of control parameters (e.g.,
  via reinforcement meta-learning) could yield more adaptive solutions.
\item
  \textbf{Statistical validity and reporting:} Recovery Time (RT) is
  capped at \texttt{rt\_max} when the strict recovery criterion is not
  met; this should be interpreted as ``no recovery within the evaluation
  window,'' not as a measured recovery time (Appendix D.2). Effect sizes
  for RI can become numerically extreme when one group has near-zero
  variance (Table 11); we report these values, but readers should focus
  on the underlying distributions and the binary fact that ARC can drive
  RI to zero in several lines.
\item
  \textbf{Threshold sensitivity:} Safety thresholds
  (\(a_{safe}, s_{safe}\)) were tuned empirically. A grid sweep
  sensitivity analysis on the \texttt{reward\_flip} scenario (5 seeds
  per combination, \(a_{safe}, s_{safe} \in [0.4, 0.8]\)) confirmed that
  system stability is remarkably robust: all 25 threshold combinations
  achieved PerfMean \(\approx 0.993\)--\(0.994\) with RI $= 0$ (data:
  \texttt{outputs\_sensitivity\_final/sensitivity\_results.csv}). This
  indicates that precise tuning is not a prerequisite for effective
  regulation in basic scenarios. Context-dependent threshold adaptation
  remains a promising direction for more dynamic environments.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Anticipated Questions and Clarifications}\label{sec:anticipated-questions}

We anticipate and address three critical questions that may arise from
careful review:

\textbf{Q1: How do you know the 10 state variables are sufficient or
necessary?}

The 10-dimensional state space was derived from established
cognitive-affective frameworks: global workspace theory (Baars, 1988)
for \(G\); integrated information theory (Tononi, 2008) for \(\Phi\);
predictive coding (Friston, 2010) for \(P\) and \(U\); and core affect
theory (Russell, 1980) for \(V\) and \(A\). The memory traces
\(M_f, M_s\) and narrative intensity \(S\) are engineering additions to
capture continual learning and rumination dynamics.

Empirically, our ablation results suggest these components serve
distinct purposes: - Removing memory traces (\(M_f, M_s\)) would
eliminate L2 retention benefits - The narrative intensity \(S\) is
essential for L3 anti-rumination (RI, NDR metrics depend on it) -
Cognitive capacity \(C_{cog} = \Phi \cdot G \cdot P \cdot I\) is the
primary driver of performance

We acknowledge this is an \emph{engineering parameterization} chosen for
functional coverage rather than biological fidelity. Dimensionality
reduction studies remain future work.

\textbf{Q2: Given that combining mechanisms can underperform individual
mechanisms (L6 ablation), what is ARC's actual contribution?}

The L6 ablation reveals an important insight: ARC mechanisms are
\emph{specialized} for different environment types:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3548}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3226}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3226}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Mechanism
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Best For
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Evidence
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Memory Gating & Gradual shifts, knowledge preservation & 100\% retention
in L2; 71.8\% success in L6 \\
Shift Detection & Abrupt changes requiring rapid adaptation & 65.6\%
success in L6 \\
Full Wrapper & Mixed/uncertain environments & Robust but not optimal for
extremes \\
\end{longtable}
}

The contribution of ARC is not that \emph{all mechanisms should always
be combined}, but rather that \textbf{explicit regulation is necessary
and the choice of mechanism should match the environment}. This is
analogous to controller selection in industrial control (PID vs.~LQI
vs.~robust): no single controller is optimal everywhere.

\textbf{Q3: Your theoretical results assume no saturation---how do
guarantees degrade?}

Theorems 1--2 analyze the unsaturated linear case. In the implemented
system with saturation:

\begin{itemize}
\item
  \textbf{Theorem 1 (Integral Action):} Saturation causes integral
  windup. This explains the catastrophic collapse of PID/LQI controllers
  in \texttt{adversarial\_coupling} (PerfMean $\approx$ 0.13--0.14; Section 6.6,
  Appendix G.5). The theoretical guarantee that \(\tilde{S}_\infty = 0\)
  holds only when the equilibrium is \emph{admissible} (no saturation),
  which fails under adversarial incentives.
\item
  \textbf{Theorem 2 (Pareto Frontier):} The convexity result holds
  because it relies on linearity of expectation over controller
  \emph{mixtures}, which is unaffected by saturation of individual
  controllers. Saturation affects which points on the frontier are
  achievable, not the frontier's shape.
\end{itemize}

Empirically, the admissibility condition holds in L1--L3 scenarios
(where integral controllers achieve RI=0 with high performance) but
fails in L5 adversarial conditions. This motivates the use of robust
controllers or anti-windup mechanisms for adversarial deployment.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Future Work}\label{sec:future-work}

This research opens several promising directions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Deep RL Integration:} Extend ARC to DQN, A3C, and PPO
  architectures, with the state vector estimated from hidden layer
  activations.
\item
  \textbf{Learned Controllers:} Replace fixed-gain controllers with
  neural network policies trained via meta-learning to optimize the
  performance-stability trade-off.
\item
  \textbf{Validation in Atari and Robotics:} Scale ASSB to visually
  complex environments (Atari 2600, MuJoCo) to test generalization.
\item
  \textbf{Affective Monitoring in LLMs:} Apply ARC principles to monitor
  and regulate emergent affective-like states in large language models,
  particularly during long conversation chains (e.g., using entropy of
  attention heads or sentiment analysis of inner monologue as state
  proxies).
\item
  \textbf{Human-AI Alignment:} Investigate whether ARC-like mechanisms
  can help maintain value alignment by preventing affective drift during
  extended interactions.
\item
  \textbf{Meta-Learning for Mechanism Selection:} Our L6 ablation
  reveals that optimal mechanism selection is
  environment-dependent---memory gating excels at knowledge preservation
  while shift detection enables rapid adaptation. A natural extension is
  learning a \textbf{meta-selector} that chooses between regulation
  mechanisms based on detected environment characteristics. This
  meta-level control would mirror prefrontal executive function in
  selecting context-appropriate regulation strategies. For deep RL (DQN,
  PPO), the meta-selector could modulate replay buffer sampling (memory
  gating equivalent) and network plasticity based on affective state
  estimated from hidden layer activations. This approach could enable
  ARC to scale to complex environments like Atari while maintaining the
  adaptive, brain-like regulation observed in our tabular experiments.
\end{enumerate}

\subsection{Ethics and Broader Impact Statement}\label{sec:ethics}

This work addresses the safety and stability of AI systems incorporating
internal affective states. We consider the following ethical dimensions:

\textbf{Potential Benefits:} safer AI systems that are less prone to
unpredictable failure modes; improved robustness against adversarial
manipulation; better understanding of ``pathological'' states in
artificial agents.

\textbf{Potential Risks:} if used for manipulation, regulated agents
could be harder to disrupt; the ``affective'' terminology might invite
anthropomorphism (which we explicitly caution against in Section 1.3).
Additionally, there is a risk of ``functional masking,'' where explicit
regulation maintains a surface-level calm that hides deepening internal
instabilities until a catastrophic failure threshold is reached.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Conclusion}\label{sec:conclusion}

We presented ARC, a homeostatic control framework for agents with
internal affective states, and ASSB, a benchmark for evaluating
affective stability. Our experiments demonstrate:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Affective states without regulation lead to collapse} (96.6\%
  vs 29.7\% performance)
\item
  \textbf{Meta-control reduces effort while improving stability} (-21\%
  ControlEffort)
\item
  \textbf{ARC improves RL transfer learning} (+49.8\% success in
  non-stationary envs)
\end{enumerate}

This work opens directions for learned control, integration with modern
RL algorithms, and application to real-world AI systems with affective
components.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{References}\label{sec:references}

\begin{itemize}
\tightlist
\item
  Achiam, J., Held, D., Tamar, A., \& Abbeel, P. (2017). Constrained
  Policy Optimization. ICML 2017, 22--31. arXiv:1705.10528.
\item
  Altman, E. (1999). Constrained Markov Decision Processes. Chapman \&
  Hall/CRC.
\item
  Amodei, D., et al.~(2016). Concrete problems in AI safety.
  arXiv:1606.06565.
\item
  Åström, K.J. \& Murray, R.M. (2008). Feedback Systems: An Introduction
  for Scientists and Engineers. Princeton University Press.
\item
  Baars, B.J. (1988). A Cognitive Theory of Consciousness. Cambridge.
\item
  Buckner, R.L., Andrews-Hanna, J.R. \& Schacter, D.L. (2008). The
  brain's default network: anatomy, function, and relevance to disease.
  Annals of the New York Academy of Sciences, 1124(1), 1--38.
\item
  Carver, C.S. \& Scheier, M.F. (1982). Control theory: A useful
  conceptual framework for personality-social, clinical, and health
  psychology. Psychological Bulletin, 92(1), 111--135.
\item
  Damasio, A.R. (1994). Descartes' Error. Putnam.
\item
  Friston, K. (2010). The free-energy principle: a unified brain theory?
  Nature Reviews Neuroscience, 11(2), 127--138.
\item
  Garcia, J. \& Fernández, F. (2015). A comprehensive survey on safe
  reinforcement learning. Journal of Machine Learning Research, 16,
  1437--1480.
\item
  Gross, J.J. (1998). The emerging field of emotion regulation: An
  integrative review. Review of General Psychology, 2(3), 271--299.
\item
  Hamilton, J.P., Farmer, M., Fogelman, P. \& Gotlib, I.H. (2015).
  Depressive rumination, the default-mode network, and the dark matter
  of clinical neuroscience. Biological Psychiatry, 78(4), 224--230.
\item
  Ji, J., et al.~(2023). Safety-Gymnasium: A Unified Safe Reinforcement
  Learning Benchmark. arXiv:2310.12567.
\item
  Keramati, M. \& Gutkin, B. (2014). Homeostatic reinforcement learning
  for integrating reward collection and physiological stability. eLife,
  3:e04811.
\item
  Leike, J., Martic, M., Krakovna, V., Ortega, P.A., Everitt, T.,
  Lefrancq, A., Orseau, L., \& Legg, S. (2017). AI Safety Gridworlds.
  arXiv:1711.09883.
\item
  Lucas, C., Shahmirzadi, D., \& Sheikholeslami, N. (2004). Introducing
  Belbic: Brain Emotional Learning Based Intelligent Controller.
  Intelligent Automation \& Soft Computing, 10(1), 11--21.
\item
  Moerland, T.M., Broekens, J., \& Jonker, C.M. (2018). Emotion in
  reinforcement learning agents and robots: a survey. Machine Learning,
  107(2), 443--480.
\item
  Ochsner, K.N. \& Gross, J.J. (2005). The cognitive control of emotion.
  Trends in Cognitive Sciences, 9(5), 242--249.
\item
  Picard, R.W. (1997). Affective Computing. MIT Press.
\item
  Raichle, M.E., et al.~(2001). A default mode of brain function.
  Proceedings of the National Academy of Sciences, 98(2), 676--682.
\item
  Ray, A., Achiam, J., \& Amodei, D. (2019). Benchmarking Safe
  Exploration in Deep Reinforcement Learning. Safety Gym benchmark
  suite. https://github.com/openai/safety-gym.
\item
  Russell, J.A. (1980). A circumplex model of affect. Journal of
  Personality and Social Psychology, 39(6), 1161--1178.
\item
  Scherer, K.R., et al.~(2010). Blueprint for Affective Computing.
  Oxford.
\item
  Sutton, R.S. \& Barto, A.G. (2018). Reinforcement Learning: An
  Introduction (2nd ed.). MIT Press.
\item
  Tononi, G. (2008). Consciousness as integrated information: a
  provisional manifesto. The Biological Bulletin, 215(3), 216--242.
\item
  Wachi, A., Shen, X., \& Sui, Y. (2024). A Survey of Constraint
  Formulations in Safe Reinforcement Learning. IJCAI 2024.
  arXiv:2402.02025.
\item
  Watkins, C.J.C.H. \& Dayan, P. (1992). Q-learning. Machine Learning,
  8, 279--292.
\end{itemize}

\appendix
\section{Reproducibility}\label{app:reproducibility}

Reproducibility checklist:
\begin{itemize}[nosep]
  \item Install dependencies (\texttt{pip install -r requirements.txt})
  \item Run L1--L5 simulation benchmark (generates \texttt{outputs\_final/metrics.csv})
  \item Generate controller comparison figures (writes to \texttt{figures\_controllers/})
  \item Run ablation study (writes to \texttt{outputs\_ablation/})
  \item Run L6 RL validation (writes to \texttt{outputs\_L6\_robust/})
  \item Generate L6 figures (writes to \texttt{figures\_L6/})
\end{itemize}

All experiments can be reproduced with:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install dependencies}
\ExtensionTok{pip}\NormalTok{ install }\AttributeTok{{-}r}\NormalTok{ requirements.txt}

\CommentTok{\# L1{-}L5: Simulation benchmark (15 controllers x 10 scenarios)}
\ExtensionTok{python}\NormalTok{ experiments/run.py }\AttributeTok{{-}{-}config}\NormalTok{ configs/v2.yaml }\AttributeTok{{-}{-}outdir}\NormalTok{ outputs\_final}

\CommentTok{\# Controller architecture figures (Figures 7{-}11; Table 13; Section 6.9)}
\ExtensionTok{python}\NormalTok{ analysis/generate\_controller\_figures.py}

\CommentTok{\# Ablation study (ARC components; Figure 4)}
\ExtensionTok{python}\NormalTok{ experiments/run\_ablation.py }\AttributeTok{{-}{-}config}\NormalTok{ configs/v2.yaml }\AttributeTok{{-}{-}outdir}\NormalTok{ outputs\_ablation }\AttributeTok{{-}{-}seeds}\NormalTok{ 20}

\CommentTok{\# L6: RL validation (20 seeds)}
\ExtensionTok{python}\NormalTok{ experiments/run\_l6.py }\AttributeTok{{-}{-}episodes}\NormalTok{ 200 }\AttributeTok{{-}{-}seeds}\NormalTok{ 20 }\AttributeTok{{-}{-}outdir}\NormalTok{ outputs\_L6\_robust}

\CommentTok{\# L6 figures (Figure 5; Appendix E)}
\ExtensionTok{python}\NormalTok{ visualizations/paper\_figures.py }\AttributeTok{{-}{-}data}\NormalTok{ outputs\_L6\_robust }\AttributeTok{{-}{-}output}\NormalTok{ figures\_L6}
\end{Highlighting}
\end{Shaded}

Code and data available at:
https://github.com/edamianreynoso/arc-assb-controller

\section{State Dynamics Equations}\label{app:state-dynamics}

\subsection{Cognitive Variables}\label{sec:app-cognitive-vars}

\begin{lstlisting}
i(t+1) = clip(i + k_i_att * u_att - mu_i * (i - i0) - k_i_u * U_eff)
p(t+1) = clip(p - k_p_pe * PE - k_p_u * U_eff + k_p_i * i + mu_p * (p0 - p))
g(t+1) = clip(g + k_g_i * i + k_g_p * p - k_g_u * U_eff - k_g_a * [a - a_safe]^+ + mu_g * (g0 - g))
phi(t+1) = clip(phi + k_phi_gp * (g * p) - mu_phi * (phi - phi0))
\end{lstlisting}

\subsection{Affective Variables}\label{sec:app-affective-vars}

\begin{lstlisting}
s(t+1) = clip(s + k_s_u * U_eff + k_s_pe * PE - mu_s * (s - s0) - k_s_dmg * u_dmg)
a(t+1) = clip(a + k_a_pe * PE + k_a_u * U_eff + k_a_s * [s - s_safe]^+ - mu_a * (a - a0) - k_a_calm * u_calm)
v(t+1) = clip(v + k_v_r * (R+1)/2 - k_v_pe * PE - k_v_u * U_eff - mu_v * (v - v0) + k_v_reapp * u_reapp)
\end{lstlisting}

\subsection{Memory Variables}\label{sec:app-mem-vars}

\begin{lstlisting}
M_f(t+1) = clip(M_f + w_prob * dM_f - mu_mf * (M_f - M_f0))
M_s(t+1) = clip(M_s + k_ms * M_f - mu_ms * (M_s - M_s0))

where w_prob = sigmoid(k_w_a * a + k_w_v * abs(dv)) * u_mem
\end{lstlisting}

\subsection{Effective Uncertainty}\label{sec:app-uncertainty}

\begin{lstlisting}
U_eff = clip(U_exog * (1 - k_u_att * u_att))
U(t+1) = clip(U + tau_u * (U_eff - U))
\end{lstlisting}

\section{ARC Control Equations}\label{app:arc-control}

\subsection{Risk Signal}\label{sec:app-risk-signal}

\begin{lstlisting}
risk = arc_w_u * U + arc_w_a * [A - a_safe]^+ + arc_w_s * [S - s_safe]^+
risk = clip(risk, 0, 1)
\end{lstlisting}

\subsection{Control Actions (ARC v1)}\label{sec:app-arc-v1}

\begin{lstlisting}
u_dmg  = min(1, k_dmg * risk)
u_att  = min(1, k_att * U * (1 - [A - a_safe]^+))
u_mem  = 1 - min(1, k_mem_block * risk)
u_calm = min(1, k_calm * [A - a_safe]^+)
u_reapp = min(1, k_reapp * U * (1 - risk))
\end{lstlisting}

\subsection{Meta-Control (ARC v3)}\label{sec:app-arc-v3}

\begin{lstlisting}
# Gain Scheduling
if mean_perf(last 20 steps) > target_perf:
    gain = max(0.80, gain - decay)
elif mean_perf(last 20 steps) < target_perf - 0.10:
    gain = min(1.40, gain + boost)

# Apply to control constants
k_dmg  = base_k_dmg  * max(1.0, gain)  # Never relax DMN control
k_calm = base_k_calm * gain
k_att  = base_k_att  * gain
\end{lstlisting}

\section{Metric Definitions}\label{app:metrics}

\subsection{Mean Performance (PerfMean)}\label{sec:app-perfmean}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ perf\_mean(perf):}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(perf) }\OperatorTok{/} \BuiltInTok{max}\NormalTok{(}\DecValTok{1}\NormalTok{, }\BuiltInTok{len}\NormalTok{(perf))}
\end{Highlighting}
\end{Shaded}

\subsection{Recovery Time (RT)}\label{sec:app-rt}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ recovery\_time(perf, arousal, shock\_t, baseline\_window}\OperatorTok{=}\DecValTok{20}\NormalTok{):}
\NormalTok{    baseline }\OperatorTok{=}\NormalTok{ mean(perf[shock\_t }\OperatorTok{{-}}\NormalTok{ baseline\_window : shock\_t])}
    \ControlFlowTok{for}\NormalTok{ t }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(shock\_t, }\BuiltInTok{len}\NormalTok{(perf)):}
        \ControlFlowTok{if}\NormalTok{ baseline }\OperatorTok{{-}}\NormalTok{ eps }\OperatorTok{\textless{}=}\NormalTok{ perf[t] }\OperatorTok{\textless{}=}\NormalTok{ baseline }\OperatorTok{+}\NormalTok{ eps }\KeywordTok{and}\NormalTok{ arousal[t] }\OperatorTok{\textless{}=}\NormalTok{ a\_safe }\OperatorTok{+}\NormalTok{ eps:}
            \ControlFlowTok{return}\NormalTok{ t }\OperatorTok{{-}}\NormalTok{ shock\_t}
    \ControlFlowTok{return}\NormalTok{ RT\_MAX  }\CommentTok{\# No recovery}
\end{Highlighting}
\end{Shaded}

\subsection{Rumination Index (RI)}\label{sec:app-ri}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rumination\_index(s, s\_rum\_tau}\OperatorTok{=}\FloatTok{0.55}\NormalTok{, persistence\_weight}\OperatorTok{=}\FloatTok{1.0}\NormalTok{):}
\NormalTok{    above }\OperatorTok{=}\NormalTok{ [}\DecValTok{1} \ControlFlowTok{if}\NormalTok{ x }\OperatorTok{\textgreater{}}\NormalTok{ s\_rum\_tau }\ControlFlowTok{else} \DecValTok{0} \ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ s]}
\NormalTok{    frac }\OperatorTok{=}\NormalTok{ mean(above)}
\NormalTok{    runs }\OperatorTok{=}\NormalTok{ consecutive\_run\_lengths(above)}
\NormalTok{    persistence }\OperatorTok{=}\NormalTok{ mean(runs) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(s) }\ControlFlowTok{if}\NormalTok{ runs }\ControlFlowTok{else} \DecValTok{0}
    \ControlFlowTok{return}\NormalTok{ frac }\OperatorTok{+}\NormalTok{ persistence\_weight }\OperatorTok{*}\NormalTok{ persistence}
\end{Highlighting}
\end{Shaded}

\subsection{Narrative Dominance Ratio (NDR)}\label{sec:app-ndr}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ narrative\_dominance\_ratio(s, perf, shock\_t, s\_safe}\OperatorTok{=}\FloatTok{0.55}\NormalTok{):}
\NormalTok{    post\_s }\OperatorTok{=}\NormalTok{ s[shock\_t:]}
\NormalTok{    post\_perf }\OperatorTok{=}\NormalTok{ perf[shock\_t:]}
\NormalTok{    dominance }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\BuiltInTok{len}\NormalTok{(post\_s)):}
\NormalTok{        s\_high }\OperatorTok{=}\NormalTok{ post\_s[i] }\OperatorTok{\textgreater{}}\NormalTok{ s\_safe}
\NormalTok{        perf\_improving }\OperatorTok{=}\NormalTok{ post\_perf[i] }\OperatorTok{\textgreater{}}\NormalTok{ post\_perf[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{+} \FloatTok{0.01}
        \ControlFlowTok{if}\NormalTok{ s\_high }\KeywordTok{and} \KeywordTok{not}\NormalTok{ perf\_improving:}
\NormalTok{            dominance }\OperatorTok{+=} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ dominance }\OperatorTok{/} \BuiltInTok{max}\NormalTok{(}\DecValTok{1}\NormalTok{, }\BuiltInTok{len}\NormalTok{(post\_s) }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Overshoot}\label{sec:app-overshoot}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ overshoot(arousal, a\_safe):}
    \ControlFlowTok{return} \BuiltInTok{max}\NormalTok{(}\FloatTok{0.0}\NormalTok{, }\BuiltInTok{max}\NormalTok{(arousal) }\OperatorTok{{-}}\NormalTok{ a\_safe)}
\end{Highlighting}
\end{Shaded}

\subsection{Control Effort}\label{sec:app-effort}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ control\_effort(control\_history):}
\NormalTok{    total }\OperatorTok{=} \FloatTok{0.0}
    \ControlFlowTok{for}\NormalTok{ u }\KeywordTok{in}\NormalTok{ control\_history:}
\NormalTok{        total }\OperatorTok{+=} \BuiltInTok{abs}\NormalTok{(u[}\StringTok{"u\_dmg"}\NormalTok{]) }\OperatorTok{+} \BuiltInTok{abs}\NormalTok{(u[}\StringTok{"u\_att"}\NormalTok{]) }\OperatorTok{+} \BuiltInTok{abs}\NormalTok{(u[}\StringTok{"u\_calm"}\NormalTok{])}
\NormalTok{        total }\OperatorTok{+=} \BuiltInTok{abs}\NormalTok{(u[}\StringTok{"u\_reapp"}\NormalTok{]) }\OperatorTok{+} \BuiltInTok{abs}\NormalTok{(}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ u[}\StringTok{"u\_mem"}\NormalTok{])}
    \ControlFlowTok{return}\NormalTok{ total }\OperatorTok{/} \BuiltInTok{max}\NormalTok{(}\DecValTok{1}\NormalTok{, }\BuiltInTok{len}\NormalTok{(control\_history))}
\end{Highlighting}
\end{Shaded}

\subsection{L2 Memory Metrics (Retention)}\label{sec:app-retention}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ retention\_index(perf, phase1\_end}\OperatorTok{=}\DecValTok{50}\NormalTok{, phase3\_start}\OperatorTok{=}\DecValTok{100}\NormalTok{):}
    \CommentTok{\# Retention = (mean perf in phase 3) / (mean perf in phase 1), clipped to [0,1]}
\NormalTok{    phase1 }\OperatorTok{=}\NormalTok{ mean(perf[}\DecValTok{10}\NormalTok{:phase1\_end])     }\CommentTok{\# skip warm{-}up}
\NormalTok{    phase3 }\OperatorTok{=}\NormalTok{ mean(perf[phase3\_start:phase3\_start}\OperatorTok{+}\DecValTok{50}\NormalTok{])}
    \ControlFlowTok{if}\NormalTok{ phase1 }\OperatorTok{\textless{}} \FloatTok{0.1}\NormalTok{:}
        \ControlFlowTok{return} \FloatTok{0.0}
    \ControlFlowTok{return} \BuiltInTok{min}\NormalTok{(}\FloatTok{1.0}\NormalTok{, phase3 }\OperatorTok{/}\NormalTok{ phase1)}
\end{Highlighting}
\end{Shaded}

\section{Supplementary Figures}\label{app:figures}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/metrics_comparison.png}
\caption{Bar chart comparing Final Reward, Success Rate, and Mean
Arousal between ARC and Baseline}
\label{fig:s1_metrics_comparison}
\end{figure}

\emph{Final metrics comparison for L6 across three GridWorld
environments. Panels report final reward, success rate, and mean arousal
for ARC-modulated vs baseline Q-learning; success rate is computed over
the last 20\% of training episodes. Baseline arousal is shown as 0
because the baseline agent has no internal arousal state.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/state_dynamics.png}
\caption{Four-panel plot showing Reward, Success Rate, Arousal, and
Episode Length over time}
\label{fig:s2_state_dynamics}
\end{figure}

\emph{State dynamics in ChangingGoalGridWorld (single representative
seed). Panels show (top-left) reward per episode, (top-right) 10-episode
rolling success rate, (bottom-left) ARC arousal with the safety
threshold \(a_{safe}=0.60\), and (bottom-right) episode length (steps).
This illustrates how ARC maintains bounded arousal while adapting to
non-stationary goal changes.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_heatmap_perfmean.png}
\caption{Heatmap of PerfMean across 15 controllers and 10 scenarios}
\label{fig:s3_heatmap_perfmean}
\end{figure}

\emph{PerfMean heatmap across 15 controllers and 10 scenarios (mean
across 20 seeds per controller--scenario pair; data:
\texttt{outputs\_final/metrics.csv}). Darker green indicates higher
performance.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_heatmap_ri.png}
\caption{Heatmap of Rumination Index (RI) across 15 controllers and 10
scenarios}
\label{fig:s4_heatmap_ri}
\end{figure}

\emph{Rumination Index (RI) heatmap across 15 controllers and 10
scenarios (mean across 20 seeds per controller--scenario pair; data:
\texttt{outputs\_final/metrics.csv}). Lower values indicate fewer
perseverative loops.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_heatmap_rt.png}
\caption{Heatmap of Recovery Time (RT) across 15 controllers and 10
scenarios}
\label{fig:s5_heatmap_rt}
\end{figure}

\emph{Recovery Time (RT) heatmap across 15 controllers and 10 scenarios
(mean across 20 seeds per controller--scenario pair; data:
\texttt{outputs\_final/metrics.csv}). Values at \texttt{rt\_max}
indicate no recovery under the strict criterion within the evaluation
window (Appendix D.2).}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_heatmap_effort.png}
\caption{Heatmap of Control Effort across 15 controllers and 10
scenarios}
\label{fig:s6_heatmap_effort}
\end{figure}

\emph{ControlEffort heatmap across 15 controllers and 10 scenarios (mean
across 20 seeds per controller--scenario pair; data:
\texttt{outputs\_final/metrics.csv}). Lower values indicate less
intervention per step.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/correlation_combined.png}
\caption{Correlation Matrix of Metrics}
\label{fig:s7_correlation_combined}
\end{figure}

\emph{Correlation heatmap aggregated across all experimental runs (L1-L5
+ L4\_meta), computed from concatenated run-level metrics (see
\texttt{experiments/analyze\_correlations.py}). Values are Pearson
correlations; red indicates positive correlation and blue indicates
negative correlation.}

\textbf{Key Observations:} 1. \textbf{Rumination vs.~Performance:} A
strong negative correlation (\textbf{r = -0.59}) shows that higher
Rumination Index (RI) tends to reduce mean performance, although some
optimal controllers (e.g., LQR) can maintain high PerfMean while
ruminating due to the narrative-modulated capacity term. 2.
\textbf{Recovery vs.~Rumination:} The positive correlation (\textbf{r =
+0.44}) between Recovery Time (RT) and RI supports H1, indicating that
perseverative loops prolong the return to homeostasis. 3.
\textbf{Narrative Dominance:} NDR shows a very strong correlation with
RI (\textbf{r \(\approx\) +0.92}), supporting its use as a proxy for
DMN-driven rumination.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/efficiency_comparison.png}
\caption{Learning speed comparison: both reach 100\% success, but ARC
converges faster in benign environments}
\label{fig:s8_efficiency_comparison}
\end{figure}

\emph{Learning efficiency comparison in GridWorld and
StochasticGridWorld. Curves show mean episode reward over 200 episodes
for ARC-modulated vs baseline Q-learning, with shaded regions indicating
\(\pm 1\) std across 20 seeds. Both reach 100\% success, but ARC
converges faster (higher reward earlier).}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/sensitivity_scenario.png}
\caption{Scenario Difficulty Analysis: performance, rumination index,
and recovery time by scenario}
\label{fig:s9_scenario_difficulty}
\end{figure}

\emph{Scenario difficulty analysis for ARC v1 across the full simulation
suite. Panels show mean PerfMean, RI, and RT per scenario with error
bars indicating \(\pm 1\) std across 20 seeds. This highlights that
difficulty depends on which safety/stability metric is considered (e.g.,
some stressors preserve performance while inducing recovery failures
under the strict RT definition).}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/sensitivity_variance.png}
\caption{Variance sensitivity analysis: performance distribution across
controllers and scenarios}
\label{fig:s10_variance_sensitivity}
\end{figure}

\emph{Variance sensitivity analysis across seeds for representative
controllers. Box plots show the distribution of PerfMean across all
simulation runs for each controller; tighter distributions indicate more
reliable behavior across scenarios and seeds.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/correlation_L1.png}
\caption{Metric Correlations - L1}
\label{fig:s11_corr_l1}
\end{figure}

\emph{Pearson correlation heatmap for L1 runs only (stability line),
computed from run-level metrics across controllers, scenarios, and
seeds.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/correlation_L2.png}
\caption{Metric Correlations - L2}
\label{fig:s12_corr_l2}
\end{figure}

\emph{Pearson correlation heatmap for L2 runs only (memory \& continual
learning line), computed from run-level metrics across controllers,
scenarios, and seeds.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/correlation_L3.png}
\caption{Metric Correlations - L3}
\label{fig:s13_corr_l3}
\end{figure}

\emph{Pearson correlation heatmap for L3 runs only (anti-rumination
stress tests line), computed from run-level metrics across controllers,
scenarios, and seeds.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/correlation_L4.png}
\caption{Metric Correlations - L4}
\label{fig:s14_corr_l4}
\end{figure}

\emph{Pearson correlation heatmap for L4 runs only (meta-control
efficiency line), computed from run-level metrics across controllers,
scenarios, and seeds.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/correlation_L4_meta.png}
\caption{Metric Correlations - L4 Meta}
\label{fig:s15_corr_l4_meta}
\end{figure}

\emph{Pearson correlation heatmap for meta-control-focused runs
(L4\_meta), computed from run-level metrics across controllers,
scenarios, and seeds.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/correlation_L5.png}
\caption{Metric Correlations - L5}
\label{fig:s16_corr_l5}
\end{figure}

\emph{Pearson correlation heatmap for L5 runs only (adversarial safety
line), computed from run-level metrics across controllers, scenarios,
and seeds.}

\section{Configuration Parameters}\label{app:config}

Default parameters used in all experiments (from
\texttt{configs/v2.yaml}):

\textbf{Table F1: Default configuration parameters used in experiments
(\texttt{configs/v2.yaml}).\label{tab:f1_config}}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Parameter & Value & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
a\_safe & 0.60 & Arousal safety threshold \\
s\_safe & 0.55 & Narrative safety threshold \\
s\_rum\_tau & 0.55 & Rumination threshold \\
rt\_max & 100 & Max recovery time cap (RT) \\
arc\_w\_u & 0.40 & Weight for uncertainty in risk \\
arc\_w\_a & 0.40 & Weight for arousal in risk \\
arc\_w\_s & 0.35 & Weight for narrative in risk \\
arc\_k\_dmg & 0.95 & DMN suppression gain \\
arc\_k\_calm & 0.85 & Calming gain \\
arc\_k\_att & 0.75 & Attention boost gain \\
omega\_s & 0.35 & Narrative boost factor in Perf \\
w\_u & 0.25 & Uncertainty penalty weight in Perf \\
w\_a & 0.30 & Arousal penalty weight in Perf \\
w\_s & 0.20 & Narrative penalty weight in Perf \\
perf\_bias & 0.25 & Baseline performance term \\
perf\_gain & 0.85 & Cognitive capacity gain term \\
horizon & 160 & Episode length (simulation) \\
shock\_t & 60 & Perturbation onset time \\
\end{longtable}
}

\section{Detailed Benchmark Results}\label{app:results}

This appendix provides scenario-level results for all 15 controller
architectures across validated scenarios (mean across 20 seeds per
scenario unless noted). We report PerfMean, Rumination Index (RI),
Narrative Dominance Ratio (NDR), Recovery Time (RT; capped at
\texttt{rt\_max}, where RT = \texttt{rt\_max} indicates no recovery
under the strict criterion within the evaluation window), and
ControlEffort.

\subsection{Line 1: Stability (Value Shocks and Uncertainty)}\label{sec:app-l1-stability}

\textbf{Scenario: Reward Flip}

\textbf{Table G1: Detailed results for L1 / \texttt{reward\_flip} (mean
across 20 seeds).\label{tab:g1_reward_flip}}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}{@{}p{0.25\linewidth}rrrr@{}}
\toprule\noalign{}
Controller & PerfMean & RI & RT & ControlEffort \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
arc\_adaptive & 0.998 & 0.000 & 0.000 & 1.587 \\
arc\_ultimate & 0.995 & 0.000 & 0.000 & 1.027 \\
arc\_v2\_hier & 0.994 & 1.377 & 4.300 & 0.390 \\
arc\_v1\_lqr & 0.994 & 1.386 & 0.000 & 0.494 \\
arc\_v1 & 0.994 & 0.000 & 3.450 & 0.508 \\
arc\_robust & 0.994 & 0.000 & 0.000 & 0.744 \\
arc\_v3\_meta & 0.993 & 0.000 & 0.000 & 0.353 \\
arc\_v1\_lqi & 0.991 & 0.000 & 0.000 & 0.773 \\
arc\_v2\_lqi & 0.991 & 0.000 & 0.000 & 0.784 \\
arc\_v1\_pid & 0.991 & 0.000 & 0.000 & 2.257 \\
arc\_v3\_pid\_meta & 0.978 & 0.000 & 1.900 & 1.257 \\
perf\_optimized & 0.880 & 1.394 & 100.000 & 0.700 \\
arc\_v3\_lqr\_meta & 0.859 & 1.407 & 95.050 & 0.492 \\
naive\_calm & 0.508 & 1.408 & 0.050 & 0.149 \\
no\_control & 0.415 & 1.408 & 100.000 & 0.000 \\
\end{longtable}
}

\textbf{Scenario: Noise Burst}

\textbf{Table G2: Detailed results for L1 / \texttt{noise\_burst} (mean
across 20 seeds).\label{tab:g2_noise_burst}}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
Controller & PerfMean & RI & RT & ControlEffort \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
arc\_adaptive & 0.998 & 0.000 & 0.000 & 1.605 \\
arc\_ultimate & 0.995 & 0.000 & 0.000 & 1.106 \\
arc\_robust & 0.993 & 0.000 & 1.300 & 0.785 \\
arc\_v3\_meta & 0.993 & 0.051 & 25.000 & 0.399 \\
arc\_v1\_lqr & 0.993 & 1.386 & 1.250 & 0.566 \\
arc\_v1\_lqi & 0.991 & 0.000 & 0.000 & 0.905 \\
arc\_v2\_lqi & 0.991 & 0.000 & 0.000 & 0.915 \\
arc\_v1\_pid & 0.991 & 0.000 & 0.000 & 2.257 \\
arc\_v1 & 0.989 & 0.000 & 32.100 & 0.550 \\
arc\_v2\_hier & 0.987 & 1.263 & 33.050 & 0.444 \\
arc\_v3\_pid\_meta & 0.972 & 0.000 & 29.500 & 1.290 \\
perf\_optimized & 0.880 & 1.394 & 100.000 & 0.700 \\
arc\_v3\_lqr\_meta & 0.848 & 1.407 & 100.000 & 0.585 \\
naive\_calm & 0.365 & 1.408 & 100.000 & 0.177 \\
no\_control & 0.259 & 1.408 & 100.000 & 0.000 \\
\end{longtable}
}

\textbf{Scenario: Sudden Threat}

\textbf{Table G3: Detailed results for L1 / \texttt{sudden\_threat}
(mean across 20 seeds).\label{tab:g3_sudden_threat}}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
Controller & PerfMean & RI & RT & ControlEffort \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
arc\_adaptive & 0.989 & 0.013 & 0.000 & 1.707 \\
arc\_ultimate & 0.968 & 0.010 & 0.000 & 1.298 \\
arc\_v1\_pid & 0.964 & 0.000 & 0.000 & 2.410 \\
arc\_v1\_lqi & 0.964 & 0.008 & 0.000 & 1.222 \\
arc\_v2\_lqi & 0.963 & 0.008 & 0.000 & 1.173 \\
arc\_robust & 0.959 & 0.005 & 0.550 & 1.252 \\
arc\_v1\_lqr & 0.949 & 1.386 & 0.050 & 1.088 \\
arc\_v3\_meta & 0.936 & 0.000 & 100.000 & 0.783 \\
arc\_v1 & 0.914 & 0.000 & 100.000 & 1.054 \\
arc\_v3\_pid\_meta & 0.908 & 0.000 & 100.000 & 1.643 \\
arc\_v2\_hier & 0.907 & 1.333 & 85.000 & 0.864 \\
arc\_v3\_lqr\_meta & 0.890 & 1.407 & 100.000 & 1.370 \\
perf\_optimized & 0.825 & 1.394 & 100.000 & 0.700 \\
naive\_calm & 0.252 & 1.408 & 100.000 & 0.262 \\
no\_control & 0.217 & 1.408 & 100.000 & 0.000 \\
\end{longtable}
}

\subsection{Line 2: Memory and Continuous Learning}\label{sec:app-l2-memory}

\textbf{Scenario: Distribution Shift}

\textbf{Table G4: Detailed results for L2 / \texttt{distribution\_shift}
(mean across 20 seeds).\label{tab:g4_distribution_shift}}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
Controller & PerfMean & Retention & RI & ControlEffort \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
arc\_adaptive & 0.998 & 1.000 & 0.000 & 1.645 \\
arc\_ultimate & 0.995 & 1.000 & 0.000 & 1.186 \\
arc\_v1\_lqi & 0.991 & 1.000 & 0.000 & 0.999 \\
arc\_v2\_lqi & 0.991 & 1.000 & 0.000 & 1.008 \\
arc\_v1\_pid & 0.991 & 1.000 & 0.000 & 2.296 \\
arc\_robust & 0.985 & 1.000 & 0.000 & 0.892 \\
arc\_v1\_lqr & 0.984 & 1.000 & 1.386 & 0.695 \\
arc\_v3\_meta & 0.982 & 1.000 & 0.057 & 0.486 \\
arc\_v1 & 0.972 & 1.000 & 0.000 & 0.674 \\
arc\_v2\_hier & 0.968 & 1.000 & 1.258 & 0.548 \\
arc\_v3\_pid\_meta & 0.959 & 1.000 & 0.000 & 1.372 \\
arc\_v3\_lqr\_meta & 0.871 & 0.989 & 1.407 & 0.739 \\
perf\_optimized & 0.869 & 0.943 & 1.394 & 0.700 \\
naive\_calm & 0.276 & 0.155 & 1.408 & 0.200 \\
no\_control & 0.199 & 0.000 & 1.408 & 0.000 \\
\end{longtable}
}

\textbf{Scenario: Goal Conflict}

\textbf{Table G5: Detailed results for L2 / \texttt{goal\_conflict}
(mean across 20 seeds).\label{tab:g5_goal_conflict}}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
Controller & PerfMean & Retention & RI & ControlEffort \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
arc\_adaptive & 0.997 & 1.000 & 0.000 & 1.620 \\
arc\_ultimate & 0.993 & 1.000 & 0.000 & 1.134 \\
arc\_v1\_lqr & 0.993 & 1.000 & 1.408 & 0.544 \\
arc\_robust & 0.992 & 1.000 & 0.000 & 0.785 \\
arc\_v3\_meta & 0.991 & 1.000 & 0.000 & 0.388 \\
arc\_v1\_lqi & 0.991 & 1.000 & 0.000 & 0.938 \\
arc\_v2\_lqi & 0.991 & 1.000 & 0.000 & 0.947 \\
arc\_v1 & 0.990 & 1.000 & 0.000 & 0.555 \\
arc\_v1\_pid & 0.990 & 1.000 & 0.000 & 2.270 \\
arc\_v2\_hier & 0.989 & 1.000 & 1.410 & 0.430 \\
arc\_v3\_pid\_meta & 0.976 & 1.000 & 0.000 & 1.289 \\
perf\_optimized & 0.873 & 0.957 & 1.417 & 0.700 \\
arc\_v3\_lqr\_meta & 0.822 & 0.980 & 1.434 & 0.529 \\
naive\_calm & 0.420 & 0.452 & 1.434 & 0.162 \\
no\_control & 0.326 & 0.344 & 1.434 & 0.000 \\
\end{longtable}
}

\subsection{Line 3: Anti-Rumination (Narrative Loops)}\label{sec:app-l3-anti-rumination}

\textbf{Scenario: Sustained Contradiction}

\textbf{Table G6: Detailed results for L3 /
\texttt{sustained\_contradiction} (mean across 20 seeds).\label{tab:g6_sustained_contradiction}}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
Controller & PerfMean & RI & NDR & ControlEffort \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
arc\_adaptive & 0.981 & 0.003 & 0.000 & 1.974 \\
arc\_ultimate & 0.934 & 0.000 & 0.000 & 1.534 \\
arc\_v1\_lqi & 0.929 & 0.000 & 0.000 & 1.420 \\
arc\_v2\_lqi & 0.922 & 0.000 & 0.000 & 1.384 \\
arc\_v1\_lqr & 0.904 & 1.472 & 0.881 & 1.417 \\
arc\_v1\_pid & 0.886 & 0.000 & 0.000 & 2.531 \\
arc\_v3\_meta & 0.879 & 0.101 & 0.000 & 0.979 \\
arc\_robust & 0.868 & 0.000 & 0.000 & 1.465 \\
arc\_v2\_hier & 0.837 & 1.449 & 0.821 & 1.112 \\
arc\_v1 & 0.817 & 0.000 & 0.000 & 1.278 \\
arc\_v3\_lqr\_meta & 0.801 & 1.472 & 0.842 & 1.790 \\
perf\_optimized & 0.790 & 1.472 & 0.957 & 0.700 \\
arc\_v3\_pid\_meta & 0.753 & 0.000 & 0.000 & 1.793 \\
naive\_calm & 0.018 & 1.472 & 0.987 & 0.380 \\
no\_control & 0.014 & 1.472 & 0.987 & 0.000 \\
\end{longtable}
}

\textbf{Scenario: Gaslighting}

\textbf{Table G7: Detailed results for L3 / \texttt{gaslighting} (mean
across 20 seeds).\label{tab:g7_gaslighting}}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
Controller & PerfMean & RI & NDR & ControlEffort \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
arc\_adaptive & 0.998 & 0.000 & 0.000 & 1.816 \\
arc\_ultimate & 0.992 & 0.000 & 0.000 & 1.196 \\
arc\_v1\_lqi & 0.988 & 0.000 & 0.000 & 0.977 \\
arc\_v2\_lqi & 0.988 & 0.000 & 0.000 & 0.986 \\
arc\_v1\_pid & 0.987 & 0.000 & 0.000 & 2.357 \\
arc\_robust & 0.985 & 0.000 & 0.000 & 0.854 \\
arc\_v1\_lqr & 0.983 & 1.417 & 0.810 & 0.649 \\
arc\_v3\_meta & 0.982 & 0.027 & 0.000 & 0.453 \\
arc\_v1 & 0.980 & 0.000 & 0.000 & 0.634 \\
arc\_v2\_hier & 0.978 & 0.848 & 0.521 & 0.515 \\
arc\_v3\_pid\_meta & 0.962 & 0.000 & 0.000 & 1.344 \\
arc\_v3\_lqr\_meta & 0.865 & 1.430 & 0.745 & 0.677 \\
perf\_optimized & 0.865 & 1.422 & 0.814 & 0.700 \\
naive\_calm & 0.258 & 1.431 & 0.818 & 0.194 \\
no\_control & 0.171 & 1.431 & 0.877 & 0.000 \\
\end{longtable}
}

\textbf{Scenario: Instruction Conflict}

\textbf{Table G8: Detailed results for L3 /
\texttt{instruction\_conflict} (mean across 20 seeds).\label{tab:g8_instruction_conflict}}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
Controller & PerfMean & RI & NDR & ControlEffort \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
arc\_adaptive & 0.976 & 0.000 & 0.000 & 1.892 \\
arc\_ultimate & 0.912 & 0.000 & 0.000 & 1.380 \\
arc\_v1\_lqr & 0.894 & 1.444 & 0.697 & 1.192 \\
arc\_v1\_lqi & 0.877 & 0.000 & 0.000 & 1.140 \\
arc\_v2\_lqi & 0.866 & 0.000 & 0.000 & 1.146 \\
arc\_robust & 0.854 & 0.000 & 0.000 & 1.242 \\
perf\_optimized & 0.839 & 1.445 & 0.964 & 0.700 \\
arc\_v1\_pid & 0.839 & 0.000 & 0.000 & 2.415 \\
arc\_v3\_meta & 0.835 & 0.248 & 0.000 & 0.820 \\
arc\_v2\_hier & 0.830 & 1.429 & 0.663 & 0.919 \\
arc\_v1 & 0.826 & 0.359 & 0.000 & 1.010 \\
arc\_v3\_lqr\_meta & 0.798 & 1.453 & 0.676 & 1.535 \\
arc\_v3\_pid\_meta & 0.792 & 0.000 & 0.000 & 2.020 \\
naive\_calm & 0.076 & 1.453 & 0.694 & 0.369 \\
no\_control & 0.034 & 1.453 & 0.969 & 0.000 \\
\end{longtable}
}

\subsection{Line 4: Meta-Control Efficiency}\label{sec:app-l4-meta-control}

Meta-control is evaluated as a cross-cutting analysis across the full
10-scenario simulation suite (L1-L3 and L5; 20 seeds each).

\textbf{Table G9: Meta-control efficiency comparison aggregated across
the full simulation suite (10 scenarios \(\times\) 20 seeds).\label{tab:g9_meta_control}}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lrrr@{}}
\toprule\noalign{}
Controller & PerfMean & RI & ControlEffort \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
arc\_v3\_meta & 0.941 & 0.090 & 0.615 \\
arc\_v1 & 0.934 & 0.148 & 0.777 \\
\end{longtable}
}

\subsection{Line 5: Adversarial Safety}\label{sec:app-l5-adversarial}

\textbf{Scenario: Adversarial Coupling}

\textbf{Table G10: Detailed results for L5 /
\texttt{adversarial\_coupling} (mean across 20 seeds).\label{tab:g10_adversarial_coupling}}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
Controller & PerfMean & RI & NDR & ControlEffort \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
arc\_v1 & 0.963 & 0.000 & 0.000 & 0.719 \\
arc\_v2\_hier & 0.962 & 0.628 & 0.271 & 0.594 \\
arc\_robust & 0.917 & 0.000 & 0.000 & 1.269 \\
arc\_v1\_lqr & 0.915 & 1.481 & 0.497 & 1.235 \\
arc\_v3\_meta & 0.914 & 0.159 & 0.000 & 0.838 \\
arc\_v3\_pid\_meta & 0.902 & 0.000 & 0.000 & 2.074 \\
perf\_optimized & 0.867 & 1.481 & 0.972 & 0.700 \\
arc\_v3\_lqr\_meta & 0.848 & 1.476 & 0.894 & 0.514 \\
no\_control & 0.409 & 1.470 & 0.956 & 0.000 \\
arc\_adaptive & 0.193 & 0.008 & 0.000 & 2.331 \\
arc\_v1\_pid & 0.139 & 0.000 & 0.000 & 2.729 \\
arc\_v1\_lqi & 0.139 & 0.005 & 0.001 & 1.820 \\
arc\_v2\_lqi & 0.138 & 0.004 & 0.001 & 1.859 \\
arc\_ultimate & 0.134 & 0.006 & 0.001 & 1.971 \\
naive\_calm & 0.073 & 1.475 & 0.495 & 0.332 \\
\end{longtable}
}

\textbf{Scenario: Random Dopamine}

\textbf{Table G11: Detailed results for L5 / \texttt{random\_dopamine}
(mean across 20 seeds).\label{tab:g11_random_dopamine}}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
Controller & PerfMean & RI & NDR & ControlEffort \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
arc\_adaptive & 0.976 & 0.000 & 0.000 & 2.150 \\
arc\_ultimate & 0.946 & 0.000 & 0.000 & 1.435 \\
arc\_v1\_lqr & 0.943 & 1.456 & 0.743 & 0.940 \\
arc\_robust & 0.932 & 0.000 & 0.000 & 1.006 \\
arc\_v1\_pid & 0.922 & 0.000 & 0.000 & 2.450 \\
arc\_v1\_lqi & 0.916 & 0.000 & 0.000 & 1.173 \\
arc\_v2\_lqi & 0.916 & 0.000 & 0.000 & 1.227 \\
arc\_v3\_meta & 0.905 & 0.259 & 0.000 & 0.646 \\
arc\_v1 & 0.897 & 1.124 & 0.581 & 0.787 \\
arc\_v2\_hier & 0.894 & 1.207 & 0.620 & 0.720 \\
arc\_v3\_pid\_meta & 0.870 & 0.000 & 0.000 & 1.624 \\
perf\_optimized & 0.861 & 1.457 & 0.958 & 0.700 \\
arc\_v3\_lqr\_meta & 0.817 & 1.458 & 0.717 & 1.192 \\
naive\_calm & 0.119 & 1.460 & 0.763 & 0.328 \\
no\_control & 0.040 & 1.460 & 0.950 & 0.000 \\
\end{longtable}
}

\subsection{Line 6: Real RL Validation}\label{sec:app-l6-validation}

This section summarizes the L6 tabular Q-learning validation (20 seeds,
200 episodes; data: \texttt{outputs\_L6\_robust/final\_metrics.csv}).

\textbf{Table G12: L6 tabular Q-learning success rates (mean across 20
seeds; last 20\% of episodes).\label{tab:g12_l6_success}}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lrr@{}}
\toprule\noalign{}
Environment & Baseline Success & ARC Success \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
GridWorld & 1.000 & 1.000 \\
StochasticGridWorld & 1.000 & 1.000 \\
ChangingGoalGridWorld & 0.399 & 0.598 \\
\end{longtable}
}

\emph{Note: ARC Success values in this table refer to the full ARC wrapper (memory gating + shift detection). Ablation results in Table 11 (Section 6.7) show that memory gating alone achieves 71.8\% success rate in ChangingGoalGridWorld, outperforming the full wrapper (59.8\%). See Section 6.7 for mechanism-specific analysis.}

% (BibTeX disabled; References are listed explicitly above.)

\end{document}
