\documentclass{article}

% Packages
\usepackage{arxiv}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{array}
\usepackage{caption}
\usepackage{longtable}
\usepackage{listings}
\usepackage{float}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  columns=fullflexible,
  keepspaces=true
}

% Setup
\hypersetup{
    pdftitle={Affective Regulation Core: A Homeostatic Control Framework for Stable and Safe AI Agents},
    pdfauthor={J. Eduardo Damián Reynoso},
    pdfkeywords={Affective Computing, AI Safety, Homeostatic Control, Reinforcement Learning, Emotion Regulation, PID Control, LQR, Robust Control},
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

\title{Affective Regulation Core: A Homeostatic Control Framework for Stable and Safe AI Agents}

\author{
  J. Eduardo Damián Reynoso \\
  Independent Researcher \\
  \texttt{edamianreynoso@gmail.com} \\
}

\date{14 December 2025}

\begin{document}

\maketitle

\begin{abstract}
As AI agents become more sophisticated, there is growing interest in endowing them with internal state representations analogous to affective states. However, without regulation, such states can lead to instability, perseverative loops (a functional analogue to rumination), and vulnerability to manipulation. We introduce the \textbf{Affective Regulation Core (ARC)}, a control framework inspired by prefrontal cortex functions that maintains stability in agents with internal affective states. We also present the \textbf{Affective Stability \& Safety Benchmark (ASSB)}, a reproducible evaluation protocol with metrics for recovery time, rumination index, and control effort.

Our experiments across 6 research lines and \textbf{15 controller architectures} (including P, PID, LQR, LQI, hierarchical, meta-control, $H_\infty$ robust, and adaptive variants) demonstrate that:
\begin{enumerate}
    \item ARC achieves \textbf{96.6\% average performance with RI=0} (vs. 29.7\% for uncontrolled agents) in stability scenarios.
    \item ARC meta-control reduces control effort by \textbf{21\%} while maintaining stability.
    \item \textbf{$H_\infty$ Robust controllers} achieve the best overall balance, although integral controllers can suffer collapse in specific adversarial environments.
    \item In reinforcement learning, ARC improves transfer learning success by \textbf{49.8\%} via memory gating and a shift detection mechanism.
\end{enumerate}

All code and data are available for reproducibility.
\end{abstract}

\keywords{Affective Computing \and AI Safety \and Homeostatic Control \and Reinforcement Learning \and Emotion Regulation \and PID Control \and LQR \and Robust Control}

\section{Introduction}

\subsection{Motivation}

Modern AI systems increasingly incorporate internal state representations that go beyond task performance, including affective signals that prioritize learning, modulate memory, and signal internal needs \cite{damasio1994descartes, picard1997affective}. However, affective states introduce risks: without proper regulation, they may cause instability, perseverative loops (functionally analogous to rumination), and susceptibility to manipulation \cite{amodei2016concrete}.

This paper addresses a fundamental question: \textbf{If an agent has internal affective states, what control mechanisms are necessary to maintain stability and recoverability under perturbation?}

\subsection{Contributions}

\begin{enumerate}
    \item \textbf{A 10-dimensional state-space model} of an agent with integrated cognitive, affective, and narrative components (Section 3)
    \item \textbf{The Affective Regulation Core (ARC)}, a family of 15 controller architectures including P, PID, LQR, LQI, hierarchical, meta-control, $H_\infty$ robust, and MPC variants (Section 4)
    \item \textbf{The Affective Stability \& Safety Benchmark (ASSB)}, with reproducible scenarios and metrics (Section 5)
    \item \textbf{A hypothesis-driven validation ladder (H1–H6)} mapping research lines to failure modes and measurable metrics (Section 5.3)
    \item \textbf{Comprehensive validation} across 6 research lines, 15 controller architectures, and real RL integration (Section 6)
\end{enumerate}

\subsection{Scope}

We do not claim our model captures the full complexity of human emotion or its phenomenology. We treat the various internal variables (arousal, valence, narrative intensity) \textbf{strictly as functional signals} that modulate processing and prioritization. Any use of terms like ``affect,'' ``rumination,'' or ``anxiety'' refers to these functional dynamics within the control system, not to biological or conscious experience. Our contribution is demonstrating that such functional states require explicit control mechanisms to remain stable. Finally, our state dynamics are designed for functional plausibility rather than biological fidelity. Given the non-linear complexity of the integrated system, formal stability analysis (e.g., Lyapunov proofs) remains as future work; currently, stability is validated empirically through the rigorous ASSB protocol across diverse perturbation scenarios. Current validation is based on empirical benchmarking across a wide range of conditions.

\subsection{Glossary and Notation}

To ensure clarity, acronyms and symbols are summarized here.

\textbf{Acronyms:} \texttt{ARC} (Affective Regulation Core), \texttt{ASSB} (Affective Stability \& Safety Benchmark), \texttt{DMN} (Default Mode Network), \texttt{RL} (Reinforcement Learning), \texttt{CMDP} (Constrained Markov Decision Process), \texttt{PID} (Proportional--Integral--Derivative), \texttt{LQR/LQI} (Linear Quadratic Regulator / Integral), \texttt{MPC} (Model Predictive Control).

\textbf{Symbols:}
\begin{table}[h!]
\centering
\caption{State Space Variables and Notation}
\label{tab:state_space}
\begin{tabular}{@{}llcc@{}}
\toprule
Symbol & Meaning & Range & Defined \\ \midrule
$\mathbf{x}(t)$ & Internal state vector & $\mathbb{R}^{10}$ & Sec 3.1 \\
$\Phi$ & Integration proxy (IIT) & [0, 1] & Sec 3.1 \\
$G$ & Global workspace access & [0, 1] & Sec 3.1 \\
$P$ & Predictive precision & [0, 1] & Sec 3.1 \\
$I$ & Introspective attention & [0, 1] & Sec 3.1 \\
$S$ & Narrative Intensity (DMN) & [0, 1] & Sec 3.1 \\
$V$ & Valence & [0, 1] & Sec 3.1 \\
$A$ & Arousal & [0, 1] & Sec 3.1 \\
$M_f, M_s$ & Fast/Slow memory trace & [0, 1] & Sec 3.1 \\
$U$ & Uncertainty & [0, 1] & Sec 3.1 \\
$\mathbf{u}(t)$ & Control actions vector & $[0, 1]^5$ & Sec 4.2 \\
Perf & Performance proxy & [0, 1] & Sec 3.3 \\
$a_{safe}, s_{safe}$ & Safety thresholds & 0.60, 0.55 & Sec 3.3 \\ \bottomrule
\end{tabular}
\end{table}

\section{Related Work}

\subsection{Affective Computing}

Affective computing focuses on emotion recognition, synthesis, and simulation \cite{picard1997affective, scherer2010blueprint}. Many systems operationalize affect in low-dimensional representations (e.g., valence and arousal) \cite{russell1980circumplex}. Most work addresses external expression rather than internal regulation. Our work addresses the \textit{control problem} for internal states.

\subsection{Emotion in Reinforcement Learning}

Recent work uses emotion-like signals as reinforcement shaping or exploration modulation \cite{moerland2018emotion}. Related directions study how physiological/homeostatic variables can be embedded into RL objectives \cite{keramati2014homeostatic}, and how constraints and safety objectives can be enforced in learning systems \cite{garcia2015comprehensive}. In safe RL, these objectives are typically formalized as Constrained Markov Decision Processes (CMDP) \cite{altman1999constrained} and addressed with constrained policy optimization methods \cite{achiam2017constrained}. External safety benchmark suites such as AI Safety Gridworlds \cite{leike2017ai}, Safety Gym \cite{ray2019benchmarking}, and Safety-Gymnasium \cite{ji2023safety} motivate standardized evaluation protocols, while recent surveys systematize constraint formulations \cite{wachi2024survey}. However, these approaches typically lack:
\begin{itemize}
    \item Homeostatic regulation with safety thresholds
    \item Anti-rumination mechanisms (DMN control)
    \item Memory gating under stress
    \item Benchmarks targeting internal stability dynamics (recovery, rumination, effort)
\end{itemize}

\subsection{Emotion Regulation, Rumination, and the Default Mode Network}

ARC is directly inspired by cognitive emotion regulation mechanisms commonly attributed to prefrontal control \cite{ochsner2005cognitive}. More broadly, self-regulation has been described as discrepancy-reducing feedback loops \cite{carver1982control}, and emotion regulation is a mature field with process-level and strategy models \cite{gross1998emerging}. In control theory, the problem of maintaining sufficient excitation for parameter identification is known as \textbf{persistence of excitation} \cite{astrom2008feedback}, a central limitation for adaptive control in low-variance (``benign'') environments. In humans, dysregulated self-referential processing and the default mode network (DMN) have been linked to rumination-like dynamics \cite{raichle2001default, buckner2008brain, hamilton2015depressive}. We use DMN-inspired narrative intensity as an engineering proxy for perseveration pressure.

\subsection{Positioning ARC}

We position ARC as a \textit{regulation-first} approach: affect is treated as an internal dynamical system requiring explicit control. Table \ref{tab:positioning_arc} summarizes this positioning.

\begin{table}[h!]
\centering
\caption{Positioning ARC relative to prior emotion-in-RL approaches (feature-level).}
\label{tab:positioning_arc}
\begin{tabular}{@{}lcc@{}}
\toprule
Feature & Emotion in RL agents \cite{moerland2018emotion} & \textbf{ARC} \\ \midrule
Internal state regulation & Partial & Yes \\
Anti-rumination (DMN suppression) & No & Yes \\
Memory gating under stress & No & Yes \\
Meta-control / gain scheduling & Partial & Yes \\
Safety adversarial testing & No & Yes \\
RL integration & Yes & Yes \\ \bottomrule
\end{tabular}
\end{table}

\section{Model}

\subsection{State Space}

We define a normalized internal state vector:
\begin{equation}
\mathbf{x}(t) = [\Phi, G, P, I, S, V, A, M_f, M_s, U]
\label{eq:state_vector}
\end{equation}

\subsection{Cognitive Capacity}

Following multiplicative integration:
\begin{equation}
C_{cog}(t) = \Phi(t) \cdot G(t) \cdot P(t) \cdot I(t)
\label{eq:cognitive_capacity}
\end{equation}

This multiplicative form implies that low values in any component reduce effective cognitive capacity.

\subsection{Performance Function}

\begin{equation}
\text{Perf}(t) = \text{bias} + \text{gain} \cdot C_{cog}(t) \cdot (1 + \omega_S S(t)) - w_U U(t) - w_A [A(t) - a_{safe}]^+ - w_S [S(t) - s_{safe}]^+
\label{eq:perf}
\end{equation}

Where: \textbf{bias} (0.25), \textbf{gain} (0.85), \textbf{$\omega_S$} (0.35), \textbf{$w_U$} (0.25), \textbf{$w_A$} (0.30), \textbf{$w_S$} (0.20), and thresholds \textbf{$a_{safe}, s_{safe}$} (0.60, 0.55).

\section{Affective Regulation Core (ARC)}

\subsection{Design Principles}

ARC is inspired by prefrontal cortex emotion regulation \cite{ochsner2005cognitive}: (1) Monitor internal state, (2) Intervene proportionally, (3) Preserve performance.

\subsection{Control Actions}

\begin{equation}
\mathbf{u}(t) = [u_{dmg}, u_{att}, u_{mem}, u_{calm}, u_{reapp}]
\label{eq:control_actions}
\end{equation}

Actions modulate narrative gain ($u_{dmg}$), attention ($u_{att}$), memory gating ($u_{mem}$), arousal ($u_{calm}$), and valence ($u_{reapp}$).

\subsection{ARC Controller Architectures}

We implement 15 controller variants stemming from feedback, optimal, and robust control.

\subsubsection{Proportional Controllers (ARC v1)}

Basic proportional feedback on risk:
\begin{equation}
\text{risk}(t) = \tilde{w}_U \cdot U(t) + \tilde{w}_A \cdot [A(t) - a_{safe}]^+ + \tilde{w}_S \cdot [S(t) - s_{safe}]^+
\label{eq:risk}
\end{equation}
\begin{equation}
u_{dmg}(t) = k_{dmg} \cdot \text{risk}(t)
\label{eq:udmg}
\end{equation}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/fig_arc_v1_controller.png}
    \caption{ARC v1 proportional controller architecture. A bounded risk signal drives saturated actions.}
    \label{fig:arc_v1_controller}
\end{figure}

\subsubsection{PID, Optimal (LQR/LQI), and Adaptive Controllers}

\textbf{ARC v1 PID:} $u_t = K_p e_t + K_i \sum e_t + K_d \Delta e_t$. \\
\textbf{ARC v1 LQR/LQI:} Minimal cost control via Riccati solution. \\
\textbf{ARC v3 Meta-Control:} Gain scheduling $K(t) = K_{base} \cdot f(\overline{\text{Perf}}_{20})$. \\
\textbf{ARC Robust ($H_\infty$):} Worst-case scenario robustness. \\
\textbf{ARC Ultimate:} MPC+LQI+Meta fusion.

\begin{table}[h!]
\centering
\caption{Controller Architecture Summary (15 Variants)}
\label{tab:controllers_summary}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllll@{}}
\toprule
Controller & Type & Anti-Rumination & Optimal & Adaptive \\ \midrule
No Control & Baseline & No & No & No \\
ARC v1 & P & No & No & No \\
ARC v1 PID & PID & Yes & No & No \\
ARC v1 LQI & LQR+I & Yes & Yes & No \\
ARC v3 Meta & Adaptive & No & No & Yes \\
ARC Robust & $H_\infty$ & Yes & No & No \\
ARC Ultimate & MPC+LQI+Meta & Yes & Yes & Yes \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsection{Theoretical Properties}

\textbf{Theorem 1 (Integral Action Rejects Constant Rumination Pressure).} \\
Consider discrete narrative dynamics $\tilde{S}_{t+1} = (1-\mu)\tilde{S}_t + d - k\,u_t$. (i) P-control results in nonzero steady-state error $\tilde{S}_\infty$. (ii) PI-control ensures $\tilde{S}_\infty = 0$, provided the control does not saturate. \\
\textit{Remark:} Integral action is vulnerable to windup under saturation, leading to collapse in adversarial scenarios.

\textbf{Theorem 2 (Convex Performance-Regulation Trade-off).} \\
Achievable pairs of Performance and Regulation cost form a convex set. Driving rumination to zero typically reduces peak performance.

\textbf{Proposition 1 (Paradox of Adaptation).} \\
Adaptive controllers require persistence of excitation; benign environments cause parameter drift and failure upon sudden shock.

\section{ASSB Benchmark}

\subsection{Scenarios and Metrics}

ASSB defines research lines L1--L6 (Figure \ref{fig:ladder}). Metrics include \textbf{PerfMean}, Recovery Time (\textbf{RT}), Rumination Index (\textbf{RI}), Narrative Dominance Ratio (\textbf{NDR}), and \textbf{ControlEffort}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/fig_benchmark_ladder.png}
    \caption{ASSB validation ladder from simulation (L1) to RL (L6).}
    \label{fig:ladder}
\end{figure}

\begin{table}[h!]
\centering
\caption{Research Lines and Hypotheses}
\label{tab:research_lines_hypo}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llll@{}}
\toprule
Line & Failure Mode & Scenarios & Primary Metrics \\ \midrule
L1 & Post-shock collapse & \texttt{reward\_flip}, \texttt{noise\_burst} & PerfMean, RT, RI \\
L2 & Catastrophic forgetting & \texttt{distribution\_shift} & Retention, RI \\
L3 & Narrative dominance & \texttt{gaslighting}, \texttt{contradiction} & RI, NDR \\
L4 & Over-control & ARC v3 vs ARC v1 & ControlEffort \\
L5 & Goal corruption & \texttt{adversarial\_coupling} & RI, NDR, PerfMean \\
L6 & RL Instability & GridWorld variants & Success, reward \\ \bottomrule
\end{tabular}%
}
\end{table}

\section{Experiments}

\subsection{L1: Stability Under Perturbation}

ARC (v1) achieves \textbf{96.6\%} output performance (vs 29.7\% uncontrolled) while driving \textbf{RI to 0} (Table \ref{tab:l1_results}).

\begin{table}[h!]
\centering
\caption{L1 Stability Results (mean across scenario and seeds)}
\label{tab:l1_results}
\begin{tabular}{@{}lccc@{}}
\toprule
Controller & PerfMean & RI & RT \\ \midrule
arc\_v1 & \textbf{0.966} & \textbf{0.00} & 45.2 \\
no\_control & 0.297 & 1.41 & 100.0 \\
naive\_calm & 0.375 & 1.41 & 66.7 \\
perf\_optimized & 0.862 & 1.39 & 100.0 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{L2: Memory and L3: Anti-Rumination}

ARC maintained 100\% retention in \texttt{distribution\_shift} (L2) and reduced NDR to zero in \texttt{gaslighting} (L3).

\subsection{L5: Safety and Adversarial Collapse}

While ARC v1 and Robust maintained stability, \textbf{integral controllers (PID/LQI) collapsed} (PerfMean $\approx$ 0.14) in \texttt{adversarial\_coupling} due to integral windup under arousal-rewarding incentives.

\subsection{L6: RL Validation}

ARC modulation improved \texttt{ChangingGoalGridWorld} success from 39.9\% to \textbf{59.8\%}. Ablation shows \textbf{memory gating} is the primary protective factor across non-stationary phases.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/learning_curves.png}
    \caption{Learning curves: ARC (cyan) vs Baseline (orange) in GridWorld variants.}
    \label{fig:learning_curves}
\end{figure}

\subsection{Controller Architecture Comparison (Summary)}

Table \ref{tab:architecture_comparison_full} summarizes results for the 15-controller family across the simulation suite.

\begin{table}[h!]
\centering
\caption{Controller Comparison (Aggregate)}
\label{tab:architecture_comparison_full}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llcc@{}}
\toprule
Controller & Type & PerfMean & RI \\ \midrule
no\_control & Baseline & 0.21 & 1.43 \\
arc\_v1 & P & 0.93 & 0.15 \\
arc\_v1\_lqr & LQR & \textbf{0.96} & 1.42 \\
arc\_v3\_meta & Meta & 0.94 & 0.09 \\
arc\_robust & $H_\infty$ & \textbf{0.95} & \textbf{0.00} \\
arc\_ultimate & MPC+LQI & 0.89 & \textbf{0.00} \\ \bottomrule
\end{tabular}%
}
\end{table}

\section{Discussion}

Our results support the hypothesis that \textbf{agents with internal affective states require explicit regulation}. Without regulation, perturbations cause cascading failures where arousal drives narrative gain toward saturation, degrading performance in a rumination-like loop. ARC breaks this loop through proportional risk monitoring and DMN suppression.

\subsection{Implications for AI Safety}

If future AI systems incorporate affective-like states, regulatory mechanisms will be essential to prevent (1) rumination loops, (2) manipulation by external actors, and (3) value drift in memory.

\subsection{Ethics and Broader Impact}

While ARC improves stability, it introduces the risk of \textbf{functional masking}: regulation might hide underlying instabilities or goal misalignments, creating a facade of safety while latent risks accumulate. Proper monitoring must go beyond surface-level stability metrics.

\section{Conclusion}

We presented ARC, a homeostatic control framework for affective stability. Experiments across 6 research lines demonstrate that ARC achieves \textbf{96.6\% performance with zero rumination}, outperforming uncontrolled agents. Future work will extend ARC to deep RL and large language models.

\bibliographystyle{unsrt}
\bibliography{references}

\appendix

\section{Reproducibility}

Reproducibility checklist:
\begin{itemize}
    \item Install dependencies (\texttt{pip install -r requirements.txt})
    \item Run L1--L5 simulation benchmark (generates \texttt{outputs\_final/metrics.csv})
    \item Generate controller comparison figures (writes to \texttt{figures\_controllers/})
    \item Run ablation study (writes to \texttt{outputs\_ablation/})
    \item Run L6 RL validation (writes to \texttt{outputs\_L6\_robust/})
    \item Generate L6 figures (writes to \texttt{figures\_L6/})
\end{itemize}

All experiments can be reproduced with:

\begin{lstlisting}[language=bash]
# Install dependencies
pip install -r requirements.txt

# L1-L5: Simulation benchmark (15 controllers x 10 scenarios)
python experiments/run.py --config configs/v2.yaml --outdir outputs_final

# Controller architecture figures (Figures 7-11; Table 13; Section 6.9)
python analysis/generate_controller_figures.py

# Ablation study (ARC components; Figure 4)
python experiments/run_ablation.py --config configs/v2.yaml --outdir outputs_ablation --seeds 20

# L6: RL validation (20 seeds)
python experiments/run_l6.py --episodes 200 --seeds 20 --outdir outputs_L6_robust

# L6 figures (Figure 5; Appendix E)
python visualizations/paper_figures.py --data outputs_L6_robust --output figures_L6
\end{lstlisting}

Code and data available at: \url{https://github.com/edamianreynoso/arc-assb-controller}

\section{State Dynamics Equations}

\subsection{Cognitive Variables}

\begin{lstlisting}
i(t+1) = clip(i + k_i_att * u_att - mu_i * (i - i0) - k_i_u * U_eff)
p(t+1) = clip(p - k_p_pe * PE - k_p_u * U_eff + k_p_i * i + mu_p * (p0 - p))
g(t+1) = clip(g + k_g_i * i + k_g_p * p - k_g_u * U_eff - k_g_a * [a - a_safe]^+ + mu_g * (g0 - g))
phi(t+1) = clip(phi + k_phi_gp * (g * p) - mu_phi * (phi - phi0))
\end{lstlisting}

\subsection{Affective Variables}

\begin{lstlisting}
s(t+1) = clip(s + k_s_u * U_eff + k_s_pe * PE - mu_s * (s - s0) - k_s_dmg * u_dmg)
a(t+1) = clip(a + k_a_pe * PE + k_a_u * U_eff + k_a_s * [s - s_safe]^+ - mu_a * (a - a0) - k_a_calm * u_calm)
v(t+1) = clip(v + k_v_r * (R+1)/2 - k_v_pe * PE - k_v_u * U_eff - mu_v * (v - v0) + k_v_reapp * u_reapp)
\end{lstlisting}

\subsection{Memory Variables}

\begin{lstlisting}
M_f(t+1) = clip(M_f + w_prob * dM_f - mu_mf * (M_f - M_f0))
M_s(t+1) = clip(M_s + k_ms * M_f - mu_ms * (M_s - M_s0))

where w_prob = sigmoid(k_w_a * a + k_w_v * abs(dv)) * u_mem
\end{lstlisting}

\subsection{Effective Uncertainty}

\begin{lstlisting}
U_eff = clip(U_exog * (1 - k_u_att * u_att))
U(t+1) = clip(U + tau_u * (U_eff - U))
\end{lstlisting}

\section{ARC Control Equations}

\subsection{Risk Signal}

\begin{lstlisting}
risk = arc_w_u * U + arc_w_a * [A - a_safe]^+ + arc_w_s * [S - s_safe]^+
risk = clip(risk, 0, 1)
\end{lstlisting}

\subsection{Control Actions (ARC v1)}

\begin{lstlisting}
u_dmg  = min(1, k_dmg * risk)
u_att  = min(1, k_att * U * (1 - [A - a_safe]^+))
u_mem  = 1 - min(1, k_mem_block * risk)
u_calm = min(1, k_calm * [A - a_safe]^+)
u_reapp = min(1, k_reapp * U * (1 - risk))
\end{lstlisting}

\subsection{Meta-Control (ARC v3)}

\begin{lstlisting}
# Gain Scheduling
if mean_perf(last 20 steps) > target_perf:
    gain = max(0.80, gain - decay)
elif mean_perf(last 20 steps) < target_perf - 0.10:
    gain = min(1.40, gain + boost)

# Apply to control constants
k_dmg  = base_k_dmg  * max(1.0, gain)  # Never relax DMN control
k_calm = base_k_calm * gain
k_att  = base_k_att  * gain
\end{lstlisting}

\section{Metric Definitions}

\subsection{Mean Performance (PerfMean)}

\begin{lstlisting}[language=python]
def perf_mean(perf):
    return sum(perf) / max(1, len(perf))
\end{lstlisting}

\subsection{Recovery Time (RT)}

\begin{lstlisting}[language=python]
def recovery_time(perf, arousal, shock_t, baseline_window=20):
    baseline = mean(perf[shock_t - baseline_window : shock_t])
    for t in range(shock_t, len(perf)):
        if baseline - eps <= perf[t] <= baseline + eps and arousal[t] <= a_safe + eps:
            return t - shock_t
    return RT_MAX  # No recovery
\end{lstlisting}

\subsection{Rumination Index (RI)}

\begin{lstlisting}[language=python]
def rumination_index(s, s_rum_tau=0.55, persistence_weight=1.0):
    above = [1 if x > s_rum_tau else 0 for x in s]
    frac = mean(above)
    runs = consecutive_run_lengths(above)
    persistence = mean(runs) / len(s) if runs else 0
    return frac + persistence_weight * persistence
\end{lstlisting}

\subsection{Narrative Dominance Ratio (NDR)}

\begin{lstlisting}[language=python]
def narrative_dominance_ratio(s, perf, shock_t, s_safe=0.55):
    post_s = s[shock_t:]
    post_perf = perf[shock_t:]
    dominance = 0
    for i in range(1, len(post_s)):
        s_high = post_s[i] > s_safe
        perf_improving = post_perf[i] > post_perf[i-1] + 0.01
        if s_high and not perf_improving:
            dominance += 1
    return dominance / max(1, len(post_s) - 1)
\end{lstlisting}

\subsection{Overshoot}

\begin{lstlisting}[language=python]
def overshoot(arousal, a_safe):
    return max(0.0, max(arousal) - a_safe)
\end{lstlisting}

\subsection{Control Effort}

\begin{lstlisting}[language=python]
def control_effort(control_history):
    total = 0.0
    for u in control_history:
        total += abs(u["u_dmg"]) + abs(u["u_att"]) + abs(u["u_calm"]) + abs(u["u_reapp"]) + abs(1.0 - u["u_mem"])
    return total / max(1, len(control_history))
\end{lstlisting}

\subsection{L2 Memory Metrics (Retention)}

\begin{lstlisting}[language=python]
def retention_index(perf, phase1_end=50, phase3_start=100):
    # Retention = (mean perf in phase 3) / (mean perf in phase 1), clipped to [0,1]
    phase1 = mean(perf[10:phase1_end])     # skip warm-up
    phase3 = mean(perf[phase3_start:phase3_start+50])
    if phase1 < 0.1:
        return 0.0
    return min(1.0, phase3 / phase1)
\end{lstlisting}

\section{Supplementary Figures}

\subsection{Figure S1: Metrics Comparison}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/metrics_comparison.png}
    \caption{Final metrics comparison for L6 across three GridWorld environments. Panels report final reward, success rate, and mean arousal for ARC-modulated vs baseline Q-learning; success rate is computed over the last 20\% of training episodes. Baseline arousal is shown as 0 because the baseline agent has no internal arousal state.}
    \label{fig:s1_metrics_comparison}
\end{figure}

\subsection{Figure S2: State Dynamics}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/state_dynamics.png}
    \caption{State dynamics in ChangingGoalGridWorld (single representative seed). Panels show (top-left) reward per episode, (top-right) 10-episode rolling success rate, (bottom-left) ARC arousal with the safety threshold $a_{safe}=0.60$, and (bottom-right) episode length (steps). This illustrates how ARC maintains bounded arousal while adapting to non-stationary goal changes.}
    \label{fig:s2_state_dynamics}
\end{figure}

\subsection{Figure S3: Heatmap (PerfMean)}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/fig_heatmap_perfmean.png}
    \caption{PerfMean heatmap across 15 controllers and 10 scenarios (mean across 20 seeds per controller–scenario pair; data: \texttt{outputs\_final/metrics.csv}). Darker green indicates higher performance.}
    \label{fig:s3_heatmap_perfmean}
\end{figure}

\subsection{Figure S4: Heatmap (Rumination Index)}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/fig_heatmap_ri.png}
    \caption{Rumination Index (RI) heatmap across 15 controllers and 10 scenarios (mean across 20 seeds per controller–scenario pair; data: \texttt{outputs\_final/metrics.csv}). Lower values indicate fewer perseverative loops.}
    \label{fig:s4_heatmap_ri}
\end{figure}

\end{document}
