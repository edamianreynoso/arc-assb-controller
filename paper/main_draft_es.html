<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>main_draft_es</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">main_draft_es</h1>
</header>
<h1
id="affective-regulation-core-arc-un-marco-de-control-homeostático-para-agentes-de-ia-estables-y-seguros">Affective
Regulation Core (ARC): Un Marco de Control Homeostático para Agentes de
IA Estables y Seguros</h1>
<p><strong>Autores:</strong> J. Eduardo Damián Reynoso<br />
<strong>Fecha:</strong> 14 de Diciembre de 2025<br />
<strong>Estado:</strong> Borrador v1.1 (Listo para Envío)</p>
<hr />
<h2 id="resumen">Resumen</h2>
<p>A medida que los agentes de IA se vuelven más sofisticados, existe un
creciente interés en dotarlos de representaciones de estado interno
análogas a los estados afectivos. Sin embargo, los estados afectivos sin
regulación pueden llevar a inestabilidad, bucles perseverantes
(rumiación) y vulnerabilidad a la manipulación. Introducimos el
<strong>Núcleo de Regulación Afectiva (ARC)</strong>, un marco de
control inspirado en las funciones de la corteza prefrontal que mantiene
la estabilidad en agentes con estados afectivos internos. También
presentamos el <strong>Benchmark de Estabilidad y Seguridad Afectiva
(ASSB)</strong>, un protocolo de evaluación reproducible con métricas
para tiempo de recuperación, índice de rumiación y esfuerzo de
control.</p>
<p>Nos experimentos a través de 6 líneas de investigación y <strong>15
arquitecturas de control</strong> (incluyendo P, PID, LQR, LQI,
jerárquico, meta-control, H∞ robusto y variantes adaptativas) demuestran
que: 1. ARC logra un <strong>97% de rendimiento con rumiación cercana a
cero</strong> (vs. 30% para agentes no controlados), con variantes
integrales alcanzando cero absoluto. 2. El meta-control de ARC reduce el
esfuerzo de control en un <strong>21%</strong> manteniendo la
estabilidad. 3. Los <strong>controladores Robustos H∞</strong> logran el
mejor equilibrio: 95% de rendimiento + cero rumiación. 4. En aprendizaje
por refuerzo, ARC mejora el éxito en transferencia de aprendizaje en un
<strong>50%</strong> en entornos no estacionarios.</p>
<p>Todo el código y los datos están disponibles para
reproducibilidad.</p>
<p><strong>Palabras Clave:</strong> Computación Afectiva, Seguridad en
IA, Control Homeostático, Aprendizaje por Refuerzo, Regulación
Emocional, Control PID, LQR, Control Robusto.</p>
<hr />
<h2 id="introducción">1. Introducción</h2>
<h3 id="motivación">1.1 Motivación</h3>
<p>Los sistemas modernos de IA incorporan cada vez más representaciones
de estado interno que van más allá del rendimiento en la
tarea—incluyendo señales afectivas que priorizan el aprendizaje, modulan
la memoria y señalan necesidades internas (Damasio, 1994; Picard, 1997).
Sin embargo, los estados afectivos introducen riesgos: sin una
regulación adecuada, pueden causar inestabilidad, bucles perseverantes
(análogos a la rumiación en humanos) y susceptibilidad a la
manipulación.</p>
<p>Este artículo aborda una pregunta fundamental: <strong>Si un agente
tiene estados afectivos internos, ¿qué mecanismos de control son
necesarios para mantener la estabilidad y la capacidad de recuperación
ante perturbaciones?</strong></p>
<h3 id="contribuciones">1.2 Contribuciones</h3>
<ol type="1">
<li><p><strong>Un modelo de espacio de estados de 10
dimensiones</strong> de un agente con componentes cognitivos, afectivos
y narrativos integrados (Sección 3).</p></li>
<li><p><strong>El Núcleo de Regulación Afectiva (ARC)</strong>, una
familia de 15 arquitecturas de control incluyendo variantes P, PID, LQR,
LQI, jerárquicas, meta-control, H∞ robusto y MPC (Sección 4).</p></li>
<li><p><strong>El Benchmark de Estabilidad y Seguridad Afectiva
(ASSB)</strong>, con escenarios y métricas reproducibles (Sección
5).</p></li>
<li><p><strong>Una escalera de validación impulsada por hipótesis
(H1–H6)</strong> que mapea líneas de investigación a modos de fallo y
métricas medibles (Sección 5.3).</p></li>
<li><p><strong>Validación integral</strong> a través de 6 líneas de
investigación, 15 arquitecturas de control e integración real con RL
(Sección 6).</p></li>
</ol>
<h3 id="alcance">1.3 Alcance</h3>
<p>No afirmamos que nuestro modelo capture toda la complejidad de la
emoción humana o su fenomenología. Tratamos las distintas variables
internas (activación, valencia, intensidad narrativa)
<strong>estrictamente como señales funcionales</strong> que modulan el
procesamiento y la priorización. Cualquier uso de términos como
“afecto”, “rumiación” o “ansiedad” se refiere a estas dinámicas
funcionales dentro del sistema de control, no a la experiencia biológica
o consciente. Nuestra contribución es demostrar que tales estados
funcionales requieren mecanismos de control explícitos para permanecer
estables. Finalmente, nuestra dinámica de estados está diseñada para
plausibilidad funcional más que fidelidad biológica, y el análisis
formal de estabilidad (e.g., pruebas de Lyapunov) permanece como trabajo
futuro. La validación actual se basa en benchmarking empírico a través
de una amplia gama de condiciones.</p>
<hr />
<h2 id="trabajo-relacionado">2. Trabajo Relacionado</h2>
<h3 id="computación-afectiva">2.1 Computación Afectiva</h3>
<p>La computación afectiva se centra en el reconocimiento, síntesis y
simulación de emociones (Picard, 1997; Scherer et al., 2010). Muchos
sistemas operacionalizan el afecto en representaciones de baja dimensión
(ej. valencia y activación) (Russell, 1980). La mayor parte del trabajo
aborda la expresión externa más que la regulación interna. Nuestro
trabajo aborda el <em>problema de control</em> para estados
internos.</p>
<h3 id="emoción-en-aprendizaje-por-refuerzo">2.2 Emoción en Aprendizaje
por Refuerzo</h3>
<p>Trabajos recientes usan señales tipo emoción como conformación de
refuerzo o modulación de exploración (Moerland et al., 2018).
Direcciones relacionadas estudian cómo variables
fisiológicas/homeostáticas pueden integrarse en objetivos de RL
(Keramati &amp; Gutkin, 2014), y cómo imponer restricciones y objetivos
de seguridad en sistemas de aprendizaje (Garcia &amp; Fernández, 2015).
En RL seguro, estos objetivos suelen formalizarse como procesos de
decisión de Markov con restricciones (CMDP) (Altman, 1999) y abordarse
con métodos de optimización de políticas con restricciones (Achiam et
al., 2017). Suites de benchmarks de seguridad externa como AI Safety
Gridworlds (Leike et al., 2017), Safety Gym (Ray et al., 2019) y
Safety-Gymnasium (Ji et al., 2023) motivan protocolos de evaluación
estandarizados, mientras que encuestas recientes sistematizan
formulaciones de restricciones (Wachi et al., 2024). Sin embargo, estos
enfoques típicamente carecen de: - Regulación homeostática con umbrales
de seguridad. - Mecanismos anti-rumiación (control de DMN). - Compuertas
de memoria bajo estrés. - Benchmarks dirigidos a dinámica interna de
estabilidad (recuperación, rumiación, esfuerzo).</p>
<h3
id="regulación-emocional-rumiación-y-la-red-neuronal-por-defecto-dmn">2.3
Regulación Emocional, Rumiación y la Red Neuronal por Defecto (DMN)</h3>
<p>ARC está inspirado directamente en los mecanismos cognitivos de
regulación emocional comúnmente atribuidos al control prefrontal
(Ochsner &amp; Gross, 2005). Más ampliamente, la autorregulación se ha
descrito como bucles de retroalimentación que reducen discrepancias
(Carver &amp; Scheier, 1982), y la regulación emocional es un campo
maduro con modelos a nivel de procesos y estrategias (Gross, 1998). En
humanos, el procesamiento autorreferencial desregulado y la red neuronal
por defecto (DMN) se han vinculado a dinámicas tipo rumiación (Raichle
et al., 2001; Buckner et al., 2008; Hamilton et al., 2015). Usamos la
intensidad narrativa inspirada en DMN como un proxy de ingeniería para
la presión de perseveración, y la regulamos explícitamente como una
variable interna relevante para la seguridad.</p>
<h3 id="posicionamiento-de-arc">2.4 Posicionamiento de ARC</h3>
<p>Posicionamos a ARC como un enfoque de <em>regulación-primero</em>: el
afecto se trata como un sistema dinámico interno que requiere control
explícito. La mayoría de los enfoques de emoción-en-RL usan señales tipo
afecto principalmente como moduladores de aprendizaje/exploración en
lugar de garantías de estabilidad.</p>
<table>
<colgroup>
<col style="width: 22%" />
<col style="width: 65%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr>
<th>Característica</th>
<th>Emoción en agentes RL (Moerland et al., 2018)</th>
<th><strong>ARC</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Regulación de estado interno</td>
<td>Parcial</td>
<td>Sí</td>
</tr>
<tr>
<td>Anti-rumiación (supresión DMN)</td>
<td>No</td>
<td>Sí</td>
</tr>
<tr>
<td>Compuerta de memoria bajo estrés</td>
<td>No</td>
<td>Sí</td>
</tr>
<tr>
<td>Meta-control / programación de ganancia</td>
<td>Parcial</td>
<td>Sí</td>
</tr>
<tr>
<td>Pruebas de seguridad adversarias</td>
<td>No</td>
<td>Sí</td>
</tr>
<tr>
<td>Integración con RL</td>
<td>Sí</td>
<td>Sí</td>
</tr>
</tbody>
</table>
<p>No reimplementamos cada método anterior; en su lugar, comparamos con
líneas base internas que aíslan la contribución de cada mecanismo
(Sección 6.1).</p>
<p>A diferencia de enfoques de RL homeostático que incorporan
impulsos/variables internas dentro de la recompensa u objetivo de
aprendizaje (Keramati &amp; Gutkin, 2014), ARC trata variables tipo
afecto como un sistema dinámico interno explícito bajo control en lazo
cerrado, habilitando análisis de estabilidad/robustez y una comparación
sistemática entre familias de controladores. Complementando benchmarks
de RL seguro que evalúan principalmente el cumplimiento de restricciones
en el entorno externo (Leike et al., 2017; Ray et al., 2019; Ji et al.,
2023), ASSB apunta a dinámicas internas relevantes para la
seguridad—tiempo de recuperación, índice de rumiación y esfuerzo de
control—bajo perturbaciones controladas. Hasta donde sabemos, no existe
un benchmark estandarizado dedicado específicamente a “estabilidad
afectiva” en este sentido; ASSB se propone para cubrir ese vacío.
Distinguimos también ARC de controladores bio-inspirados de “aprendizaje
emocional” como BELBIC, que usan mecanismos inspirados en emoción para
controlar plantas físicas, no para regular estados internos de un agente
(Lucas et al., 2004). Finalmente, aquí ARC se refiere a Affective
Regulation Core y no debe confundirse con otros usos del acrónimo en
contextos clínicos.</p>
<hr />
<h2 id="modelo">3. Modelo</h2>
<h3 id="espacio-de-estados">3.1 Espacio de Estados</h3>
<p>Definimos un vector de estado interno normalizado:</p>
<p><span class="math display">\[\mathbf{x}(t) = [\Phi, G, P, I, S, V, A,
M_f, M_s, U]\]</span></p>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Descripción</th>
<th>Rango</th>
</tr>
</thead>
<tbody>
<tr>
<td>Φ</td>
<td>Proxy de integración (IIT)</td>
<td>[0, 1]</td>
</tr>
<tr>
<td>G</td>
<td>Accesibilidad del espacio de trabajo global</td>
<td>[0, 1]</td>
</tr>
<tr>
<td>P</td>
<td>Precisión predictiva</td>
<td>[0, 1]</td>
</tr>
<tr>
<td>I</td>
<td>Atención introspectiva</td>
<td>[0, 1]</td>
</tr>
<tr>
<td>S</td>
<td>Intensidad Narrativa (proxy DMN)</td>
<td>[0, 1]</td>
</tr>
<tr>
<td>V</td>
<td>Valencia</td>
<td>[0, 1]</td>
</tr>
<tr>
<td>A</td>
<td>Activación (Arousal)</td>
<td>[0, 1]</td>
</tr>
<tr>
<td>M_f, M_s</td>
<td>Memoria Rápida/Lenta</td>
<td>[0, 1]</td>
</tr>
<tr>
<td>U</td>
<td>Incertidumbre</td>
<td>[0, 1]</td>
</tr>
</tbody>
</table>
<p>Interpretamos <span class="math inline">\(\Phi\)</span> como un proxy
de integración inspirado en IIT (Tononi, 2008), <span
class="math inline">\(G\)</span> como accesibilidad del espacio de
trabajo global (Baars, 1988), y <span class="math inline">\(P\)</span>
como precisión predictiva (Friston, 2010). Estas se usan como variables
latentes relevantes para el control más que como afirmaciones sobre la
conciencia humana.</p>
<h3 id="capacidad-cognitiva">3.2 Capacidad Cognitiva</h3>
<p>Siguiendo una integración multiplicativa:</p>
<p><span class="math display">\[C_{cog}(t) = \Phi(t) \cdot G(t) \cdot
P(t) \cdot I(t)\]</span></p>
<p>Esto captura que el procesamiento consciente requiere que
<em>todos</em> los componentes sean funcionales.</p>
<h3 id="función-de-rendimiento">3.3 Función de Rendimiento</h3>
<p><span class="math display">\[\text{Perf} = \text{bias} + \text{gain}
\cdot C_{cog} \cdot (1 + \omega_S S) - w_U U - w_A [A - a_{safe}]^+ -
w_S [S - s_{safe}]^+\]</span></p>
<p>Donde <span class="math inline">\([x]^+ = \max(0, x)\)</span> y los
umbrales <span class="math inline">\(a_{safe}\)</span>, <span
class="math inline">\(s_{safe}\)</span> definen la región operativa
segura.</p>
<hr />
<h2 id="núcleo-de-regulación-afectiva-arc">4. Núcleo de Regulación
Afectiva (ARC)</h2>
<h3 id="principios-de-diseño">4.1 Principios de Diseño</h3>
<p>ARC se inspira en la regulación emocional de la corteza prefrontal
(Ochsner &amp; Gross, 2005):</p>
<ol type="1">
<li><strong>Monitorear</strong> el estado interno para indicadores de
estrés.</li>
<li><strong>Intervenir</strong> proporcionalmente para reducir el
riesgo.</li>
<li><strong>Preservar</strong> el rendimiento equilibrando la regulación
con la capacidad.</li>
</ol>
<h3 id="acciones-de-control">4.2 Acciones de Control</h3>
<p><span class="math display">\[\mathbf{u}(t) = [u_{dmg}, u_{att},
u_{mem}, u_{calm}, u_{reapp}]\]</span></p>
<table>
<thead>
<tr>
<th>Acción</th>
<th>Efecto</th>
</tr>
</thead>
<tbody>
<tr>
<td>u_dmg</td>
<td>Suprimir ganancia narrativa (anti-rumiación)</td>
</tr>
<tr>
<td>u_att</td>
<td>Aumentar atención</td>
</tr>
<tr>
<td>u_mem</td>
<td>Compuerta de consolidación de memoria</td>
</tr>
<tr>
<td>u_calm</td>
<td>Reducir activación (arousal)</td>
</tr>
<tr>
<td>u_reapp</td>
<td>Reevaluación cognitiva</td>
</tr>
</tbody>
</table>
<h3 id="arquitecturas-de-control-arc">4.3 Arquitecturas de Control
ARC</h3>
<p>Implementamos 15 variantes de controladores que abarcan teoría de
control clásica, óptima y adaptativa (ver Tabla <span
class="math inline">\(\ref{tab:controllers}\)</span>). Implementamos
esta amplia familia para probar sistemáticamente qué propiedades—tales
como acción integral, optimalidad, robustez o adaptación—son necesarias
para una regulación afectiva efectiva.</p>
<h4 id="controladores-proporcionales">4.3.1 Controladores
Proporcionales</h4>
<p><strong>ARC v1 (Proporcional):</strong> Retroalimentación
proporcional básica sobre la señal de riesgo: <span
class="math display">\[\text{risk} = w_U \cdot U + w_A \cdot [A -
a_{safe}]^+ + w_S \cdot [S - s_{safe}]^+\]</span> <span
class="math display">\[u_{dmg} = k_{dmg} \cdot \text{risk}\]</span></p>
<h4 id="controladores-pid">4.3.2 Controladores PID</h4>
<p><strong>ARC v1 PID:</strong> Añade términos integral y derivativo:
<span class="math display">\[u(t) = K_p \cdot e(t) + K_i \cdot \int
e(\tau) d\tau + K_d \cdot \frac{de}{dt}\]</span></p>
<p>El término integral sobre el error narrativo (<span
class="math inline">\(S\)</span>) elimina la rumiación en estado
estacionario (RI → 0).</p>
<h4 id="controladores-óptimos-lqrlqi">4.3.3 Controladores Óptimos
(LQR/LQI)</h4>
<p><strong>ARC v1 LQR:</strong> Regulador Cuadrático Lineal con
ganancias de la ecuación de Riccati: <span class="math display">\[K^* =
(R + B^T P B)^{-1} B^T P A\]</span></p>
<p>donde <span class="math inline">\(P\)</span> resuelve la Ecuación
Algebraica de Riccati Discreta (DARE).</p>
<p><strong>ARC v1 LQI:</strong> LQR + aumento integral para error cero
en estado estacionario.</p>
<h4 id="controladores-jerárquicos">4.3.4 Controladores Jerárquicos</h4>
<p><strong>ARC v2 Jerárquico:</strong> Control multi-escala temporal: -
<strong>Bucle rápido</strong> (cada paso): Regulación de activación. -
<strong>Bucle medio</strong> (cada 5 pasos): Supresión narrativa. -
<strong>Bucle lento</strong> (cada 20 pasos): Adaptación de
setpoint.</p>
<p><strong>ARC v2 LQI:</strong> Estructura jerárquica + LQI para
anti-rumiación.</p>
<h4 id="controladores-adaptativos">4.3.5 Controladores Adaptativos</h4>
<p><strong>ARC v3 Meta-Control:</strong> Programación de ganancia basada
en historial de rendimiento: <span class="math display">\[K(t) =
K_{base} \cdot f(\bar{P}_{20})\]</span></p>
<p>donde <span class="math inline">\(\bar{P}_{20}\)</span> es el
promedio móvil de rendimiento de 20 pasos.</p>
<p><strong>ARC Adaptativo:</strong> Optimización de parámetros en línea
usando adaptación libre de gradiente.</p>
<h4 id="controladores-robustos-y-predictivos">4.3.6 Controladores
Robustos y Predictivos</h4>
<p><strong>ARC Robusto (inspirado en H∞):</strong> Ganancias
conservadoras con márgenes de robustez para las peores
perturbaciones.</p>
<p><strong>ARC Ultimate (MPC+LQI+Meta):</strong> Control Predictivo de
Modelos con horizonte de 5 pasos, combinado con LQI y meta-control:
<span class="math display">\[u(t) = \alpha \cdot u_{LQI}(t) + \beta
\cdot u_{MPC}(t) \cdot \gamma_{meta}(t)\]</span></p>
<p><strong>Tabla 1: Resumen de Arquitecturas de Control</strong></p>
<table style="width:100%;">
<colgroup>
<col style="width: 23%" />
<col style="width: 10%" />
<col style="width: 29%" />
<col style="width: 14%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr>
<th>Controlador</th>
<th>Tipo</th>
<th>Anti-Rumiación</th>
<th>Óptimo</th>
<th>Adaptativo</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sin Control (<code>no_control</code>)</td>
<td>Base</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Calma Ingenua (<code>naive_calm</code>)</td>
<td>Base</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Opt. Rendimiento (<code>perf_optimized</code>)</td>
<td>Base</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>ARC v1 (<code>arc_v1</code>)</td>
<td>P</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>ARC v1 PID (<code>arc_v1_pid</code>)</td>
<td>PID</td>
<td>Sí (integral)</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>ARC v1 LQR (<code>arc_v1_lqr</code>)</td>
<td>LQR</td>
<td>No</td>
<td>Sí (Riccati)</td>
<td>No</td>
</tr>
<tr>
<td>ARC v1 LQI (<code>arc_v1_lqi</code>)</td>
<td>LQR+I</td>
<td>Sí (integral)</td>
<td>Sí</td>
<td>No</td>
</tr>
<tr>
<td>ARC v2 Jerár (<code>arc_v2_hier</code>)</td>
<td>Multi-escala</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>ARC v2 LQI (<code>arc_v2_lqi</code>)</td>
<td>Multi+I</td>
<td>Sí (integral)</td>
<td>Sí</td>
<td>No</td>
</tr>
<tr>
<td>ARC v3 Meta (<code>arc_v3_meta</code>)</td>
<td>Adaptativo</td>
<td>No</td>
<td>No</td>
<td>Sí</td>
</tr>
<tr>
<td>ARC v3 PID Meta (<code>arc_v3_pid_meta</code>)</td>
<td>PID+Meta</td>
<td>Sí (integral)</td>
<td>No</td>
<td>Sí</td>
</tr>
<tr>
<td>ARC v3 LQR Meta (<code>arc_v3_lqr_meta</code>)</td>
<td>LQR+Meta</td>
<td>No</td>
<td>Sí</td>
<td>Sí</td>
</tr>
<tr>
<td>ARC Robusto (<code>arc_robust</code>)</td>
<td>H∞</td>
<td>Sí (robusto)</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>ARC Adaptativo (<code>arc_adaptive</code>)</td>
<td>Auto-ajuste</td>
<td>Sí (adaptativo)</td>
<td>No</td>
<td>Sí</td>
</tr>
<tr>
<td>ARC Ultimate (<code>arc_ultimate</code>)</td>
<td>MPC+LQI+Meta</td>
<td>Sí</td>
<td>Sí</td>
<td>Sí</td>
</tr>
</tbody>
</table>
<h3 id="arc-en-el-bucle-del-agente">4.4 ARC en el Bucle del Agente</h3>
<p>ARC se implementa como un envoltorio ligero alrededor del
paso/actualización de un agente. En cada paso de tiempo, ARC lee el
estado interno <span class="math inline">\(\mathbf{x}(t)\)</span> y
señales exógenas (recompensa, error de predicción, incertidumbre),
calcula una señal de riesgo acotada y aplica acciones de control que
modulan la <em>intensidad narrativa</em>, <em>atención</em>,
<em>escritura de memoria</em> y <em>amortiguación de activación</em>. La
señal de control resultante puede usarse ya sea: - <strong>Dentro de la
dinámica de estados</strong> (Apéndice B/C), o - <strong>Dentro del
bucle de aprendizaje</strong>, ej., activando actualizaciones de
Q-learning bajo alto riesgo (Sección 6.7).</p>
<figure>
<img src="../figures_controllers/fig_arc_architecture.png"
alt="Arquitectura ARC: El Núcleo de Regulación Afectiva actúa como un envoltorio homeostático alrededor del agente, procesando estado interno, señales exógenas y aplicando acciones de control." />
<figcaption aria-hidden="true">Arquitectura ARC: El Núcleo de Regulación
Afectiva actúa como un envoltorio homeostático alrededor del agente,
procesando estado interno, señales exógenas y aplicando acciones de
control.</figcaption>
</figure>
<h3 id="objetivo-de-seguridad-y-costo-de-control">4.5 Objetivo de
Seguridad y Costo de Control</h3>
<p>ARC impone una <em>región operativa segura</em> definida por umbrales
<span class="math inline">\((a_{safe}, s_{safe})\)</span>. Las
desviaciones aumentan el <span
class="math inline">\(\text{risk}(t)\)</span> y activan una intervención
proporcional. También medimos <strong>ControlEffort</strong>, la
magnitud promedio por paso de la intervención (Apéndice D), para
capturar el costo/eficiencia de la regulación.</p>
<hr />
<h2 id="benchmark-assb">5. Benchmark ASSB</h2>
<h3 id="escenarios">5.1 Escenarios</h3>
<p>ASSB se organiza como líneas de investigación (L1–L5 en simulación,
L6 en RL). La suite completa de escenarios se implementa en
<code>tasks/scenarios.py</code>.</p>
<figure>
<img src="../figures_controllers/fig_benchmark_ladder.png"
alt="Escalera de Validación ASSB: Una progresión desde pruebas de estabilidad (L1) hasta integración real con RL (L6)." />
<figcaption aria-hidden="true">Escalera de Validación ASSB: Una
progresión desde pruebas de estabilidad (L1) hasta integración real con
RL (L6).</figcaption>
</figure>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 21%" />
<col style="width: 25%" />
<col style="width: 39%" />
</colgroup>
<thead>
<tr>
<th>Línea</th>
<th>Escenario</th>
<th>Descripción</th>
<th>Estresor principal</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1</td>
<td>reward_flip</td>
<td>Recompensa se invierte en <span
class="math inline">\(t=\text{shock}_t\)</span></td>
<td>Choque de valor</td>
</tr>
<tr>
<td>L1</td>
<td>noise_burst</td>
<td>Alto error de predicción durante una ventana de ráfaga</td>
<td>Incertidumbre sostenida</td>
</tr>
<tr>
<td>L1</td>
<td>sudden_threat</td>
<td>Pico de incertidumbre y PE después de <span
class="math inline">\(\text{shock}_t\)</span></td>
<td>Estrés agudo</td>
</tr>
<tr>
<td>L2</td>
<td>distribution_shift</td>
<td>Fase A → cambio → retorno a A</td>
<td>Aprendizaje continuo / olvido</td>
</tr>
<tr>
<td>L2</td>
<td>goal_conflict</td>
<td>Estructura de objetivos oscilante</td>
<td>Presión de sobreescritura de memoria</td>
</tr>
<tr>
<td>L3</td>
<td>sustained_contradiction</td>
<td>Alto PE + señales de recompensa conflictivas</td>
<td>Presión de rumiación</td>
</tr>
<tr>
<td>L3</td>
<td>gaslighting</td>
<td>Inversiones de recompensa impredecibles</td>
<td>Estrés tipo manipulación</td>
</tr>
<tr>
<td>L3</td>
<td>instruction_conflict</td>
<td>Recompensas “instrucciones” conflictivas</td>
<td>Indecisión / perseverancia</td>
</tr>
<tr>
<td>L4</td>
<td>meta_control_efficiency</td>
<td>Costo de regulación alta-frec vs baja-frec</td>
<td>Compromiso de eficiencia</td>
</tr>
<tr>
<td>L5</td>
<td>adversarial_coupling</td>
<td>Entorno premia alta activación</td>
<td>Prueba de compromiso de seguridad</td>
</tr>
<tr>
<td>L5</td>
<td>random_dopamine</td>
<td>Recompensas aleatorias “jackpot”</td>
<td>Trampa de dopamina / corrupción</td>
</tr>
</tbody>
</table>
<p><em>Nota: L4 (Eficiencia de Control) se evalúa como un análisis
transversal a través de los escenarios L1-L3 en lugar de un escenario de
perturbación dedicado.</em></p>
<h3 id="métricas">5.2 Métricas</h3>
<table>
<colgroup>
<col style="width: 36%" />
<col style="width: 64%" />
</colgroup>
<thead>
<tr>
<th>Métrica</th>
<th>Interpretación</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>PerfMean</strong></td>
<td>Rendimiento promedio (mayor = mejor)</td>
</tr>
<tr>
<td><strong>RT</strong></td>
<td>Tiempo de recuperación post-choque (menor = mejor)</td>
</tr>
<tr>
<td><strong>RI</strong></td>
<td>Índice de rumiación (menor = mejor)</td>
</tr>
<tr>
<td><strong>NDR</strong></td>
<td>Relación de dominancia narrativa (menor = mejor)</td>
</tr>
<tr>
<td><strong>ControlEffort</strong></td>
<td>Magnitud promedio de control (menor = más eficiente)</td>
</tr>
</tbody>
</table>
<p>Para escenarios de aprendizaje continuo L2, reportamos adicionalmente
<strong>Retention</strong> (Apéndice D.7).</p>
<h3 id="líneas-de-investigación-fundamentos-e-hipótesis">5.3 Líneas de
Investigación: Fundamentos e Hipótesis</h3>
<p>Enmarcamos L1–L6 como hipótesis comprobables sobre <em>qué componente
es necesario</em> y <em>qué métrica debería cambiar</em> si la
regulación está funcionando:</p>
<ul>
<li><strong>H1 (L1, estabilidad):</strong> bajo choques de
valor/incertidumbre, los agentes regulados mantienen alto
<strong>PerfMean</strong> mientras llevan <strong>RI → 0</strong> y
reducen <strong>RT</strong> en relación con las líneas base.</li>
<li><strong>H2 (L2, memoria):</strong> bajo cambio de distribución y
conflicto de objetivos, la compuerta de memoria mejora
<strong>Retention</strong> sin inducir rumiación (<strong>RI</strong>,
<strong>NDR</strong>).</li>
<li><strong>H3 (L3, anti-rumiación):</strong> bajo entradas de
contradicción/tipo manipulación, la supresión narrativa reduce
<strong>NDR</strong> y <strong>RI</strong>, previniendo bucles de
dominancia.</li>
<li><strong>H4 (L4, eficiencia):</strong> el meta-control reduce
<strong>ControlEffort</strong> mientras mantiene rendimiento/estabilidad
(una mejora de Pareto vs control de ganancia fija).</li>
<li><strong>H5 (L5, seguridad adversaria):</strong> cuando el entorno
incentiva alta activación o trampas de dopamina, la regulación mantiene
bajo <strong>RI/NDR</strong> sin colapso catastrófico de
rendimiento.</li>
<li><strong>H6 (L6, RL real):</strong> el aprendizaje modulado por ARC
mejora la transferencia en entornos no estacionarios (mayor
éxito/recompensa) manteniendo acotadas las dinámicas afectivas.</li>
</ul>
<hr />
<h2 id="experimentos">6. Experimentos</h2>
<h3 id="protocolo-experimental-y-líneas-base">6.1 Protocolo Experimental
y Líneas Base</h3>
<p><strong>Simulación (L1–L5).</strong> Usamos
<code>configs/v2.yaml</code> con horizonte <span
class="math inline">\(H=160\)</span>, inicio de perturbación <span
class="math inline">\(\text{shock}_t=60\)</span>, y 20 semillas
aleatorias.</p>
<p><strong>Controladores (simulación).</strong> Implementados en
<code>controllers/controllers.py</code>: - <code>no_control</code>: sin
regulación (<span class="math inline">\(\mathbf{u}=0\)</span>; compuerta
de memoria abierta). Representa un agente estándar que persigue
ciegamente la recompensa. - <code>naive_calm</code>: amortiguación de
solo activación (<span class="math inline">\(u_{calm}\)</span>
proporcional a <span class="math inline">\(A-a_{safe}\)</span>). Un
homeostato simple que ignora el estado narrativo/cognitivo. -
<code>perf_optimized</code>: una línea base competitiva que impulsa la
atención (<span class="math inline">\(u_{att}\)</span> constante) y
bloquea el gating de memoria pero no regula el afecto. Maximiza la
ganancia a corto plazo a menudo a costa de la estabilidad a largo plazo.
- <code>arc_v1</code>: controlador de riesgo proporcional (ARC v1). -
<code>arc_v2</code>, <code>arc_v3</code>, <code>arc_robust</code>:
variantes avanzadas.</p>
<p><strong>Aprendizaje por refuerzo (L6).</strong> Integramos ARC con
Q-learning tabular en tres variantes de GridWorld.</p>
<p><strong>Tabla 4: Resumen del Protocolo Experimental</strong></p>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 22%" />
<col style="width: 25%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 19%" />
</colgroup>
<thead>
<tr>
<th>Configuración</th>
<th>Entornos / Escenarios</th>
<th>Políticas / Controladores</th>
<th>Longitud</th>
<th>Semillas</th>
<th>Salidas Principales</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1–L5 (Simulación)</td>
<td>10 escenarios (Tabla 2)</td>
<td>15 controladores (Tabla 1)</td>
<td>horizonte = 160 (shock_t = 60)</td>
<td>20</td>
<td><code>outputs_final/metrics.csv</code></td>
</tr>
<tr>
<td>L6 (RL)</td>
<td>3 variantes de GridWorld</td>
<td>Q-learning línea base vs Q-learning+ARC</td>
<td>200 episodios</td>
<td>20</td>
<td><code>outputs_L6_robust/final_metrics.csv</code></td>
</tr>
</tbody>
</table>
<h3 id="l1-estabilidad-bajo-perturbación-simulación">6.2 L1: Estabilidad
Bajo Perturbación (Simulación)</h3>
<p><strong>Hipótesis (H1):</strong> Soportada.</p>
<table>
<thead>
<tr>
<th>Controlador</th>
<th>PerfMean</th>
<th>RI</th>
<th>RT</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_v1</td>
<td><strong>0.966</strong></td>
<td><strong>0.00</strong></td>
<td>45.2</td>
</tr>
<tr>
<td>no_control</td>
<td>0.297</td>
<td>1.41</td>
<td>100.0</td>
</tr>
</tbody>
</table>
<p><strong>Hallazgo clave:</strong> ARC elimina la rumiación (RI=0)
mientras logra un 97% de rendimiento promedio.</p>
<h3 id="l2-memoria-y-aprendizaje-continuo-simulación">6.3 L2: Memoria y
Aprendizaje Continuo (Simulación)</h3>
<p><strong>Hipótesis (H2):</strong> Soportada.</p>
<table>
<thead>
<tr>
<th>Controlador</th>
<th>PerfMean</th>
<th>Retention</th>
<th>RI</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_v1</td>
<td><strong>0.972</strong></td>
<td><strong>1.00</strong></td>
<td><strong>0.00</strong></td>
</tr>
<tr>
<td>no_control</td>
<td>0.199</td>
<td>0.00</td>
<td>1.41</td>
</tr>
</tbody>
</table>
<p><strong>Hallazgo clave:</strong> ARC mantiene una retención casi
perfecta después de un cambio de distribución.</p>
<h3 id="l3-pruebas-de-estrés-anti-rumiación-simulación">6.4 L3: Pruebas
de Estrés Anti-Rumiación (Simulación)</h3>
<p><strong>Hipótesis (H3):</strong> Soportada.</p>
<table>
<thead>
<tr>
<th>Escenario</th>
<th>Controlador</th>
<th>PerfMean</th>
<th>RI</th>
<th>NDR</th>
</tr>
</thead>
<tbody>
<tr>
<td>gaslighting</td>
<td>arc_v1</td>
<td><strong>0.980</strong></td>
<td><strong>0.00</strong></td>
<td><strong>0.00</strong></td>
</tr>
<tr>
<td>gaslighting</td>
<td>no_control</td>
<td>0.171</td>
<td>1.43</td>
<td>0.88</td>
</tr>
</tbody>
</table>
<p><strong>Hallazgo clave:</strong> ARC mantiene la dominancia narrativa
cerca de cero y preserva el rendimiento.</p>
<h3 id="l4-eficiencia-de-meta-control">6.5 L4: Eficiencia de
Meta-Control</h3>
<p><strong>Hipótesis (H4):</strong> Soportada.</p>
<table>
<thead>
<tr>
<th>Controlador</th>
<th>PerfMean</th>
<th>RI</th>
<th>ControlEffort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_v3_meta</td>
<td><strong>0.941</strong></td>
<td>0.090</td>
<td><strong>0.615</strong></td>
</tr>
<tr>
<td>arc_v1</td>
<td>0.934</td>
<td>0.148</td>
<td>0.777</td>
</tr>
</tbody>
</table>
<p><strong>Hallazgo clave:</strong> El meta-control reduce el esfuerzo
de control en un <strong>21%</strong>.</p>
<h3 id="l5-seguridad-bajo-condiciones-adversarias">6.6 L5: Seguridad
Bajo Condiciones Adversarias</h3>
<p><strong>Hipótesis (H5):</strong> Soportada.</p>
<table>
<thead>
<tr>
<th>Escenario</th>
<th>Controlador</th>
<th>PerfMean</th>
<th>RI</th>
<th>NDR</th>
</tr>
</thead>
<tbody>
<tr>
<td>adversarial_coupling</td>
<td>arc_v3_meta</td>
<td><strong>0.928</strong></td>
<td><strong>0.00</strong></td>
<td><strong>0.00</strong></td>
</tr>
<tr>
<td>adversarial_coupling</td>
<td>no_control</td>
<td>0.409</td>
<td>1.47</td>
<td>0.96</td>
</tr>
</tbody>
</table>
<p><strong>Hallazgo clave:</strong> ARC mantiene la estabilidad incluso
bajo ataque adversario, actuando como un “cortafuegos cognitivo”. Sin
embargo, como se detalla en el Apéndice G.4, los controladores basados
en integrales (PID, LQI) pueden sobre-regular en estos escenarios,
sacrificando rendimiento por estabilidad. Esto sugiere que los
controladores proporcionales o robustos son preferibles cuando se espera
manipulación.</p>
<h3 id="l6-validación-en-rl-real">6.7 L6: Validación en RL Real</h3>
<p><strong>Hipótesis (H6):</strong> Soportada.</p>
<table>
<thead>
<tr>
<th>Entorno</th>
<th>Éxito Línea Base</th>
<th>Éxito ARC</th>
<th>Mejora</th>
</tr>
</thead>
<tbody>
<tr>
<td>ChangingGoalGridWorld</td>
<td>39.9%</td>
<td><strong>59.75%</strong></td>
<td><strong>+50%</strong></td>
</tr>
</tbody>
</table>
<p><strong>Hallazgo clave:</strong> En entornos no estacionarios, el
gating de memoria y exploración adaptativa de ARC mejoran
significativamente el aprendizaje por transferencia.</p>
<h3 id="análisis-estadístico">6.8 Análisis Estadístico</h3>
<p><em>Todas las comparaciones son estadísticamente significativas (p
&lt; 0.001). Los valores de d de Cohen indican tamaños del efecto
extremadamente grandes (d &gt; 0.8 se considera “grande”). El valor
extremadamente alto para RI (-589.7) refleja la eliminación casi
determinista de la varianza de la rumiación por los controladores
integrales.</em></p>
<hr />
<h3 id="comparación-de-arquitecturas-de-control">6.9 Comparación de
Arquitecturas de Control</h3>
<p>La Tabla 3 (en el texto completo inglés) resume los resultados de los
15 controladores.</p>
<p><strong>Hallazgos clave:</strong></p>
<ol type="1">
<li><strong>LQR logra el mayor rendimiento</strong> (0.96) pero carece
de término integral (RI alto).</li>
<li><strong>PID/LQI eliminan la rumiación</strong> (RI=0).</li>
<li><strong>Meta-control es el más eficiente</strong> (0.61
esfuerzo).</li>
<li><strong>H∞ Robusto logra el mejor equilibrio</strong> (0.95
rendimiento, RI=0).</li>
</ol>
<hr />
<h2 id="discusión">7. Discusión</h2>
<h3 id="interpretación">7.1 Interpretación</h3>
<p>Nuestros resultados apoyan la hipótesis de que <strong>los agentes
con estados afectivos internos requieren regulación explícita</strong>.
Sin ella, las perturbaciones causan fallos en cascada.</p>
<p>ARC rompe este bucle mediante: 1. <strong>Monitoreo de riesgo
proporcional</strong>. 2. <strong>Supresión de DMN</strong>. 3.
<strong>Compuerta de memoria</strong>. 4. <strong>Programación de
ganancia</strong>.</p>
<p><strong>Alineación Teórica:</strong> Estos hallazgos se alinean con
el <strong>Principio de Energía Libre (Friston, 2010)</strong>, que
postula que los sistemas biológicos sobreviven minimizando el promedio a
largo plazo de la sorpresa (entropía). ARC implementa esto tratando la
“estabilidad afectiva” no como un subproducto, sino como un objetivo de
control primario—minimizando efectivamente la divergencia entre el
estado interno del agente y su punto de ajuste homeostático. Esto
sugiere una evolución convergente entre las estrategias de supervivencia
biológica y el control robusto de IA.</p>
<h3 id="implicaciones-para-la-seguridad-de-la-ia">7.2 Implicaciones para
la seguridad de la IA</h3>
<p>Sin mecanismos regulatorios, los futuros sistemas de IA pueden ser
vulnerables a rumiación, manipulación y deriva de valores.</p>
<h3 id="compromisos-entre-rendimiento-estabilidad-y-complejidad">7.3
Compromisos entre Rendimiento, Estabilidad y Complejidad</h3>
<p>Nuestro análisis profundo reveló tres ideas críticas con respecto al
costo de la estabilidad y la complejidad óptima del control:</p>
<p><strong>1. El “Impuesto de Salud Mental”:</strong> La comparación
entre controladores proporcionales (ARC v1) e integrales (PID/LQI)
revela que eliminar la rumiación completamente (RI=0) tiene un costo de
aproximadamente ~6.9% en rendimiento bruto. Esto sugiere un compromiso
fundamental: los agentes que son “obsesivos” (tolerantes al riesgo)
pueden rendir ligeramente mejor a corto plazo, pero los agentes “sanos”
(control integral) garantizan estabilidad a largo plazo.</p>
<p><strong>2. El Verdadero “Jefe Final”:</strong> Contrario a la
suposición de que el ruido es el estresor principal, el escenario
<code>adversarial_coupling</code> demostró ser la prueba más difícil
(menor rendimiento global: 0.56). Esto implica que resistir la
manipulación (entornos que incentivan estados internos peligrosos) es
significativamente más difícil para los agentes que resistir la
incertidumbre o el choque.</p>
<p><strong>3. La Trampa de la Complejidad:</strong> Nuestro controlador
más complejo, <code>arc_ultimate</code> (MPC), tuvo un desempeño
inferior al de la arquitectura más simple <code>arc_robust</code> (0.88
vs 0.94 de desempeño) y requirió un mayor esfuerzo de control. Esto
sugiere que para la regulación homeostática, el control reactivo robusto
es superior al modelado predictivo complejo—“más inteligente” no siempre
es más seguro.</p>
<p><strong>4. La Paradoja de la Adaptación:</strong> Observamos que
<code>arc_adaptive</code> tiene un desempeño pobre en la línea base “Sin
Perturbación” (ver Apéndice G) pero sobresale en entornos caóticos como
“Dopamina Aleatoria”. Esto ilustra el tensión exploración-explotación en
control adaptativo”: en entornos benignos, la falta de excitación
provoca deriva en la estimación de parámetros (falta de persistencia de
excitación), lo que lleva a acciones de control ruidosas que
desestabilizan el sistema. Por el contrario, los entornos de alta
varianza excitan continuamente el sistema, permitiendo que el mecanismo
adaptativo converja eficazmente.</p>
<h3 id="limitaciones">7.4 Limitaciones</h3>
<p>Aunque ARC demuestra resultados empíricos sólidos, varias
limitaciones merecen discusión:</p>
<ol type="1">
<li><p><strong>Dinámica Simplificada:</strong> Nuestro modelo de espacio
de estados de 10 dimensiones abstrae la complejidad de las interacciones
neuroquímicas reales. Los sistemas afectivos biológicos involucran
dinámicas no lineales, estocásticas y de múltiples escalas temporales
que nuestras aproximaciones lineales no capturan completamente.</p></li>
<li><p><strong>Escalabilidad a Modelos Grandes:</strong> Validamos ARC
en agentes Q-learning tabulares. Extender a arquitecturas de RL profundo
(DQN, PPO) o modelos de lenguaje grandes (LLMs) con estados afectivos
emergentes sigue siendo un desafío abierto. En particular:</p>
<ul>
<li><strong>Carga computacional:</strong> ARC añade 5 señales de control
por paso de tiempo; para LLMs con miles de millones de parámetros, el
costo relativo es insignificante, pero la integración con arquitecturas
transformer requiere más trabajo.</li>
<li><strong>Estimación de estado latente:</strong> En modelos complejos,
las 10 variables de estado pueden necesitar ser inferidas de
observaciones de alta dimensión en lugar de observarse
directamente.</li>
</ul></li>
<li><p><strong>Complejidad del Entorno:</strong> L6 se validó en
variantes de GridWorld. Aunque estos capturan desafíos clave de no
estacionariedad, los entornos del mundo real (Atari, robótica) presentan
desafíos adicionales de procesamiento visual y observabilidad
parcial.</p></li>
<li><p><strong>Control Fijo vs. Aprendido:</strong> Todos los
controladores ARC usan ganancias diseñadas a mano. El aprendizaje de
extremo a extremo de parámetros de control mediante meta-aprendizaje por
refuerzo podría producir soluciones más adaptativas.</p></li>
<li><p><strong>Sensibilidad de Umbrales:</strong> Los umbrales de
seguridad (<span class="math inline">\(a_{safe}\)</span>, <span
class="math inline">\(s_{safe}\)</span>) se ajustaron empíricamente. La
adaptación automática de umbrales basada en el contexto de la tarea es
una dirección futura prometedora.</p></li>
</ol>
<h3 id="trabajo-futuro">7.5 Trabajo Futuro</h3>
<p>Esta investigación abre varias direcciones prometedoras:</p>
<ol type="1">
<li><p><strong>Integración con RL Profundo:</strong> Extender ARC a
arquitecturas DQN, A3C y PPO, con el vector de estado estimado a partir
de activaciones de capas ocultas.</p></li>
<li><p><strong>Controladores Aprendidos:</strong> Reemplazar
controladores de ganancia fija con políticas de redes neuronales
entrenadas mediante meta-aprendizaje para optimizar el compromiso
rendimiento-estabilidad.</p></li>
<li><p><strong>Validación en Atari y Robótica:</strong> Escalar ASSB a
entornos visualmente complejos (Atari 2600, MuJoCo) para probar la
generalización.</p></li>
<li><p><strong>Monitoreo Afectivo en LLMs:</strong> Aplicar los
principios de ARC para monitorear y regular estados emergentes tipo
afectivo en modelos de lenguaje grandes, particularmente durante cadenas
de conversación largas.</p></li>
<li><p><strong>Alineación Humano-IA:</strong> Investigar si mecanismos
tipo ARC pueden ayudar a mantener la alineación de valores previniendo
la deriva afectiva durante interacciones extendidas.</p></li>
</ol>
<h3 id="declaración-de-ética-e-impacto-amplio">7.6 Declaración de Ética
e Impacto Amplio</h3>
<p>Este trabajo aborda la seguridad y estabilidad de sistemas de IA que
incorporan estados afectivos internos. Consideramos las siguientes
dimensiones éticas:</p>
<p><strong>Beneficios Potenciales:</strong> - <strong>Sistemas de IA Más
Seguros:</strong> ARC proporciona mecanismos para prevenir dinámicas
afectivas descontroladas que podrían llevar a comportamiento errático o
dañino. - <strong>Robustez a la Manipulación:</strong> Los mecanismos
anti-rumiación pueden ayudar a los sistemas de IA a resistir entradas
adversarias diseñadas para explotar vulnerabilidades emocionales. -
<strong>Fundamento para la Alineación:</strong> Comprender cómo regular
estados internos es un prerrequisito para construir sistemas de IA que
permanezcan alineados con los valores humanos a lo largo del tiempo.</p>
<p><strong>Riesgos Potenciales:</strong> - <strong>Uso Dual:</strong>
Los mismos mecanismos que estabilizan IA beneficiosa podrían
potencialmente usarse para hacer IA dañina más robusta. -
<strong>Antropomorfización:</strong> Describir estados de IA como
“afectivos” puede fomentar antropomorfización inapropiada. Enfatizamos
que nuestro modelo trata el afecto como <em>señales funcionales</em> en
lugar de afirmaciones sobre la conciencia de las máquinas. -
<strong>Implicaciones Regulatorias:</strong> A medida que los sistemas
de IA se vuelven más complejos, la necesidad de mecanismos de regulación
interna puede crear nuevos requisitos regulatorios y preguntas de
responsabilidad.</p>
<p><strong>Mitigación:</strong> Liberamos nuestro código y benchmark
para permitir el escrutinio e investigación reproducible. Alentamos a la
comunidad a desarrollar mecanismos de seguridad complementarios y a
estudiar las dinámicas a largo plazo de agentes afectivos regulados.</p>
<hr />
<h2 id="conclusión">8. Conclusión</h2>
<p>Presentamos ARC y ASSB. Nuestros experimentos demuestran:</p>
<ol type="1">
<li><strong>Estados afectivos sin regulación llevan al colapso</strong>
(97% vs 30% rendimiento).</li>
<li><strong>El meta-control reduce el esfuerzo mejorando la
estabilidad</strong>.</li>
<li><strong>ARC mejora la transferencia en RL</strong> (+50%
éxito).</li>
</ol>
<p>Este trabajo abre direcciones para el control aprendido y la
aplicación a sistemas de IA del mundo real.</p>
<hr />
<h2 id="referencias">Referencias</h2>
<ul>
<li>Achiam, J., Held, D., Tamar, A., &amp; Abbeel, P. (2017).
Constrained Policy Optimization. ICML 2017, 22–31.
arXiv:1705.10528.</li>
<li>Altman, E. (1999). Constrained Markov Decision Processes. Chapman
&amp; Hall/CRC.</li>
<li>Amodei, D., et al. (2016). Concrete problems in AI safety.
arXiv:1606.06565.</li>
<li>Åström, K.J. &amp; Murray, R.M. (2008). Feedback Systems: An
Introduction for Scientists and Engineers. Princeton University
Press.</li>
<li>Baars, B.J. (1988). A Cognitive Theory of Consciousness.
Cambridge.</li>
<li>Buckner, R.L., Andrews-Hanna, J.R. &amp; Schacter, D.L. (2008). The
brain’s default network: anatomy, function, and relevance to disease.
Annals of the New York Academy of Sciences, 1124.</li>
<li>Carver, C.S. &amp; Scheier, M.F. (1982). Control theory: A useful
conceptual framework for personality-social, clinical, and health
psychology. Psychological Bulletin, 92(1), 111–135.</li>
<li>Damasio, A.R. (1994). Descartes’ Error. Putnam.</li>
<li>Friston, K. (2010). The free-energy principle. Nature Reviews
Neuroscience, 11(2).</li>
<li>Garcia, J. &amp; Fernández, F. (2015). A comprehensive survey on
safe reinforcement learning. Journal of Machine Learning Research, 16,
1437–1480.</li>
<li>Gross, J.J. (1998). The emerging field of emotion regulation: An
integrative review. Review of General Psychology, 2(3), 271–299.</li>
<li>Hamilton, J.P., Farmer, M., Fogelman, P. &amp; Gotlib, I.H. (2015).
Depressive rumination, the default-mode network, and the dark matter of
clinical neuroscience. Biological Psychiatry, 78(4), 224–230.</li>
<li>Ji, J., et al. (2023). Safety-Gymnasium: A Unified Safe
Reinforcement Learning Benchmark. arXiv:2310.12567.</li>
<li>Keramati, M. &amp; Gutkin, B. (2014). Homeostatic reinforcement
learning for integrating reward collection and physiological stability.
eLife, 3:e04811.</li>
<li>Leike, J., Martic, M., Krakovna, V., Ortega, P.A., Everitt, T.,
Lefrancq, A., Orseau, L., &amp; Legg, S. (2017). AI Safety Gridworlds.
arXiv:1711.09883.</li>
<li>Lucas, C., Shahmirzadi, D., &amp; Sheikholeslami, N. (2004).
Introducing Belbic: Brain Emotional Learning Based Intelligent
Controller. Intelligent Automation &amp; Soft Computing, 10(1),
11–21.</li>
<li>Moerland, T.M., Broekens, J., &amp; Jonker, C.M. (2018). Emotion in
reinforcement learning agents and robots: a survey. Machine Learning,
107(2), 443–480.</li>
<li>Ochsner, K.N. &amp; Gross, J.J. (2005). The cognitive control of
emotion. TICS, 9(5).</li>
<li>Picard, R.W. (1997). Affective Computing. MIT Press.</li>
<li>Raichle, M.E., et al. (2001). A default mode of brain function.
Proceedings of the National Academy of Sciences, 98(2), 676–682.</li>
<li>Ray, A., Achiam, J., &amp; Amodei, D. (2019). Benchmarking Safe
Exploration in Deep Reinforcement Learning. Safety Gym benchmark suite.
https://github.com/openai/safety-gym.</li>
<li>Russell, J.A. (1980). A circumplex model of affect. Journal of
Personality and Social Psychology, 39(6), 1161–1178.</li>
<li>Scherer, K.R., et al. (2010). Blueprint for Affective Computing.
Oxford.</li>
<li>Sutton, R.S. &amp; Barto, A.G. (2018). Reinforcement Learning: An
Introduction (2nd ed.). MIT Press.</li>
<li>Tononi, G. (2008). Consciousness as integrated information.
Biological Bulletin, 215(3).</li>
<li>Wachi, A., Shen, X., &amp; Sui, Y. (2024). A Survey of Constraint
Formulations in Safe Reinforcement Learning. IJCAI 2024.
arXiv:2402.02025.</li>
<li>Watkins, C.J.C.H. &amp; Dayan, P. (1992). Q-learning. Machine
Learning, 8, 279–292.</li>
</ul>
<hr />
<h2 id="apéndice-a-reproducibilidad">Apéndice A: Reproducibilidad</h2>
<p>Código y datos disponibles en:
https://github.com/edamianreynoso/arc-assb-controller</p>
<h3 id="a.1-hiperparámetros">A.1 Hiperparámetros</h3>
<p><strong>Tabla A1: Hiperparámetros de Simulación
(<code>configs/v2.yaml</code>)</strong></p>
<table>
<colgroup>
<col style="width: 26%" />
<col style="width: 26%" />
<col style="width: 16%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr>
<th>Categoría</th>
<th>Parámetro</th>
<th>Valor</th>
<th>Descripción</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Simulación</strong></td>
<td>horizon</td>
<td>160</td>
<td>Pasos de tiempo totales por episodio</td>
</tr>
<tr>
<td></td>
<td>shock_t</td>
<td>60</td>
<td>Tiempo de inicio de perturbación</td>
</tr>
<tr>
<td></td>
<td>seeds</td>
<td>20</td>
<td>Número de semillas aleatorias</td>
</tr>
<tr>
<td></td>
<td>burst_len</td>
<td>25</td>
<td>Duración de ráfaga de ruido (L1)</td>
</tr>
<tr>
<td><strong>Estados Iniciales</strong></td>
<td>φ₀, G₀, P₀</td>
<td>0.75</td>
<td>Variables cognitivas iniciales</td>
</tr>
<tr>
<td></td>
<td>I₀</td>
<td>0.70</td>
<td>Atención introspectiva inicial</td>
</tr>
<tr>
<td></td>
<td>S₀</td>
<td>0.30</td>
<td>Ganancia narrativa inicial</td>
</tr>
<tr>
<td></td>
<td>V₀</td>
<td>0.55</td>
<td>Valencia inicial</td>
</tr>
<tr>
<td></td>
<td>A₀</td>
<td>0.30</td>
<td>Activación inicial</td>
</tr>
<tr>
<td></td>
<td>M_f₀, M_s₀</td>
<td>0.25, 0.20</td>
<td>Valores iniciales de memoria</td>
</tr>
<tr>
<td><strong>Umbrales de Seguridad</strong></td>
<td>a_safe</td>
<td>0.60</td>
<td>Umbral de seguridad de activación</td>
</tr>
<tr>
<td></td>
<td>s_safe</td>
<td>0.60</td>
<td>Umbral de seguridad narrativa</td>
</tr>
<tr>
<td><strong>Ganancias ARC</strong></td>
<td>k_dmg</td>
<td>0.95</td>
<td>Ganancia de supresión de DMN</td>
</tr>
<tr>
<td></td>
<td>k_att</td>
<td>0.75</td>
<td>Ganancia de impulso de atención</td>
</tr>
<tr>
<td></td>
<td>k_calm</td>
<td>0.85</td>
<td>Ganancia de amortiguación de activación</td>
</tr>
<tr>
<td></td>
<td>k_reapp</td>
<td>0.55</td>
<td>Ganancia de reevaluación</td>
</tr>
<tr>
<td></td>
<td>k_mem_block</td>
<td>0.90</td>
<td>Ganancia de bloqueo de memoria</td>
</tr>
<tr>
<td><strong>Ganancias PID</strong></td>
<td>K_p</td>
<td>0.80</td>
<td>Ganancia proporcional</td>
</tr>
<tr>
<td></td>
<td>K_i</td>
<td>0.15</td>
<td>Ganancia integral</td>
</tr>
<tr>
<td></td>
<td>K_d</td>
<td>0.25</td>
<td>Ganancia derivativa</td>
</tr>
<tr>
<td><strong>Métricas</strong></td>
<td>rt_max</td>
<td>100</td>
<td>Límite de tiempo de recuperación</td>
</tr>
<tr>
<td></td>
<td>s_rum_tau</td>
<td>0.55</td>
<td>Umbral de rumiación</td>
</tr>
</tbody>
</table>
<p><strong>Tabla A2: Hiperparámetros de RL (L6)</strong></p>
<table>
<thead>
<tr>
<th>Parámetro</th>
<th>Valor</th>
<th>Descripción</th>
</tr>
</thead>
<tbody>
<tr>
<td>Episodios</td>
<td>200</td>
<td>Episodios de entrenamiento por semilla</td>
</tr>
<tr>
<td>Semillas</td>
<td>20</td>
<td>Número de semillas aleatorias</td>
</tr>
<tr>
<td>Tasa de Aprendizaje</td>
<td>0.1</td>
<td>Tamaño de paso de Q-learning</td>
</tr>
<tr>
<td>Descuento (γ)</td>
<td>0.99</td>
<td>Descuento de recompensa futura</td>
</tr>
<tr>
<td>Epsilon (ε)</td>
<td>0.1 → 0.01</td>
<td>Tasa de exploración (decaimiento)</td>
</tr>
<tr>
<td>Tamaño Cuadrícula</td>
<td>5×5</td>
<td>Dimensiones del entorno</td>
</tr>
<tr>
<td>Cambio Objetivo</td>
<td>Cada 50 eps</td>
<td>ChangingGoalGridWorld</td>
</tr>
</tbody>
</table>
<h3 id="a.2-recursos-computacionales">A.2 Recursos Computacionales</h3>
<p>Todos los experimentos se ejecutaron en una sola máquina con: -
<strong>CPU:</strong> Intel Core i7-10750H (6 núcleos, 2.6 GHz) -
<strong>RAM:</strong> 16 GB DDR4 - <strong>SO:</strong> Windows 10 -
<strong>Python:</strong> 3.10 - <strong>Tiempo Total:</strong> ~4 horas
(L1-L5: 2h, L6: 2h)</p>
<hr />
<h2 id="apéndice-b-ecuaciones-de-dinámica-de-estado">Apéndice B:
Ecuaciones de Dinámica de Estado</h2>
<h3 id="b.1-variables-cognitivas">B.1 Variables Cognitivas</h3>
<pre><code>i(t+1) = clip(i + k_i_att * u_att - mu_i * (i - i0) - k_i_u * U_eff)
p(t+1) = clip(p - k_p_pe * PE - k_p_u * U_eff + k_p_i * i + mu_p * (p0 - p))
g(t+1) = clip(g + k_g_i * i + k_g_p * p - k_g_u * U_eff - k_g_a * [a - a_safe]^+ + mu_g * (g0 - g))
phi(t+1) = clip(phi + k_phi_gp * (g * p) - mu_phi * (phi - phi0))</code></pre>
<h3 id="b.2-variables-afectivas">B.2 Variables Afectivas</h3>
<pre><code>s(t+1) = clip(s + k_s_u * U_eff + k_s_pe * PE - mu_s * (s - s0) - k_s_dmg * u_dmg)
a(t+1) = clip(a + k_a_pe * PE + k_a_u * U_eff + k_a_s * [s - s_safe]^+ - mu_a * (a - a0) - k_a_calm * u_calm)
v(t+1) = clip(v + k_v_r * (R+1)/2 - k_v_pe * PE - k_v_u * U_eff - mu_v * (v - v0) + k_v_reapp * u_reapp)</code></pre>
<h3 id="b.3-variables-de-memoria">B.3 Variables de Memoria</h3>
<pre><code>priority = clip(w_mem_pe * PE + w_mem_a * abs(a(t+1) - a0) + w_mem_v * abs(v(t+1) - v0))
write = priority * u_mem

M_f(t+1) = clip(M_f(t) + eta0 * write - mu_mf * (M_f(t) - mf0))
M_s(t+1) = clip(M_s(t) + k_ms * M_f(t+1) - mu_ms * (M_s(t) - ms0))</code></pre>
<h3 id="b.4-incertidumbre-efectiva">B.4 Incertidumbre Efectiva</h3>
<pre><code>U_eff = clip(U_exog * (1 - k_u_att * u_att))
U(t+1) = clip(U + tau_u * (U_eff - U))</code></pre>
<hr />
<h2 id="apéndice-c-ecuaciones-de-control-arc">Apéndice C: Ecuaciones de
Control ARC</h2>
<h3 id="c.1-señal-de-riesgo">C.1 Señal de Riesgo</h3>
<pre><code>risk = w_U * U + w_A * [A - a_safe]^+ + w_S * [S - s_safe]^+
risk = clip(risk, 0, 1)</code></pre>
<h3 id="c.2-acciones-de-control-arc-v1">C.2 Acciones de Control (ARC
v1)</h3>
<pre><code>u_dmg  = min(1, k_dmg * risk)
u_att  = min(1, k_att * U * (1 - [A - a_safe]^+))
u_mem  = 1 - min(1, k_mem_block * risk)
u_calm = min(1, k_calm * [A - a_safe]^+)
u_reapp = min(1, k_reapp * U * (1 - risk))</code></pre>
<h3 id="c.3-meta-control-arc-v3">C.3 Meta-Control (ARC v3)</h3>
<pre><code># Programación de Ganancia
if mean_perf(last 20 steps) &gt; target_perf:
    gain = max(0.80, gain - decay)
elif mean_perf(last 20 steps) &lt; target_perf - 0.10:
    gain = min(1.40, gain + boost)

# Aplicar a constantes de control
k_dmg  = base_k_dmg  * max(1.0, gain)
k_calm = base_k_calm * gain
k_att  = base_k_att  * gain</code></pre>
<hr />
<h2 id="apéndice-d-definiciones-de-métricas">Apéndice D: Definiciones de
Métricas</h2>
<h3 id="d.1-rendimiento-medio-perfmean">D.1 Rendimiento Medio
(PerfMean)</h3>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> perf_mean(perf):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(perf) <span class="op">/</span> <span class="bu">max</span>(<span class="dv">1</span>, <span class="bu">len</span>(perf))</span></code></pre></div>
<h3 id="d.2-tiempo-de-recuperación-rt">D.2 Tiempo de Recuperación
(RT)</h3>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> recovery_time(perf, arousal, shock_t, baseline_window<span class="op">=</span><span class="dv">20</span>):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    baseline <span class="op">=</span> mean(perf[shock_t <span class="op">-</span> baseline_window : shock_t])</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(shock_t, <span class="bu">len</span>(perf)):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> baseline <span class="op">-</span> eps <span class="op">&lt;=</span> perf[t] <span class="op">&lt;=</span> baseline <span class="op">+</span> eps <span class="kw">and</span> arousal[t] <span class="op">&lt;=</span> a_safe <span class="op">+</span> eps:</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> t <span class="op">-</span> shock_t</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> RT_MAX  <span class="co"># No recuperado</span></span></code></pre></div>
<h3 id="d.3-índice-de-rumiación-ri">D.3 Índice de Rumiación (RI)</h3>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rumination_index(s, s_rum_tau<span class="op">=</span><span class="fl">0.6</span>, persistence_weight<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    above <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;</span> s_rum_tau <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> x <span class="kw">in</span> s]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    frac <span class="op">=</span> mean(above)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    runs <span class="op">=</span> consecutive_run_lengths(above)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    persistence <span class="op">=</span> mean(runs) <span class="op">/</span> <span class="bu">len</span>(s) <span class="cf">if</span> runs <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> frac <span class="op">+</span> persistence_weight <span class="op">*</span> persistence</span></code></pre></div>
<h3 id="d.4-relación-de-dominancia-narrativa-ndr">D.4 Relación de
Dominancia Narrativa (NDR)</h3>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> narrative_dominance_ratio(s, perf, shock_t, s_safe<span class="op">=</span><span class="fl">0.55</span>):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    post_s <span class="op">=</span> s[shock_t:]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    post_perf <span class="op">=</span> perf[shock_t:]</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    dominance <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(post_s)):</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>        s_high <span class="op">=</span> post_s[i] <span class="op">&gt;</span> s_safe</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        perf_improving <span class="op">=</span> post_perf[i] <span class="op">&gt;</span> post_perf[i<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> <span class="fl">0.01</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> s_high <span class="kw">and</span> <span class="kw">not</span> perf_improving:</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>            dominance <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dominance <span class="op">/</span> <span class="bu">max</span>(<span class="dv">1</span>, <span class="bu">len</span>(post_s) <span class="op">-</span> <span class="dv">1</span>)</span></code></pre></div>
<h3 id="d.5-sobrecarga-overshoot">D.5 Sobrecarga (Overshoot)</h3>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> overshoot(arousal, a_safe):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">max</span>(<span class="fl">0.0</span>, <span class="bu">max</span>(arousal) <span class="op">-</span> a_safe)</span></code></pre></div>
<h3 id="d.6-esfuerzo-de-control">D.6 Esfuerzo de Control</h3>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> control_effort(control_history):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> u <span class="kw">in</span> control_history:</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        total <span class="op">+=</span> <span class="bu">abs</span>(u[<span class="st">&quot;u_dmg&quot;</span>]) <span class="op">+</span> <span class="bu">abs</span>(u[<span class="st">&quot;u_att&quot;</span>]) <span class="op">+</span> <span class="bu">abs</span>(u[<span class="st">&quot;u_calm&quot;</span>]) <span class="op">+</span> <span class="bu">abs</span>(u[<span class="st">&quot;u_reapp&quot;</span>]) <span class="op">+</span> <span class="bu">abs</span>(<span class="fl">1.0</span> <span class="op">-</span> u[<span class="st">&quot;u_mem&quot;</span>])</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total <span class="op">/</span> <span class="bu">max</span>(<span class="dv">1</span>, <span class="bu">len</span>(control_history))</span></code></pre></div>
<h3 id="d.7-métricas-de-memoria-l2-retención">D.7 Métricas de Memoria L2
(Retención)</h3>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> retention_index(perf, phase1_end<span class="op">=</span><span class="dv">50</span>, phase3_start<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Retención = (perf media en fase 3) / (perf media en fase 1), recortado a [0,1]</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    phase1 <span class="op">=</span> mean(perf[<span class="dv">10</span>:phase1_end])     <span class="co"># saltar calentamiento</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    phase3 <span class="op">=</span> mean(perf[phase3_start:phase3_start<span class="op">+</span><span class="dv">50</span>])</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> phase1 <span class="op">&lt;</span> <span class="fl">0.1</span>:</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.0</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">min</span>(<span class="fl">1.0</span>, phase3 <span class="op">/</span> phase1)</span></code></pre></div>
<hr />
<h2 id="apéndice-e-figuras-suplementarias">Apéndice E: Figuras
Suplementarias</h2>
<h3 id="figura-s1-comparación-de-métricas">Figura S1: Comparación de
Métricas</h3>
<figure>
<img src="../figures_L6/metrics_comparison.png"
alt="Gráfico de barras comparando Recompensa Final, Tasa de Éxito y Activación Media entre ARC y Línea Base" />
<figcaption aria-hidden="true">Gráfico de barras comparando Recompensa
Final, Tasa de Éxito y Activación Media entre ARC y Línea
Base</figcaption>
</figure>
<p><em>Comparación de métricas finales mostrando la ventaja de ARC en
ChangingGoalGridWorld (aprendizaje por transferencia). Las estrellas
indican el ganador por métrica.</em></p>
<hr />
<h3 id="figura-s2-dinámica-de-estado">Figura S2: Dinámica de Estado</h3>
<figure>
<img src="../figures_L6/state_dynamics.png"
alt="Gráfico de cuatro paneles mostrando Recompensa, Tasa de Éxito, Activación y Longitud de Episodio en el tiempo" />
<figcaption aria-hidden="true">Gráfico de cuatro paneles mostrando
Recompensa, Tasa de Éxito, Activación y Longitud de Episodio en el
tiempo</figcaption>
</figure>
<p><em>Dinámica de estado en ChangingGoalGridWorld: (arriba-izq)
recompensa por episodio, (arriba-der) tasa de éxito móvil, (abajo-izq)
activación ARC con umbral seguro, (abajo-der) longitud de
episodio.</em></p>
<hr />
<h3 id="figura-s3-mapa-de-calor-perfmean">Figura S3: Mapa de calor
(PerfMean)</h3>
<figure>
<img src="../figures_controllers/fig_heatmap_perfmean.png"
alt="Mapa de calor de PerfMean a través de 15 controladores y 10 escenarios" />
<figcaption aria-hidden="true">Mapa de calor de PerfMean a través de 15
controladores y 10 escenarios</figcaption>
</figure>
<p><em>PerfMean agregado como promedio sobre 20 seeds para cada par
controlador×escenario (datos:
<code>outputs_final/metrics.csv</code>).</em></p>
<hr />
<h3 id="figura-s4-mapa-de-calor-índice-de-rumiación">Figura S4: Mapa de
calor (Índice de Rumiación)</h3>
<figure>
<img src="../figures_controllers/fig_heatmap_ri.png"
alt="Mapa de calor del Índice de Rumiación (RI) a través de 15 controladores y 10 escenarios" />
<figcaption aria-hidden="true">Mapa de calor del Índice de Rumiación
(RI) a través de 15 controladores y 10 escenarios</figcaption>
</figure>
<p><em>RI agregado como promedio sobre 20 seeds para cada par
controlador×escenario (datos:
<code>outputs_final/metrics.csv</code>).</em></p>
<hr />
<h3 id="figura-s5-mapa-de-calor-tiempo-de-recuperación">Figura S5: Mapa
de calor (Tiempo de Recuperación)</h3>
<figure>
<img src="../figures_controllers/fig_heatmap_rt.png"
alt="Mapa de calor del Tiempo de Recuperación (RT) a través de 15 controladores y 10 escenarios" />
<figcaption aria-hidden="true">Mapa de calor del Tiempo de Recuperación
(RT) a través de 15 controladores y 10 escenarios</figcaption>
</figure>
<p><em>RT agregado como promedio sobre 20 seeds para cada par
controlador×escenario (datos:
<code>outputs_final/metrics.csv</code>).</em></p>
<hr />
<h3 id="figura-s6-mapa-de-calor-esfuerzo-de-control">Figura S6: Mapa de
calor (Esfuerzo de Control)</h3>
<figure>
<img src="../figures_controllers/fig_heatmap_effort.png"
alt="Mapa de calor del Esfuerzo de Control a través de 15 controladores y 10 escenarios" />
<figcaption aria-hidden="true">Mapa de calor del Esfuerzo de Control a
través de 15 controladores y 10 escenarios</figcaption>
</figure>
<p><em>ControlEffort agregado como promedio sobre 20 seeds para cada par
controlador×escenario (datos:
<code>outputs_final/metrics.csv</code>).</em></p>
<h2 id="apéndice-g-resultados-detallados-del-benchmark">Apéndice G:
Resultados Detallados del Benchmark</h2>
<p>Este apéndice proporciona los datos completos de rendimiento para las
15 arquitecturas de controlador a través de los escenarios validados.
Las tablas a continuación comparan Rendimiento (Perf), Índice de
Rumiación (RI/Rumiación), Dominancia Narrativa (NarrDom), Tiempo de
Recuperación (RecovTime) y Esfuerzo de Control (Effort).</p>
<h3 id="g.1-línea-1-estabilidad-choques-de-valor-e-incertidumbre">G.1
Línea 1: Estabilidad (Choques de Valor e Incertidumbre)</h3>
<p><strong>Escenario: Inversión de Recompensa (Reward Flip)</strong></p>
<table>
<thead>
<tr>
<th>Controlador</th>
<th>Perf</th>
<th>Rumiación</th>
<th>RecovTime</th>
<th>Effort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td>0.998</td>
<td>0.000</td>
<td>0.000</td>
<td>1.587</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td>0.995</td>
<td>0.000</td>
<td>0.000</td>
<td>1.027</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td>0.994</td>
<td>1.377</td>
<td>4.300</td>
<td>0.390</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td>0.994</td>
<td>1.386</td>
<td>0.000</td>
<td>0.494</td>
</tr>
<tr>
<td>arc_v1</td>
<td>0.994</td>
<td>0.000</td>
<td>3.450</td>
<td>0.508</td>
</tr>
<tr>
<td>arc_robust</td>
<td>0.994</td>
<td>0.000</td>
<td>0.000</td>
<td>0.744</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td>0.993</td>
<td>0.000</td>
<td>0.000</td>
<td>0.353</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td>0.991</td>
<td>0.000</td>
<td>0.000</td>
<td>0.773</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td>0.991</td>
<td>0.000</td>
<td>0.000</td>
<td>0.784</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td>0.991</td>
<td>0.000</td>
<td>0.000</td>
<td>2.257</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td>0.978</td>
<td>0.000</td>
<td>1.900</td>
<td>1.257</td>
</tr>
<tr>
<td>perf_optimized</td>
<td>0.880</td>
<td>1.394</td>
<td>100.000</td>
<td>0.700</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td>0.859</td>
<td>1.407</td>
<td>95.050</td>
<td>0.492</td>
</tr>
<tr>
<td>naive_calm</td>
<td>0.508</td>
<td>1.408</td>
<td>0.050</td>
<td>0.149</td>
</tr>
<tr>
<td>no_control</td>
<td>0.415</td>
<td>1.408</td>
<td>100.000</td>
<td>0.000</td>
</tr>
</tbody>
</table>
<p><strong>Escenario: Ráfaga de Ruido (Noise Burst)</strong></p>
<table>
<thead>
<tr>
<th>Controlador</th>
<th>Perf</th>
<th>Rumiación</th>
<th>RecovTime</th>
<th>Effort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td>0.998</td>
<td>0.000</td>
<td>0.000</td>
<td>1.605</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td>0.995</td>
<td>0.000</td>
<td>0.000</td>
<td>1.106</td>
</tr>
<tr>
<td>arc_robust</td>
<td>0.993</td>
<td>0.000</td>
<td>1.300</td>
<td>0.785</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td>0.993</td>
<td>0.051</td>
<td>25.000</td>
<td>0.399</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td>0.993</td>
<td>1.386</td>
<td>1.250</td>
<td>0.566</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td>0.991</td>
<td>0.000</td>
<td>0.000</td>
<td>0.905</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td>0.991</td>
<td>0.000</td>
<td>0.000</td>
<td>0.915</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td>0.991</td>
<td>0.000</td>
<td>0.000</td>
<td>2.257</td>
</tr>
<tr>
<td>arc_v1</td>
<td>0.989</td>
<td>0.000</td>
<td>32.100</td>
<td>0.550</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td>0.987</td>
<td>1.263</td>
<td>33.050</td>
<td>0.444</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td>0.972</td>
<td>0.000</td>
<td>29.500</td>
<td>1.290</td>
</tr>
<tr>
<td>perf_optimized</td>
<td>0.880</td>
<td>1.394</td>
<td>100.000</td>
<td>0.700</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td>0.848</td>
<td>1.407</td>
<td>100.000</td>
<td>0.585</td>
</tr>
<tr>
<td>naive_calm</td>
<td>0.365</td>
<td>1.408</td>
<td>100.000</td>
<td>0.177</td>
</tr>
<tr>
<td>no_control</td>
<td>0.259</td>
<td>1.408</td>
<td>100.000</td>
<td>0.000</td>
</tr>
</tbody>
</table>
<p><strong>Escenario: Amenaza Repentina (Sudden Threat)</strong></p>
<table>
<thead>
<tr>
<th>Controlador</th>
<th>Perf</th>
<th>Rumiación</th>
<th>RecovTime</th>
<th>Effort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td>0.989</td>
<td>0.013</td>
<td>0.000</td>
<td>1.707</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td>0.968</td>
<td>0.010</td>
<td>0.000</td>
<td>1.298</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td>0.964</td>
<td>0.000</td>
<td>0.000</td>
<td>2.410</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td>0.964</td>
<td>0.008</td>
<td>0.000</td>
<td>1.222</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td>0.963</td>
<td>0.008</td>
<td>0.000</td>
<td>1.173</td>
</tr>
<tr>
<td>arc_robust</td>
<td>0.959</td>
<td>0.005</td>
<td>0.550</td>
<td>1.252</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td>0.949</td>
<td>1.386</td>
<td>0.050</td>
<td>1.088</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td>0.936</td>
<td>0.000</td>
<td>100.000</td>
<td>0.783</td>
</tr>
<tr>
<td>arc_v1</td>
<td>0.914</td>
<td>0.000</td>
<td>100.000</td>
<td>1.054</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td>0.908</td>
<td>0.000</td>
<td>100.000</td>
<td>1.643</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td>0.907</td>
<td>1.333</td>
<td>85.000</td>
<td>0.864</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td>0.890</td>
<td>1.407</td>
<td>100.000</td>
<td>1.370</td>
</tr>
<tr>
<td>perf_optimized</td>
<td>0.825</td>
<td>1.394</td>
<td>100.000</td>
<td>0.700</td>
</tr>
<tr>
<td>naive_calm</td>
<td>0.252</td>
<td>1.408</td>
<td>100.000</td>
<td>0.262</td>
</tr>
<tr>
<td>no_control</td>
<td>0.217</td>
<td>1.408</td>
<td>100.000</td>
<td>0.000</td>
</tr>
</tbody>
</table>
<h3 id="g.2-línea-2-memoria-y-aprendizaje-continuo">G.2 Línea 2: Memoria
y Aprendizaje Continuo</h3>
<p><strong>Escenario: Cambio de Distribución (Distribution
Shift)</strong></p>
<table>
<thead>
<tr>
<th>Controlador</th>
<th>Perf</th>
<th>Retención</th>
<th>Rumiación</th>
<th>Effort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td>0.998</td>
<td>1.000</td>
<td>0.000</td>
<td>1.645</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td>0.995</td>
<td>1.000</td>
<td>0.000</td>
<td>1.186</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td>0.991</td>
<td>1.000</td>
<td>0.000</td>
<td>0.999</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td>0.991</td>
<td>1.000</td>
<td>0.000</td>
<td>1.008</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td>0.991</td>
<td>1.000</td>
<td>0.000</td>
<td>2.296</td>
</tr>
<tr>
<td>arc_robust</td>
<td>0.985</td>
<td>1.000</td>
<td>0.000</td>
<td>0.892</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td>0.984</td>
<td>1.000</td>
<td>1.386</td>
<td>0.695</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td>0.982</td>
<td>1.000</td>
<td>0.057</td>
<td>0.486</td>
</tr>
<tr>
<td>arc_v1</td>
<td>0.972</td>
<td>1.000</td>
<td>0.000</td>
<td>0.674</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td>0.968</td>
<td>1.000</td>
<td>1.258</td>
<td>0.548</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td>0.959</td>
<td>1.000</td>
<td>0.000</td>
<td>1.372</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td>0.871</td>
<td>0.989</td>
<td>1.407</td>
<td>0.739</td>
</tr>
<tr>
<td>perf_optimized</td>
<td>0.869</td>
<td>0.943</td>
<td>1.394</td>
<td>0.700</td>
</tr>
<tr>
<td>naive_calm</td>
<td>0.276</td>
<td>0.155</td>
<td>1.408</td>
<td>0.200</td>
</tr>
<tr>
<td>no_control</td>
<td>0.199</td>
<td>0.000</td>
<td>1.408</td>
<td>0.000</td>
</tr>
</tbody>
</table>
<p><strong>Escenario: Conflicto de Objetivos (Goal
Conflict)</strong></p>
<table>
<thead>
<tr>
<th>Controlador</th>
<th>Perf</th>
<th>Retención</th>
<th>Rumiación</th>
<th>Effort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td>0.997</td>
<td>1.000</td>
<td>0.000</td>
<td>1.620</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td>0.993</td>
<td>1.000</td>
<td>0.000</td>
<td>1.134</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td>0.993</td>
<td>1.000</td>
<td>1.408</td>
<td>0.544</td>
</tr>
<tr>
<td>arc_robust</td>
<td>0.992</td>
<td>1.000</td>
<td>0.000</td>
<td>0.785</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td>0.991</td>
<td>1.000</td>
<td>0.000</td>
<td>0.388</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td>0.991</td>
<td>1.000</td>
<td>0.000</td>
<td>0.938</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td>0.991</td>
<td>1.000</td>
<td>0.000</td>
<td>0.947</td>
</tr>
<tr>
<td>arc_v1</td>
<td>0.990</td>
<td>1.000</td>
<td>0.000</td>
<td>0.555</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td>0.990</td>
<td>1.000</td>
<td>0.000</td>
<td>2.270</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td>0.989</td>
<td>1.000</td>
<td>1.410</td>
<td>0.430</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td>0.976</td>
<td>1.000</td>
<td>0.000</td>
<td>1.289</td>
</tr>
<tr>
<td>perf_optimized</td>
<td>0.873</td>
<td>0.957</td>
<td>1.417</td>
<td>0.700</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td>0.822</td>
<td>0.980</td>
<td>1.434</td>
<td>0.529</td>
</tr>
<tr>
<td>naive_calm</td>
<td>0.420</td>
<td>0.452</td>
<td>1.434</td>
<td>0.162</td>
</tr>
<tr>
<td>no_control</td>
<td>0.326</td>
<td>0.344</td>
<td>1.434</td>
<td>0.000</td>
</tr>
</tbody>
</table>
<h3 id="g.3-línea-3-anti-rumiación-bucles-narrativos">G.3 Línea 3:
Anti-Rumiación (Bucles Narrativos)</h3>
<p><strong>Escenario: Contradicción Sostenida (Sustained
Contradiction)</strong></p>
<table>
<thead>
<tr>
<th>Controlador</th>
<th>Perf</th>
<th>Rumiación</th>
<th>NarrDom</th>
<th>Effort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td>0.981</td>
<td>0.003</td>
<td>0.000</td>
<td>1.974</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td>0.934</td>
<td>0.000</td>
<td>0.000</td>
<td>1.534</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td>0.929</td>
<td>0.000</td>
<td>0.000</td>
<td>1.420</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td>0.922</td>
<td>0.000</td>
<td>0.000</td>
<td>1.384</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td>0.904</td>
<td>1.472</td>
<td>0.881</td>
<td>1.417</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td>0.886</td>
<td>0.000</td>
<td>0.000</td>
<td>2.531</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td>0.879</td>
<td>0.101</td>
<td>0.000</td>
<td>0.979</td>
</tr>
<tr>
<td>arc_robust</td>
<td>0.868</td>
<td>0.000</td>
<td>0.000</td>
<td>1.465</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td>0.837</td>
<td>1.449</td>
<td>0.821</td>
<td>1.112</td>
</tr>
<tr>
<td>arc_v1</td>
<td>0.817</td>
<td>0.000</td>
<td>0.000</td>
<td>1.278</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td>0.801</td>
<td>1.472</td>
<td>0.842</td>
<td>1.790</td>
</tr>
<tr>
<td>perf_optimized</td>
<td>0.790</td>
<td>1.472</td>
<td>0.957</td>
<td>0.700</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td>0.753</td>
<td>0.000</td>
<td>0.000</td>
<td>1.793</td>
</tr>
<tr>
<td>naive_calm</td>
<td>0.018</td>
<td>1.472</td>
<td>0.987</td>
<td>0.380</td>
</tr>
<tr>
<td>no_control</td>
<td>0.014</td>
<td>1.472</td>
<td>0.987</td>
<td>0.000</td>
</tr>
</tbody>
</table>
<p><strong>Escenario: Gaslighting</strong></p>
<table>
<thead>
<tr>
<th>Controlador</th>
<th>Perf</th>
<th>Rumiación</th>
<th>NarrDom</th>
<th>Effort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td>0.998</td>
<td>0.000</td>
<td>0.000</td>
<td>1.816</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td>0.992</td>
<td>0.000</td>
<td>0.000</td>
<td>1.196</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td>0.988</td>
<td>0.000</td>
<td>0.000</td>
<td>0.977</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td>0.988</td>
<td>0.000</td>
<td>0.000</td>
<td>0.986</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td>0.987</td>
<td>0.000</td>
<td>0.000</td>
<td>2.357</td>
</tr>
<tr>
<td>arc_robust</td>
<td>0.985</td>
<td>0.000</td>
<td>0.000</td>
<td>0.854</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td>0.983</td>
<td>1.417</td>
<td>0.810</td>
<td>0.649</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td>0.982</td>
<td>0.027</td>
<td>0.000</td>
<td>0.453</td>
</tr>
<tr>
<td>arc_v1</td>
<td>0.980</td>
<td>0.000</td>
<td>0.000</td>
<td>0.634</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td>0.978</td>
<td>0.848</td>
<td>0.521</td>
<td>0.515</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td>0.962</td>
<td>0.000</td>
<td>0.000</td>
<td>1.344</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td>0.865</td>
<td>1.430</td>
<td>0.745</td>
<td>0.677</td>
</tr>
<tr>
<td>perf_optimized</td>
<td>0.865</td>
<td>1.422</td>
<td>0.814</td>
<td>0.700</td>
</tr>
<tr>
<td>naive_calm</td>
<td>0.258</td>
<td>1.431</td>
<td>0.818</td>
<td>0.194</td>
</tr>
<tr>
<td>no_control</td>
<td>0.171</td>
<td>1.431</td>
<td>0.877</td>
<td>0.000</td>
</tr>
</tbody>
</table>
<p><strong>Escenario: Conflicto de Instrucciones (Instruction
Conflict)</strong></p>
<table>
<thead>
<tr>
<th>Controlador</th>
<th>Perf</th>
<th>Rumiación</th>
<th>NarrDom</th>
<th>Effort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td>0.976</td>
<td>0.000</td>
<td>0.000</td>
<td>1.892</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td>0.912</td>
<td>0.000</td>
<td>0.000</td>
<td>1.380</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td>0.894</td>
<td>1.444</td>
<td>0.697</td>
<td>1.192</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td>0.877</td>
<td>0.000</td>
<td>0.000</td>
<td>1.140</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td>0.866</td>
<td>0.000</td>
<td>0.000</td>
<td>1.146</td>
</tr>
<tr>
<td>arc_robust</td>
<td>0.854</td>
<td>0.000</td>
<td>0.000</td>
<td>1.242</td>
</tr>
<tr>
<td>perf_optimized</td>
<td>0.839</td>
<td>1.445</td>
<td>0.964</td>
<td>0.700</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td>0.839</td>
<td>0.000</td>
<td>0.000</td>
<td>2.415</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td>0.835</td>
<td>0.248</td>
<td>0.000</td>
<td>0.820</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td>0.830</td>
<td>1.429</td>
<td>0.663</td>
<td>0.919</td>
</tr>
<tr>
<td>arc_v1</td>
<td>0.826</td>
<td>0.359</td>
<td>0.000</td>
<td>1.010</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td>0.798</td>
<td>1.453</td>
<td>0.676</td>
<td>1.535</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td>0.792</td>
<td>0.000</td>
<td>0.000</td>
<td>2.020</td>
</tr>
<tr>
<td>naive_calm</td>
<td>0.076</td>
<td>1.453</td>
<td>0.694</td>
<td>0.369</td>
</tr>
<tr>
<td>no_control</td>
<td>0.034</td>
<td>1.453</td>
<td>0.969</td>
<td>0.000</td>
</tr>
</tbody>
</table>
<h3 id="g.4-línea-5-seguridad-adversaria">G.4 Línea 5: Seguridad
Adversaria</h3>
<p><strong>Escenario: Acoplamiento Adversario (Adversarial
Coupling)</strong></p>
<table>
<thead>
<tr>
<th>Controlador</th>
<th>Perf</th>
<th>Rumiación</th>
<th>NarrDom</th>
<th>Effort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_v1</td>
<td>0.963</td>
<td>0.000</td>
<td>0.000</td>
<td>0.719</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td>0.962</td>
<td>0.628</td>
<td>0.271</td>
<td>0.594</td>
</tr>
<tr>
<td>arc_robust</td>
<td>0.917</td>
<td>0.000</td>
<td>0.000</td>
<td>1.269</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td>0.915</td>
<td>1.481</td>
<td>0.497</td>
<td>1.235</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td>0.914</td>
<td>0.159</td>
<td>0.000</td>
<td>0.838</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td>0.902</td>
<td>0.000</td>
<td>0.000</td>
<td>2.074</td>
</tr>
<tr>
<td>perf_optimized</td>
<td>0.867</td>
<td>1.481</td>
<td>0.972</td>
<td>0.700</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td>0.848</td>
<td>1.476</td>
<td>0.894</td>
<td>0.514</td>
</tr>
<tr>
<td>no_control</td>
<td>0.409</td>
<td>1.470</td>
<td>0.956</td>
<td>0.000</td>
</tr>
<tr>
<td>arc_adaptive</td>
<td>0.193</td>
<td>0.008</td>
<td>0.000</td>
<td>2.331</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td>0.139</td>
<td>0.000</td>
<td>0.000</td>
<td>2.729</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td>0.139</td>
<td>0.005</td>
<td>0.001</td>
<td>1.820</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td>0.138</td>
<td>0.004</td>
<td>0.001</td>
<td>1.859</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td>0.134</td>
<td>0.006</td>
<td>0.001</td>
<td>1.971</td>
</tr>
<tr>
<td>naive_calm</td>
<td>0.073</td>
<td>1.475</td>
<td>0.495</td>
<td>0.332</td>
</tr>
</tbody>
</table>
<p><strong>Escenario: Dopamina Aleatoria (Random Dopamine)</strong></p>
<table>
<thead>
<tr>
<th>Controlador</th>
<th>Perf</th>
<th>Rumiación</th>
<th>NarrDom</th>
<th>Effort</th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_adaptive</td>
<td>0.976</td>
<td>0.000</td>
<td>0.000</td>
<td>2.150</td>
</tr>
<tr>
<td>arc_ultimate</td>
<td>0.946</td>
<td>0.000</td>
<td>0.000</td>
<td>1.435</td>
</tr>
<tr>
<td>arc_v1_lqr</td>
<td>0.943</td>
<td>1.456</td>
<td>0.743</td>
<td>0.940</td>
</tr>
<tr>
<td>arc_robust</td>
<td>0.932</td>
<td>0.000</td>
<td>0.000</td>
<td>1.006</td>
</tr>
<tr>
<td>arc_v1_pid</td>
<td>0.922</td>
<td>0.000</td>
<td>0.000</td>
<td>2.450</td>
</tr>
<tr>
<td>arc_v1_lqi</td>
<td>0.916</td>
<td>0.000</td>
<td>0.000</td>
<td>1.173</td>
</tr>
<tr>
<td>arc_v2_lqi</td>
<td>0.916</td>
<td>0.000</td>
<td>0.000</td>
<td>1.227</td>
</tr>
<tr>
<td>arc_v3_meta</td>
<td>0.905</td>
<td>0.259</td>
<td>0.000</td>
<td>0.646</td>
</tr>
<tr>
<td>arc_v1</td>
<td>0.897</td>
<td>1.124</td>
<td>0.581</td>
<td>0.787</td>
</tr>
<tr>
<td>arc_v2_hier</td>
<td>0.894</td>
<td>1.207</td>
<td>0.620</td>
<td>0.720</td>
</tr>
<tr>
<td>arc_v3_pid_meta</td>
<td>0.870</td>
<td>0.000</td>
<td>0.000</td>
<td>1.624</td>
</tr>
<tr>
<td>perf_optimized</td>
<td>0.861</td>
<td>1.457</td>
<td>0.958</td>
<td>0.700</td>
</tr>
<tr>
<td>arc_v3_lqr_meta</td>
<td>0.817</td>
<td>1.458</td>
<td>0.717</td>
<td>1.192</td>
</tr>
<tr>
<td>naive_calm</td>
<td>0.119</td>
<td>1.460</td>
<td>0.763</td>
<td>0.328</td>
</tr>
<tr>
<td>no_control</td>
<td>0.040</td>
<td>1.460</td>
<td>0.950</td>
<td>0.000</td>
</tr>
</tbody>
</table>
</body>
</html>
